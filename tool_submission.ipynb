{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a90699-3991-420e-b394-83425ad1d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94839d2e-5bd4-4e34-a6f5-37e3bd4245bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_papers_from_jsonl(file_path):\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    paper = json.loads(line.strip())\n",
    "                    papers.append(paper)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping a line due to JSON decoding error.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "\n",
    "    print(f\"Loaded papers from {file_path}.\")\n",
    "    return papers\n",
    "\n",
    "# Specify the path to your JSONL file\n",
    "jsonl_file_path = \"extracted_dfrws_papers_NEWEST_final.jsonl\"\n",
    "\n",
    "# Load papers\n",
    "papers = load_papers_from_jsonl(jsonl_file_path)\n",
    "\n",
    "# Display loaded papers (Optional)\n",
    "for i, paper in enumerate(papers[:5]):  # Limit display to the first 5 papers for readability\n",
    "    print(f\"Paper {i+1}:\")\n",
    "    print(f\"Title: {paper.get('title', 'No title provided')}\")\n",
    "    print(\"Content:\")\n",
    "    print(paper.get('content', 'No content provided')[:500])  # Print first 500 characters of content\n",
    "    print(\"-\" * 50)  # Separator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1fa3e0-6ca7-4ee1-910e-e909021eaefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_title_prompt(paper):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "    return f'''\n",
    "    You are tasked with extracting the full title from the digital forensics paper titled \"{title}\".\n",
    "\n",
    "    Guidelines:\n",
    "    - The title is usually at the top of the first page or in the first section.\n",
    "    - Extract the title in its entirety.\n",
    "\n",
    "    Your response must be in the following JSON format:\n",
    "    {{\n",
    "        \"title\": \"Title of the paper here\"\n",
    "    }}\n",
    "\n",
    "    Here is the paper content:\n",
    "    <Start of Paper Content>\n",
    "    {content}\n",
    "    <End of Paper Content>\n",
    "\n",
    "    Your response: \"\"\"\n",
    "    '''\n",
    "\n",
    "\n",
    "def generate_tools_prompt(paper):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "    \n",
    "    return f'''\n",
    "    You are tasked with extracting **tools** mentioned in the digital forensics paper titled \"{title}\".\n",
    "\n",
    " Guidelines:\n",
    " \n",
    "     \\t1. A **tool** is any named software, framework, system, or purpose-built script that is explicitly used or created for forensic or anti-forensic purposes.\n",
    "        \\t- Only include a tool if it is actually used, created, or extended in the paper. \n",
    "        \\t- As an AI assistant, you must differentiate between tools that are actually **used**, **created**, or **extended** in the paper versus those that are only **referenced** or **mentioned**. \n",
    "        \\t- Some papers explicitly state phrases like *“we created a tool”* or *“we used a tool.”* \n",
    "        \\t- However, if a tool is only mentioned for context in sections such as **Related Work**, **Literature Review**, **Background**, or in paper types like **SLR**, **Survey**, or **SoK**, it should NOT be included in the tools output.\n",
    "        \\t- General-purpose programming languages (e.g., Python, Java, C++), machine learning libraries or algorithms (e.g., Random Forest, SVM, TensorFlow, scikit-learn), and build systems (e.g., Ninja, CMake, Make) are NOT considered forensic or anti-forensic tools. \n",
    "        \\t- EXCLUDE supporting software. The following are NOT forensic/anti-forensic tools unless the paper explicitly presents a purpose-built forensic plugin/module built on top of them (in which case ONLY the plugin/module is the tool, not the platform):\n",
    "        \\t\\t-  • Databases: MySQL (https://www.mysql.com/), PostgreSQL (https://www.postgresql.org/), SQLite (https://www.sqlite.org/), MongoDB (https://www.mongodb.com/)\n",
    "        \\t\\t-  • Web servers / app servers: Apache HTTP Server (https://httpd.apache.org/), Nginx (https://nginx.org/), Microsoft IIS, Tomcat (https://tomcat.apache.org/)\n",
    "        \\t\\t-  • Operating systems: Ubuntu, Debian, Windows, macOS\n",
    "        \\t\\t-  • Build/compilers: Ninja (https://ninja-build.org/), CMake (https://cmake.org/), Make, GCC/Clang\n",
    "        \\t\\t-  • Languages & ML libs/algorithms: Python, Java, C/C++, R, Random Forest, SVM/SVC, TensorFlow, scikit-learn, PyTorch\n",
    "        \\t\\t- URL/Domain rule (strong): If a tool’s URL resolves to one of the above platform vendor domains (e.g., \"https://www.mysql.com/\" for MySQL, \"https://www.sqlite.org/\" for SQlite), classify it as supporting software and EXCLUDE it from the tools output (return no entry). Do NOT relabel it as a tool. \n",
    "\n",
    "\n",
    "    \\t2. For each tool actually **used**, **created**, or **extended**, provide:\n",
    "        \\t- \"tool_name\"\n",
    "        \\t- \"action\": \"used\" or \"created\" or \"extended\"\n",
    "        \\t- \"repository_link\" (URL or empty string)\n",
    "        \\t- \"license\": must be one of:\n",
    "          \\t\\t- \"open-source\": source code is publicly available, allowing others to inspect, modify, and extend. Open-source tools are continuously updated and widely reused. They enable creativity and expansion since others can build upon the original code. Users can also often download ready-to-run executables in addition to modifying the source. Examples: Kali Linux, CAINE, Autopsy.\n",
    "          \\t\\t- \"proprietary\": source code is closed and controlled by the originator (e.g., company or vendor). Users may download and run the executable (installer, binary, or licensed version), but cannot view or modify the source code. Proprietary tools are widely used in practice, especially in law enforcement, but cannot be extended by the community. Examples: FTK Forensic Toolkit, FTK Imager, Magnet AXIOM (Magnet Forensics).\n",
    "          \\t\\t- \"not-mentioned\": if the license type is not explicitly stated and no reliable source (e.g., URL, DOI) is available.\n",
    "          \\t- **Note:** The license type is independent of origin. Academic research tools can be released as either open-source (e.g., Bulk Extractor) or proprietary (e.g., closed binaries distributed by an academic team). \n",
    "          \\t- If no evidence is available in either the paper or its referenced sources, return \"not-mentioned\".\n",
    "\n",
    "        \\t- \"origin\": one of:\n",
    "          \\t\\t- \"academic_research_DFRWS\" if the tool was first introduced by this DFRWS paper\n",
    "          \\t\\t- \"academic_research_external\" if the tool was created in other academic venues (conferences, journals, academic projects)\n",
    "          \\t\\t- \"organization\" if the tool was created by companies, vendors, or non-academic organizations\n",
    "          \\t\\t- \"not-mentioned\" if the origin is not explicitly stated.\n",
    "\n",
    "     \\t3. Special case:\n",
    "        \\t- If the paper introduces a plugin, module, extension, or significant modification of an existing tool, mark \"action\" as \"extended\" and apply the same origin rules.\n",
    "        \\t- Also list the base tool separately if it was explicitly used.\n",
    "\n",
    "\n",
    "    \\t4. If the paper uses **no tools**, return: null\n",
    "\n",
    "       {{\n",
    "         \"tools\": [{{\"tool_name\": null, \"action\": null, \"repository_link\": null, \"repository_type\": null, \"origin\": null}}]\n",
    "       }}\n",
    "    \\t- Example 1: In the paper “Audit Data Reduction Using Neural Networks and Support Vector Machines” by Srinivas Mukkamala and Andrew Sung (DFRWS 2002), the authors used Neural Networks and Support Vector Machines (SVMs),\n",
    "    referencing the SVMlight implementation. According to the Guidelines, these are general-purpose machine learning libraries/algorithms and \n",
    "    not forensic or anti-forensic tools. Since no forensic tool was actually created or used, the expected output is:\n",
    "    \n",
    "    {{\n",
    "      \"tools\": null\n",
    "    }}\n",
    "    \\t- Example 2: In the paper “Language and Gender Author Cohort Analysis of E-mail for Computer Forensics” by Olivier de Vel, Malcolm Corney, Alison Anderson, and George Mohay (DFRWS 2002), \n",
    "    the authors use Support Vector Machines **SVMlight** for classification on stylometric/structural features. **SVMlight** is a general-purpose ML library, \n",
    "    not a forensic tool, so per the Guidelines the output is:\n",
    "\n",
    "    {{\n",
    "      \"tools\": null\n",
    "    }}\n",
    "    \n",
    "    \\t5. If a tool is found but any field (action, repository_link, repository_type, or authorship) is not mentioned, assign \"not-mentioned\" to that field.\n",
    "\n",
    "    JSON Format:\n",
    "    {{\n",
    "      \"tools\": [\n",
    "        {{\n",
    "          \"tool_name\": \"Volatility\",\n",
    "          \"action\": \"used\",\n",
    "          \"repository_link\": \"https://github.com/volatilityfoundation/volatility\",\n",
    "          \"repository_type\": \"open-source\",\n",
    "          \"origin\": \"organization\"\n",
    "        }},\n",
    "    \n",
    "        {{\n",
    "          \"tool_name\":\"DROP\",\n",
    "          \"action\": \"created\"\n",
    "          \"repository_link\": \"https://github.com/unhcfreg/DROP\",\n",
    "          \"repository_type\": \"open-source\",\n",
    "          \"origin\": \"academic_research_DFRWS\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "     \\t6. For example in the paper \"So fresh, so clean: Cloud forensic analysis of the Amazon iRobot Roomba vacuum\", DFRWS authors created\n",
    "    a tool and made it open-source:\n",
    "    {{\n",
    "      \"tools\": [\n",
    "        {{\n",
    "          \"tool_name\": \"PyRoomba\",\n",
    "          \"action\": \"created\",\n",
    "          \"repository_link\": \"https://github.com/BiTLab-BaggiliTruthLab/PyRoomba\",\n",
    "          \"repository_type\": \"open-source\",\n",
    "          \"origin\": \"academic_research_DFRWS\"\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    Here is the paper content:\n",
    "    <Start of Paper Content>\n",
    "    {content}\n",
    "    <End of Paper Content>\n",
    "\n",
    "    Your response: \"\"\"\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee3024-25b5-44c0-bc83-0f3c1d181771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read Excel (.xlsx)\n",
    "df = pd.read_excel(\"tools_new_expanded.csv.xlsx\")  # or rename file to .xlsx if it really is Excel\n",
    "\n",
    "# ---------- Normalization helpers ----------\n",
    "def norm(s):\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    return str(s).strip()\n",
    "\n",
    "def norm_lower(s):\n",
    "    s = norm(s)\n",
    "    return s.lower() if s is not None else None\n",
    "\n",
    "def normalize_license(val):\n",
    "    v = norm_lower(val)\n",
    "    if v is None or v in {\"\", \"na\", \"n/a\", \"none\", \"null\"}:\n",
    "        return \"not-mentioned\"\n",
    "    # collapse variants like \"open-source (gplv3)\"\n",
    "    if v.startswith(\"open-source\"):\n",
    "        return \"open-source\"\n",
    "    if v.startswith(\"proprietary\"):\n",
    "        return \"proprietary\"\n",
    "    if v == \"not-mentioned\":\n",
    "        return \"not-mentioned\"\n",
    "  \n",
    "\n",
    "def normalize_action(val):\n",
    "    v = norm_lower(val)\n",
    "    if v in {\"used\", \"created\", \"extended\"}:\n",
    "        return v\n",
    "    return None  # treat anything else as \"no tool entry\"\n",
    "\n",
    "def normalize_origin(val):\n",
    "    v = norm_lower(val)\n",
    "    allowed = {\n",
    "        \"academic_research_dfrws\",\n",
    "        \"academic_research_external\",\n",
    "        \"non-academia\",\n",
    "        \"not-mentioned\",\n",
    "    }\n",
    "    if v in allowed:\n",
    "        return v\n",
    "    return \"not-mentioned\"\n",
    "\n",
    "# ---------- Create normalized columns ----------\n",
    "df[\"action_norm\"]  = df.get(\"Action\", pd.Series([None]*len(df))).apply(normalize_action)\n",
    "df[\"origin_norm\"]  = df.get(\"Origin\", pd.Series([\"not-mentioned\"]*len(df))).apply(normalize_origin)\n",
    "df[\"license_norm\"] = df.get(\"License\", pd.Series([\"not-mentioned\"]*len(df))).apply(normalize_license)\n",
    "\n",
    "# ---------- Helper to pretty print a value_counts dict ----------\n",
    "def print_counts(title, series):\n",
    "    print(title)\n",
    "    vc = series.value_counts()\n",
    "    for k, v in vc.items():\n",
    "        print(f\"  {k}: {int(v)}\")\n",
    "    if vc.empty:\n",
    "        print(\"  (none)\")\n",
    "    print()\n",
    "\n",
    "# ---------- Global metrics ----------\n",
    "total_rows = len(df)\n",
    "null_count = int(df[\"action_norm\"].isna().sum())\n",
    "used_count     = int((df[\"action_norm\"] == \"used\").sum())\n",
    "created_count  = int((df[\"action_norm\"] == \"created\").sum())\n",
    "extended_count = int((df[\"action_norm\"] == \"extended\").sum())\n",
    "\n",
    "print(f\"Total rows (tool entries): {total_rows}\")\n",
    "print(f\"Null tool entries (no valid action): {null_count}\\n\")\n",
    "print(f\"Used tools: {used_count}\")\n",
    "print(f\"Created tools: {created_count}\")\n",
    "print(f\"Extended tools: {extended_count}\\n\")\n",
    "\n",
    "# ---------- Overall distributions ----------\n",
    "print_counts(\"Overall license counts (normalized):\", df[\"license_norm\"])\n",
    "print_counts(\"Overall origin counts (normalized):\", df[\"origin_norm\"])\n",
    "\n",
    "# ---------- Created breakdown ----------\n",
    "df_created = df[df[\"action_norm\"] == \"created\"]\n",
    "print_counts(\"Created tools by origin:\", df_created[\"origin_norm\"])\n",
    "print_counts(\"Created tools by license:\", df_created[\"license_norm\"])\n",
    "\n",
    "# ---------- DFRWS-created licenses ----------\n",
    "df_dfrws_created = df_created[df_created[\"origin_norm\"] == \"academic_research_dfrws\"]\n",
    "print_counts(\"Licenses of DFRWS-created tools (normalized):\", df_dfrws_created[\"license_norm\"])\n",
    "\n",
    "# ---------- USED breakdown (new) ----------\n",
    "df_used = df[df[\"action_norm\"] == \"used\"]\n",
    "print_counts(\"Used tools by origin:\", df_used[\"origin_norm\"])\n",
    "print_counts(\"Used tools by license:\", df_used[\"license_norm\"])\n",
    "\n",
    "# ---------- EXTENDED breakdown (optional, symmetry) ----------\n",
    "df_extended = df[df[\"action_norm\"] == \"extended\"]\n",
    "print_counts(\"Extended tools by origin:\", df_extended[\"origin_norm\"])\n",
    "print_counts(\"Extended tools by license:\", df_extended[\"license_norm\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb49152-c9b3-429d-b450-c5b7825404ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Imports & Config --------------------------\n",
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_PATH = Path(\"tools_new_expanded.csv.xlsx\")  # your file path\n",
    "ENCODING = \"latin-1\"\n",
    "SHEET_NAME = 0\n",
    "SAVE_OUTPUTS = False\n",
    "AUTO_INSTALL_OPENPYXL = True  # set False if you don't want auto-install\n",
    "\n",
    "# -------------------------- Helpers --------------------------\n",
    "def ensure_openpyxl():\n",
    "    try:\n",
    "        import openpyxl  # noqa: F401\n",
    "        return True\n",
    "    except ImportError:\n",
    "        if not AUTO_INSTALL_OPENPYXL:\n",
    "            return False\n",
    "        try:\n",
    "            print(\"openpyxl not found. Installing...\", flush=True)\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openpyxl\"])\n",
    "            import openpyxl  # noqa: F401\n",
    "            print(\"openpyxl installed.\\n\", flush=True)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install openpyxl: {e}\", flush=True)\n",
    "            return False\n",
    "\n",
    "def load_table(p: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Try CSV first. If it looks like Excel or CSV parse fails,\n",
    "    try Excel (using openpyxl). Reads everything as string.\n",
    "    \"\"\"\n",
    "    # Try CSV\n",
    "    try:\n",
    "        return pd.read_csv(p, encoding=ENCODING, dtype=str)\n",
    "    except Exception as csv_err:\n",
    "        # Fall back to Excel if CSV failed\n",
    "        ok = ensure_openpyxl()\n",
    "        if not ok:\n",
    "            raise ImportError(\n",
    "                \"Missing 'openpyxl' needed to read Excel. \"\n",
    "                \"Install it via: pip install openpyxl\"\n",
    "            ) from csv_err\n",
    "        return pd.read_excel(p, sheet_name=SHEET_NAME, dtype=str)\n",
    "\n",
    "def norm(s):\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    return str(s).strip()\n",
    "\n",
    "def norm_lower(s):\n",
    "    s = norm(s)\n",
    "    return s.lower() if s is not None else None\n",
    "\n",
    "def normalize_license(val):\n",
    "    v = norm_lower(val)\n",
    "    if v is None or v in {\"\", \"na\", \"n/a\", \"none\", \"null\"}:\n",
    "        return \"not-specified\"\n",
    "    if v.startswith(\"open-source\"):\n",
    "        return \"open-source\"\n",
    "    if v.startswith(\"proprietary\"):\n",
    "        return \"proprietary\"\n",
    "    if v == \"not-specified\":\n",
    "        return \"not-specified\"\n",
    "    if any(x in v for x in [\"gpl\", \"mit\", \"apache\", \"bsd\", \"lgpl\", \"agpl\"]):\n",
    "        return \"open-source\"\n",
    "    return \"not-specified\"\n",
    "\n",
    "def normalize_action(val):\n",
    "    v = norm_lower(val)\n",
    "    return v if v in {\"used\", \"created\", \"extended\"} else None\n",
    "\n",
    "def normalize_origin(val):\n",
    "    v = norm_lower(val)\n",
    "    allowed = {\n",
    "        \"academic_research_dfrws\",\n",
    "        \"academic_research_external\",\n",
    "        \"non-academia\",\n",
    "        \"not-specified\",\n",
    "    }\n",
    "    return v if v in allowed else \"not-specified\"\n",
    "\n",
    "def map_case_insensitive(df: pd.DataFrame):\n",
    "    seen = {}\n",
    "    for c in df.columns:\n",
    "        cl = c.strip().lower()\n",
    "        if cl not in seen:\n",
    "            seen[cl] = c\n",
    "    return seen\n",
    "\n",
    "def get_col(df: pd.DataFrame, colmap: dict, name: str):\n",
    "    return colmap.get(name.strip().lower())\n",
    "\n",
    "def print_table(title, table):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(table.to_string())\n",
    "\n",
    "def print_counts(title, series: pd.Series):\n",
    "    print(f\"\\n{title}\")\n",
    "    counts = series.value_counts(dropna=False)\n",
    "    for k, v in counts.items():\n",
    "        label = \"NaN\" if pd.isna(k) else str(k)\n",
    "        print(f\"  {label}: {int(v)}\")\n",
    "\n",
    "# -------------------------- Main --------------------------\n",
    "def main():\n",
    "    df = load_table(INPUT_PATH)\n",
    "\n",
    "    # Clean header names and map case-insensitively\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    colmap = map_case_insensitive(df)\n",
    "\n",
    "    action_col  = get_col(df, colmap, \"Action\")\n",
    "    origin_col  = get_col(df, colmap, \"Origin\")\n",
    "    license_col = get_col(df, colmap, \"License\")\n",
    "\n",
    "    # Fallback columns if missing\n",
    "    if action_col is None:\n",
    "        df[\"__Action__\"] = None\n",
    "        action_col = \"__Action__\"\n",
    "    if origin_col is None:\n",
    "        df[\"__Origin__\"] = \"not-specified\"\n",
    "        origin_col = \"__Origin__\"\n",
    "    if license_col is None:\n",
    "        df[\"__License__\"] = \"not-specified\"\n",
    "        license_col = \"__License__\"\n",
    "\n",
    "    # Normalized columns\n",
    "    df[\"action_norm\"]  = df[action_col].apply(normalize_action)\n",
    "    df[\"origin_norm\"]  = df[origin_col].apply(normalize_origin)\n",
    "    df[\"license_norm\"] = df[license_col].apply(normalize_license)\n",
    "\n",
    "    # Global stats\n",
    "    total_rows = len(df)\n",
    "    null_count = int(df[\"action_norm\"].isna().sum())\n",
    "    df_act = df[df[\"action_norm\"].notna()].copy()\n",
    "    total_action_instances = len(df_act)\n",
    "\n",
    "    print(f\"Total tool entries (paper–tool instances): {total_rows}\")\n",
    "    print(f\"Null tool entries (no valid action): {null_count}\\n\")\n",
    "\n",
    "    # Overall action breakdown\n",
    "    print_counts(\"Overall license counts (normalized):\", df[\"license_norm\"])\n",
    "    print_counts(\"Overall origin counts (normalized):\",  df[\"origin_norm\"])\n",
    "\n",
    "    # Action-specific subsets\n",
    "    df_used    = df[df[\"action_norm\"] == \"used\"]\n",
    "    df_created = df[df[\"action_norm\"] == \"created\"]\n",
    "    df_extended= df[df[\"action_norm\"] == \"extended\"]\n",
    "\n",
    "    print(f\"\\nUsed tools: {len(df_used)}\")\n",
    "    print(f\"Created tools: {len(df_created)}\")\n",
    "    print(f\"Extended tools: {len(df_extended)}\")\n",
    "\n",
    "    # Created breakdowns\n",
    "    print_counts(\"Created tools by origin:\",  df_created[\"origin_norm\"])\n",
    "    print_counts(\"Created tools by license:\", df_created[\"license_norm\"])\n",
    "\n",
    "    df_dfrws_created = df_created[df_created[\"origin_norm\"] == \"academic_research_dfrws\"]\n",
    "    print_counts(\"Licenses of DFRWS-created tools (normalized):\", df_dfrws_created[\"license_norm\"])\n",
    "\n",
    "    # Used breakdowns\n",
    "    print_counts(\"Used tools by origin:\",  df_used[\"origin_norm\"])\n",
    "    print_counts(\"Used tools by license:\", df_used[\"license_norm\"])\n",
    "\n",
    "    # Extended breakdowns\n",
    "    print_counts(\"Extended tools by origin:\",  df_extended[\"origin_norm\"])\n",
    "    print_counts(\"Extended tools by license:\", df_extended[\"license_norm\"])\n",
    "\n",
    "    # Pivot tables\n",
    "    action_cols = [\"used\", \"created\", \"extended\"]\n",
    "    if total_action_instances == 0:\n",
    "        print(\"\\nNo valid action rows found. Check the 'Action' column values.\")\n",
    "        return\n",
    "\n",
    "    license_action = pd.crosstab(\n",
    "        df_act[\"license_norm\"], df_act[\"action_norm\"], dropna=False\n",
    "    ).reindex(columns=action_cols, fill_value=0)\n",
    "    license_action.loc[\"TOTAL\"] = license_action.sum()\n",
    "    license_action[\"TOTAL\"] = license_action.sum(axis=1)\n",
    "\n",
    "    origin_action = pd.crosstab(\n",
    "        df_act[\"origin_norm\"], df_act[\"action_norm\"], dropna=False\n",
    "    ).reindex(columns=action_cols, fill_value=0)\n",
    "    origin_action.loc[\"TOTAL\"] = origin_action.sum()\n",
    "    origin_action[\"TOTAL\"] = origin_action.sum(axis=1)\n",
    "\n",
    "    print_table(\"License × Action counts\", license_action)\n",
    "    print_table(\"Origin × Action counts\", origin_action)\n",
    "\n",
    "    if SAVE_OUTPUTS:\n",
    "        license_action.to_csv(\"license_x_action_counts.csv\", index=True)\n",
    "        origin_action.to_csv(\"origin_x_action_counts.csv\", index=True)\n",
    "        print(\"\\nSaved: license_x_action_counts.csv, origin_x_action_counts.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5dda4-9ec5-4367-a406-fb20f0cbeb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Frequency of widely used tools from tools_new_expanded.csv.xlsx\n",
    "\n",
    "What it does:\n",
    "- Reads the Excel file (expects columns: Title, Tool Name, Action, Repository Link, License, Origin)\n",
    "- Normalizes tool names (trim whitespace; optional lowercasing)\n",
    "- Computes:\n",
    "  1) Overall tool frequency (all actions)\n",
    "  2) \"Widely used\" tool frequency (Action == \"used\")\n",
    "  3) (Optional) Frequency by action type\n",
    "- Saves results to CSVs and prints top N to console\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "INPUT_PATH = Path(\"tools_new_expanded.csv.xlsx\")  # adjust path if needed\n",
    "SHEET_NAME = 0  # or set to a sheet name string\n",
    "TOP_N = 30\n",
    "\n",
    "# If you want case-insensitive merging of tool names, set True\n",
    "LOWERCASE_TOOLNAMES = False\n",
    "\n",
    "# ----------------------------\n",
    "# Load\n",
    "# ----------------------------\n",
    "df = pd.read_excel(INPUT_PATH, sheet_name=SHEET_NAME)\n",
    "\n",
    "# Ensure expected columns exist (rename here if your headers differ slightly)\n",
    "expected = [\"Title\", \"Tool Name\", \"Action\", \"Repository Link\", \"License\", \"Origin\"]\n",
    "missing = [c for c in expected if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\\nFound: {list(df.columns)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Clean / normalize\n",
    "# ----------------------------\n",
    "df[\"Tool Name\"] = df[\"Tool Name\"].astype(\"string\").str.strip()\n",
    "df[\"Action\"] = df[\"Action\"].astype(\"string\").str.strip().str.lower()\n",
    "\n",
    "# Drop null/empty tool names\n",
    "df = df[df[\"Tool Name\"].notna() & (df[\"Tool Name\"] != \"\")].copy()\n",
    "\n",
    "# Optional: normalize tool names to lower-case for grouping\n",
    "if LOWERCASE_TOOLNAMES:\n",
    "    df[\"Tool Name Norm\"] = df[\"Tool Name\"].str.lower()\n",
    "else:\n",
    "    df[\"Tool Name Norm\"] = df[\"Tool Name\"]\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Overall frequency (all actions)\n",
    "# ----------------------------\n",
    "overall = (\n",
    "    df.groupby(\"Tool Name Norm\", dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    "      .sort_values([\"count\", \"Tool Name Norm\"], ascending=[False, True])\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Widely used = Action == \"used\"\n",
    "# ----------------------------\n",
    "used_df = df[df[\"Action\"] == \"used\"].copy()\n",
    "\n",
    "used_freq = (\n",
    "    used_df.groupby(\"Tool Name Norm\", dropna=False)\n",
    "          .size()\n",
    "          .reset_index(name=\"count_used\")\n",
    "          .sort_values([\"count_used\", \"Tool Name Norm\"], ascending=[False, True])\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Frequency by action (used/created/extended) per tool\n",
    "# ----------------------------\n",
    "by_action = (\n",
    "    df.pivot_table(\n",
    "        index=\"Tool Name Norm\",\n",
    "        columns=\"Action\",\n",
    "        values=\"Title\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0,\n",
    "        dropna=False,\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Add totals and sort by \"used\" first if present\n",
    "if \"used\" in by_action.columns:\n",
    "    by_action[\"TOTAL\"] = by_action.drop(columns=[\"Tool Name Norm\"]).sum(axis=1)\n",
    "    by_action = by_action.sort_values([\"used\", \"TOTAL\", \"Tool Name Norm\"], ascending=[False, False, True])\n",
    "else:\n",
    "    by_action[\"TOTAL\"] = by_action.drop(columns=[\"Tool Name Norm\"]).sum(axis=1)\n",
    "    by_action = by_action.sort_values([\"TOTAL\", \"Tool Name Norm\"], ascending=[False, True])\n",
    "\n",
    "# ----------------------------\n",
    "# Output\n",
    "# ----------------------------\n",
    "out_dir = Path(\"tool_frequency_outputs\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "overall_path = out_dir / \"tool_frequency_overall.csv\"\n",
    "used_path = out_dir / \"tool_frequency_used_only.csv\"\n",
    "by_action_path = out_dir / \"tool_frequency_by_action.csv\"\n",
    "\n",
    "overall.to_csv(overall_path, index=False)\n",
    "used_freq.to_csv(used_path, index=False)\n",
    "by_action.to_csv(by_action_path, index=False)\n",
    "\n",
    "print(\"\\nTop tools (overall):\")\n",
    "print(overall.head(TOP_N).to_string(index=False))\n",
    "\n",
    "print(\"\\nTop tools (Action == 'used'):\")\n",
    "print(used_freq.head(TOP_N).to_string(index=False))\n",
    "\n",
    "print(f\"\\nSaved:\\n- {overall_path}\\n- {used_path}\\n- {by_action_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Optional: quick bar chart for top used tools\n",
    "# Uncomment if you want a plot saved as PNG\n",
    "# ----------------------------\n",
    "# import matplotlib.pyplot as plt\n",
    "# top_used = used_freq.head(TOP_N)\n",
    "# plt.figure()\n",
    "# plt.bar(top_used[\"Tool Name Norm\"], top_used[\"count_used\"])\n",
    "# plt.xticks(rotation=75, ha=\"right\")\n",
    "# plt.ylabel(\"Count (used)\")\n",
    "# plt.title(f\"Top {TOP_N} Widely Used Tools (Action='used')\")\n",
    "# plt.tight_layout()\n",
    "# plot_path = out_dir / f\"top_{TOP_N}_used_tools.png\"\n",
    "# plt.savefig(plot_path, dpi=200)\n",
    "# print(f\"Saved plot: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c038c-4bc2-40ae-835f-b6612ea115f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete script: Overall tool frequency (ALL actions) + bar chart with license legend\n",
    "Fixes the \"nan tool\" issue by removing missing/blank tool names BEFORE string conversion.\n",
    "\n",
    "Input:  tools_new_expanded.csv.xlsx\n",
    "Cols:   Title, Tool Name, Action, Repository Link, License, Origin\n",
    "Output:\n",
    "  - Prints top N tools (overall frequency)\n",
    "  - Displays bar chart (colored by license)\n",
    "  - Saves cleaned frequency table + chart to disk\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "INPUT_FILE = Path(\"tools_new_expanded.csv.xlsx\")\n",
    "TOP_N = 15\n",
    "\n",
    "OUT_DIR = Path(\"tool_frequency_outputs\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "FREQ_CSV = OUT_DIR / \"tool_frequency_overall.csv\"\n",
    "PLOT_PNG = OUT_DIR / f\"top_{TOP_N}_tools_overall_by_license.png\"\n",
    "\n",
    "LICENSE_COLORS = {\n",
    "    \"open-source\": \"#4daf4a\",     # green\n",
    "    \"proprietary\": \"#e41a1c\",     # red\n",
    "    \"not-specified\": \"#999999\",   # gray\n",
    "}\n",
    "\n",
    "# Canonical tool-name normalization rules\n",
    "# (Add more ecosystems here as needed)\n",
    "TOOL_NORMALIZATION = [\n",
    "    # Normalize all Cellebrite variants to an ecosystem label\n",
    "    (r\"\\bcellebrite\\b\", \"Cellebrite UFED (ecosystem)\"),\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Load\n",
    "# ----------------------------\n",
    "df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "# ----------------------------\n",
    "# Clean (IMPORTANT: drop NaN tool names before converting to string)\n",
    "# ----------------------------\n",
    "df = df[df[\"Tool Name\"].notna()].copy()\n",
    "\n",
    "df[\"Tool Name\"] = df[\"Tool Name\"].astype(\"string\").str.strip()\n",
    "df[\"Action\"] = df[\"Action\"].astype(\"string\").str.lower().str.strip()\n",
    "df[\"License\"] = df[\"License\"].astype(\"string\").str.lower().str.strip()\n",
    "\n",
    "# Drop empty tool names and any accidental \"nan\" strings\n",
    "df = df[df[\"Tool Name\"].notna() & (df[\"Tool Name\"] != \"\")]\n",
    "df = df[df[\"Tool Name\"].str.lower() != \"nan\"]\n",
    "\n",
    "# Exclude DB Browser for SQLite\n",
    "df = df[df[\"Tool Name\"].str.lower() != \"db browser for sqlite\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Normalize tool names (ecosystem-level)\n",
    "# ----------------------------\n",
    "for pattern, canonical in TOOL_NORMALIZATION:\n",
    "    df.loc[df[\"Tool Name\"].str.contains(pattern, case=False, na=False, regex=True), \"Tool Name\"] = canonical\n",
    "\n",
    "# ----------------------------\n",
    "# Normalize License values (optional safety)\n",
    "# ----------------------------\n",
    "df[\"License\"] = df[\"License\"].fillna(\"not-specified\")\n",
    "df.loc[~df[\"License\"].isin(LICENSE_COLORS.keys()), \"License\"] = \"not-specified\"\n",
    "\n",
    "# ----------------------------\n",
    "# Overall frequency (all actions)\n",
    "# ----------------------------\n",
    "freq = (\n",
    "    df.groupby([\"Tool Name\", \"License\"], dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    "      .sort_values([\"count\", \"Tool Name\"], ascending=[False, True])\n",
    ")\n",
    "\n",
    "# Save frequency table\n",
    "freq.to_csv(FREQ_CSV, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Display table (top N)\n",
    "# ----------------------------\n",
    "print(f\"\\nTop {TOP_N} tools by overall frequency (all actions):\\n\")\n",
    "print(freq.head(TOP_N).to_string(index=False))\n",
    "\n",
    "# ----------------------------\n",
    "# Plot (top N)\n",
    "# ----------------------------\n",
    "top = freq.head(TOP_N).copy()\n",
    "colors = top[\"License\"].map(lambda x: LICENSE_COLORS.get(x, LICENSE_COLORS[\"not-specified\"]))\n",
    "\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.bar(top[\"Tool Name\"], top[\"count\"], color=colors)\n",
    "plt.xticks(rotation=70, ha=\"right\")\n",
    "plt.ylabel(\"Frequency (all actions)\")\n",
    "plt.title(f\"Top {TOP_N} Tools by Overall Frequency (colored by license)\")\n",
    "\n",
    "# Legend\n",
    "legend_handles = [\n",
    "    plt.Line2D([0], [0], color=c, lw=6, label=l.replace(\"-\", \" \").title())\n",
    "    for l, c in LICENSE_COLORS.items()\n",
    "]\n",
    "plt.legend(handles=legend_handles, title=\"License\", frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_PNG, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSaved:\\n- Frequency table: {FREQ_CSV}\\n- Plot: {PLOT_PNG}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
