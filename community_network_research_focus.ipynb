{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba4d77-79c4-46b6-a09a-7212abdea3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d4458-da72-4a59-99fb-12ec76f88f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import PyPDF2\n",
    "\n",
    "# Function to extract PDFs from a folder\n",
    "def extract_pdfs_from_folder(pdf_folder):\n",
    "    pdf_files = []\n",
    "    for file_name in os.listdir(pdf_folder):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            pdf_files.append(os.path.join(pdf_folder, file_name))\n",
    "    return pdf_files\n",
    "\n",
    "# Robust title extractor with multi-line handling and city/date filtering\n",
    "def extract_title_from_pdf(pdf_text):\n",
    "    lines = pdf_text.splitlines()\n",
    "\n",
    "    skip_patterns = [\n",
    "        r'DFRWS.*\\d{4}',\n",
    "        r'Digital Investigation',\n",
    "        r'Forensic Science International',\n",
    "        r'ScienceDirect',\n",
    "        r'ELSEVIER',\n",
    "        r'CrossMark',\n",
    "        r'Contents lists available at',\n",
    "        r'journal homepage',\n",
    "        r'Available at',\n",
    "        r'www\\.',\n",
    "        r'^$',\n",
    "        r'^DIGITAL FORENSIC RESEARCH CONFERENCE$',\n",
    "        r'^DOI[:\\s]',\n",
    "        r'^http[s]?://',\n",
    "        r'creativecommons.org',\n",
    "    ]\n",
    "    bad_title_patterns = [\n",
    "        r'^\\(?[A-Z][a-z]+, [A-Z]{2} \\(',       # City, State ( e.g., Syracuse, NY (\n",
    "        r'^\\(?[A-Z][a-z]+ \\d{1,2}(st|nd|rd|th)?\\)?',  # Aug 6th\n",
    "        r'^\\(?\\d{4}\\)?$',                      # year\n",
    "    ]\n",
    "    stop_patterns = [\n",
    "        r'^By ',\n",
    "        r'From the proceedings of',\n",
    "        r'DFRWS \\d{4}',\n",
    "        r'\\(\\w{3,9} \\d{1,2}',\n",
    "        r'^\\d{4} \\d{1,2}:\\d{2}',               # timestamps\n",
    "        r'^Elsevier',\n",
    "        r'^Keywords',\n",
    "    ]\n",
    "\n",
    "    clean_lines = [line.strip() for line in lines if line.strip()]\n",
    "    title_candidates = []\n",
    "\n",
    "    for i in range(min(50, len(clean_lines))):\n",
    "        line = clean_lines[i]\n",
    "        if any(re.search(pat, line, re.IGNORECASE) for pat in skip_patterns):\n",
    "            continue\n",
    "        if any(re.search(pat, line.strip(), re.IGNORECASE) for pat in bad_title_patterns):\n",
    "            continue\n",
    "        if any(re.search(pat, line, re.IGNORECASE) for pat in stop_patterns):\n",
    "            break\n",
    "        if len(line.split()) < 2:\n",
    "            continue\n",
    "        title_candidates.append((i, line))\n",
    "\n",
    "    # Step 1: Multi-line title merge\n",
    "    for i, line in title_candidates:\n",
    "        if i + 1 < len(clean_lines):\n",
    "            next_line = clean_lines[i + 1].strip()\n",
    "            full_title = f\"{line} {next_line}\"\n",
    "            if 6 <= len(full_title.split()) <= 20 and not full_title.endswith('.'):\n",
    "                return full_title\n",
    "\n",
    "    # Step 2: Title right before \"By\"\n",
    "    for idx, line in title_candidates:\n",
    "        if idx + 1 < len(clean_lines) and clean_lines[idx + 1].lower().startswith(\"by \"):\n",
    "            return line\n",
    "\n",
    "    # Step 3: Fallback to any decent candidate\n",
    "    for _, line in title_candidates:\n",
    "        if len(line.split()) >= 4:\n",
    "            return line\n",
    "\n",
    "    return \"Unknown Title\"\n",
    "\n",
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    try:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        pdf_text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page_text = pdf_reader.pages[page_num].extract_text()\n",
    "            if page_text:\n",
    "                pdf_text += page_text\n",
    "        return pdf_text\n",
    "    except KeyError as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {pdf_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main script\n",
    "pdf_folder = r\"pdf_full_dataset\"\n",
    "output_path = 'extracted_dfrws_papers_NEWEST_final.jsonl'\n",
    "\n",
    "papers = []\n",
    "pdf_files = extract_pdfs_from_folder(pdf_folder)\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    if text:\n",
    "        title = extract_title_from_pdf(text)\n",
    "        papers.append({\"title\": title, \"content\": text})\n",
    "        print(f\" {os.path.basename(pdf_file)} → Title: {title}\")\n",
    "    else:\n",
    "        print(f\" Failed to extract text from: {os.path.basename(pdf_file)}\")\n",
    "\n",
    "# Save output to JSONL\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for paper in papers:\n",
    "        safe_text = json.dumps(paper, ensure_ascii=False).encode('utf-8', 'ignore').decode('utf-8')\n",
    "        f.write(safe_text + \"\\n\")\n",
    "\n",
    "print(f\"\\n Extraction complete. Output saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bde52e-efdb-4be8-9271-068b8c5fc4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78137715-56db-4b0d-8418-aa7f0a027040",
   "metadata": {},
   "source": [
    "Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92ae48-193b-49b3-a4d6-81dbce3e788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_title_prompt(paper):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "    return f'''\n",
    "    You are tasked with extracting the full title from the digital forensics paper titled \"{title}\".\n",
    "\n",
    "    Guidelines:\n",
    "    - The title is usually at the top of the first page or in the first section.\n",
    "    - Extract the title in its entirety.\n",
    "\n",
    "    Your response must be in the following JSON format:\n",
    "    {{\n",
    "        \"title\": \"Title of the paper here\"\n",
    "    }}\n",
    "\n",
    "    Here is the paper content:\n",
    "    <Start of Paper Content>\n",
    "    {content}\n",
    "    <End of Paper Content>\n",
    "\n",
    "    Your response: \"\"\"\n",
    "    '''\n",
    "\n",
    "\n",
    "def generate_tools_prompt(paper):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "    \n",
    "    return f'''\n",
    "    You are tasked with extracting **tools** mentioned in the digital forensics paper titled \"{title}\".\n",
    "\n",
    " Guidelines:\n",
    " \n",
    "     \\t1. A **tool** is any named software, framework, system, or purpose-built script that is explicitly used or created for forensic or anti-forensic purposes.\n",
    "        \\t- Only include a tool if it is actually used, created, or extended in the paper. \n",
    "        \\t- As an AI assistant, you must differentiate between tools that are actually **used**, **created**, or **extended** in the paper versus those that are only **referenced** or **mentioned**. \n",
    "        \\t- Some papers explicitly state phrases like *“we created a tool”* or *“we used a tool.”* \n",
    "        \\t- However, if a tool is only mentioned for context in sections such as **Related Work**, **Literature Review**, **Background**, or in paper types like **SLR**, **Survey**, or **SoK**, it should NOT be included in the tools output.\n",
    "        \\t- General-purpose programming languages (e.g., Python, Java, C++), machine learning libraries or algorithms (e.g., Random Forest, SVM, TensorFlow, scikit-learn), and build systems (e.g., Ninja, CMake, Make) are NOT considered forensic or anti-forensic tools. \n",
    "        \\t- EXCLUDE supporting software. The following are NOT forensic/anti-forensic tools unless the paper explicitly presents a purpose-built forensic plugin/module built on top of them (in which case ONLY the plugin/module is the tool, not the platform):\n",
    "        \\t\\t-  • Databases: MySQL (https://www.mysql.com/), PostgreSQL (https://www.postgresql.org/), SQLite (https://www.sqlite.org/), MongoDB (https://www.mongodb.com/)\n",
    "        \\t\\t-  • Web servers / app servers: Apache HTTP Server (https://httpd.apache.org/), Nginx (https://nginx.org/), Microsoft IIS, Tomcat (https://tomcat.apache.org/)\n",
    "        \\t\\t-  • Operating systems: Ubuntu, Debian, Windows, macOS\n",
    "        \\t\\t-  • Build/compilers: Ninja (https://ninja-build.org/), CMake (https://cmake.org/), Make, GCC/Clang\n",
    "        \\t\\t-  • Languages & ML libs/algorithms: Python, Java, C/C++, R, Random Forest, SVM/SVC, TensorFlow, scikit-learn, PyTorch\n",
    "        \\t\\t- URL/Domain rule (strong): If a tool’s URL resolves to one of the above platform vendor domains (e.g., \"https://www.mysql.com/\" for MySQL, \"https://www.sqlite.org/\" for SQlite), classify it as supporting software and EXCLUDE it from the tools output (return no entry). Do NOT relabel it as a tool. \n",
    "\n",
    "\n",
    "    \\t2. For each tool actually **used**, **created**, or **extended**, provide:\n",
    "        \\t- \"tool_name\"\n",
    "        \\t- \"action\": \"used\" or \"created\" or \"extended\"\n",
    "        \\t- \"repository_link\" (URL or empty string)\n",
    "        \\t- \"license\": must be one of:\n",
    "          \\t\\t- \"open-source\": source code is publicly available, allowing others to inspect, modify, and extend. Open-source tools are continuously updated and widely reused. They enable creativity and expansion since others can build upon the original code. Users can also often download ready-to-run executables in addition to modifying the source. Examples: Kali Linux, CAINE, Autopsy.\n",
    "          \\t\\t- \"proprietary\": source code is closed and controlled by the originator (e.g., company or vendor). Users may download and run the executable (installer, binary, or licensed version), but cannot view or modify the source code. Proprietary tools are widely used in practice, especially in law enforcement, but cannot be extended by the community. Examples: FTK Forensic Toolkit, FTK Imager, Magnet AXIOM (Magnet Forensics).\n",
    "          \\t\\t- \"not-mentioned\": if the license type is not explicitly stated and no reliable source (e.g., URL, DOI) is available.\n",
    "          \\t- **Note:** The license type is independent of origin. Academic research tools can be released as either open-source (e.g., Bulk Extractor) or proprietary (e.g., closed binaries distributed by an academic team). \n",
    "          \\t- If no evidence is available in either the paper or its referenced sources, return \"not-mentioned\".\n",
    "\n",
    "        \\t- \"origin\": one of:\n",
    "          \\t\\t- \"academic_research_DFRWS\" if the tool was first introduced by this DFRWS paper\n",
    "          \\t\\t- \"academic_research_external\" if the tool was created in other academic venues (conferences, journals, academic projects)\n",
    "          \\t\\t- \"organization\" if the tool was created by companies, vendors, or non-academic organizations\n",
    "          \\t\\t- \"not-mentioned\" if the origin is not explicitly stated.\n",
    "\n",
    "     \\t3. Special case:\n",
    "        \\t- If the paper introduces a plugin, module, extension, or significant modification of an existing tool, mark \"action\" as \"extended\" and apply the same origin rules.\n",
    "        \\t- Also list the base tool separately if it was explicitly used.\n",
    "\n",
    "\n",
    "    \\t4. If the paper uses **no tools**, return: null\n",
    "\n",
    "       {{\n",
    "         \"tools\": [{{\"tool_name\": null, \"action\": null, \"repository_link\": null, \"repository_type\": null, \"origin\": null}}]\n",
    "       }}\n",
    "    \\t- Example 1: In the paper “Audit Data Reduction Using Neural Networks and Support Vector Machines” by Srinivas Mukkamala and Andrew Sung (DFRWS 2002), the authors used Neural Networks and Support Vector Machines (SVMs),\n",
    "    referencing the SVMlight implementation. According to the Guidelines, these are general-purpose machine learning libraries/algorithms and \n",
    "    not forensic or anti-forensic tools. Since no forensic tool was actually created or used, the expected output is:\n",
    "    \n",
    "    {{\n",
    "      \"tools\": null\n",
    "    }}\n",
    "    \\t- Example 2: In the paper “Language and Gender Author Cohort Analysis of E-mail for Computer Forensics” by Olivier de Vel, Malcolm Corney, Alison Anderson, and George Mohay (DFRWS 2002), \n",
    "    the authors use Support Vector Machines **SVMlight** for classification on stylometric/structural features. **SVMlight** is a general-purpose ML library, \n",
    "    not a forensic tool, so per the Guidelines the output is:\n",
    "\n",
    "    {{\n",
    "      \"tools\": null\n",
    "    }}\n",
    "    \n",
    "    \\t5. If a tool is found but any field (action, repository_link, repository_type, or authorship) is not mentioned, assign \"not-mentioned\" to that field.\n",
    "\n",
    "    JSON Format:\n",
    "    {{\n",
    "      \"tools\": [\n",
    "        {{\n",
    "          \"tool_name\": \"Volatility\",\n",
    "          \"action\": \"used\",\n",
    "          \"repository_link\": \"https://github.com/volatilityfoundation/volatility\",\n",
    "          \"repository_type\": \"open-source\",\n",
    "          \"origin\": \"organization\"\n",
    "        }},\n",
    "    \n",
    "        {{\n",
    "          \"tool_name\":\"DROP\",\n",
    "          \"action\": \"created\"\n",
    "          \"repository_link\": \"https://github.com/unhcfreg/DROP\",\n",
    "          \"repository_type\": \"open-source\",\n",
    "          \"origin\": \"academic_research_DFRWS\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "     \\t6. For example in the paper \"So fresh, so clean: Cloud forensic analysis of the Amazon iRobot Roomba vacuum\", DFRWS authors created\n",
    "    a tool and made it open-source:\n",
    "    {{\n",
    "      \"tools\": [\n",
    "        {{\n",
    "          \"tool_name\": \"PyRoomba\",\n",
    "          \"action\": \"created\",\n",
    "          \"repository_link\": \"https://github.com/BiTLab-BaggiliTruthLab/PyRoomba\",\n",
    "          \"repository_type\": \"open-source\",\n",
    "          \"origin\": \"academic_research_DFRWS\"\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    Here is the paper content:\n",
    "    <Start of Paper Content>\n",
    "    {content}\n",
    "    <End of Paper Content>\n",
    "\n",
    "    Your response: \"\"\"\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a7c36-b4dd-4fb4-87d3-be858c3f3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv(\"api_key.env\")\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "\n",
    "# Your processor\n",
    "def process_papers_for_tasks(papers, tasks):\n",
    "    task_results = {}\n",
    "    for i, paper in enumerate(papers):\n",
    "        paper_title = paper['title']\n",
    "        print(f\"\\nProcessing paper: {paper_title}\")\n",
    "        task_results[paper_title] = {}\n",
    "\n",
    "        for task in tasks:\n",
    "            if task == \"title\":\n",
    "                user_prompt = generate_title_prompt(paper)\n",
    "            elif task == \"tools\":\n",
    "                user_prompt = generate_tools_prompt(paper)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=0.2,\n",
    "                    max_tokens=5000\n",
    "                )\n",
    "                response_text = response.choices[0].message.content\n",
    "                print(response_text)\n",
    "                task_results[paper_title][task] = response_text\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {task} for paper {i+1}: {e}\")\n",
    "                task_results[paper_title][task] = f\"error: {e}\"\n",
    "\n",
    "    return task_results\n",
    "\n",
    "# -------------------------------\n",
    "# Run on all papers\n",
    "# -------------------------------\n",
    "test_papers = papers  # assumes 'papers' is a list of dicts with 'title' and 'content'\n",
    "tasks = [\"title\", \"tools\"]\n",
    "\n",
    "results = process_papers_for_tasks(test_papers, tasks)\n",
    "\n",
    "# -------------------------------\n",
    "# Save CSV\n",
    "# -------------------------------\n",
    "with open(\"results_new_missed_tools_2005.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Paper Title\"] + tasks)\n",
    "    for paper_title, r in results.items():\n",
    "        row = [paper_title] + [r.get(task, \"No result\") for task in tasks]\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Results saved to results_new_missed_tools_2005.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e1ebc-a252-4345-8f83-1b8c5bb616ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv(\"api_key.env\")\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Your prompt generators must be defined above or in the same file:\n",
    "# def generate_title_prompt(paper): …\n",
    "# def generate_tools_prompt(paper): …\n",
    "\n",
    "# Your processor\n",
    "def process_papers_for_tasks(papers, tasks):\n",
    "    task_results = {}\n",
    "    for i, paper in enumerate(papers):\n",
    "        paper_title = paper['title']\n",
    "        print(f\"\\nProcessing paper: {paper_title}\")\n",
    "        task_results[paper_title] = {}\n",
    "\n",
    "        for task in tasks:\n",
    "            if task == \"title\":\n",
    "                user_prompt = generate_title_prompt(paper)\n",
    "            elif task == \"tools\":\n",
    "                user_prompt = generate_tools_prompt(paper)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=0.2,\n",
    "                    max_tokens=5000\n",
    "                )\n",
    "                response_text = response.choices[0].message.content\n",
    "                print(response_text)\n",
    "                task_results[paper_title][task] = response_text\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {task} for paper {i+1}: {e}\")\n",
    "                task_results[paper_title][task] = f\"error: {e}\"\n",
    "\n",
    "    return task_results\n",
    "\n",
    "\n",
    "# Run on all papers\n",
    "\n",
    "test_papers = papers  # assumes 'papers' is a list of dicts with 'title' and 'content'\n",
    "tasks = [\"title\", \"tools\"]\n",
    "\n",
    "results = process_papers_for_tasks(test_papers, tasks)\n",
    "\n",
    "# Save CSV\n",
    "with open(\"results_new_last_tools.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Paper Title\"] + tasks)\n",
    "    for paper_title, r in results.items():\n",
    "        row = [paper_title] + [r.get(task, \"No result\") for task in tasks]\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Results saved to results_new_last_tools.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f210f29-d920-4593-8936-1d3d197e18d2",
   "metadata": {},
   "source": [
    "Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10d6b9-8302-4461-ba0a-e2410cb54261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_metadata_prompt(task, paper, ontology_json=None):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "\n",
    "    if task == \"title\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the full title from the digital forensics paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - The title is usually at the top of the first page or in the first section.\n",
    "        - Extract the title in its entirety.\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"title\": \"Title of the paper here\"\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "\n",
    "    elif task == \"authors\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the list of authors from the paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - The authors' names are typically listed directly below the title or in the header/footer.\n",
    "        - Extract all authors, separated by commas.\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"authors\": \"Comma-separated list of authors' names here\"\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "    elif task == \"school_names\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the name(s) of academic institutions or organizations affiliated with the authors of the paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - The school or institutional affiliations are usually listed below the authors' names or in the first page.\n",
    "        - Extract **all** school names or affiliations mentioned.\n",
    "        - Include universities, colleges, research institutions, or companies if provided.\n",
    "        - Remove duplicates if the same institution is mentioned multiple times.\n",
    "\n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"school_names\": [\"University of California, Berkeley\", \"National University of Singapore\"]\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "    elif task == \"author_countries\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the country or countries where the authors or their affiliated institutions are based for the paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - Country names are often listed after the institution name, or appear in the corresponding author information, address section, or footnotes.\n",
    "        - Only look at the countries mentioned alongside author names, typically found at the top of the paper. Do not scan the entire document.\n",
    "        - If a country is not explicitly stated but can be reliably inferred from a well-known institution (e.g., MIT → USA), include it.\n",
    "        - Do not guess or hallucinate. If the country cannot be determined, return \"null\".\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"author_countries\": [\"USA\", \"Germany\"]\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "\n",
    "\n",
    "    elif task == \"conference\":\n",
    "        return f'''\n",
    "        \n",
    "        You are tasked with identifying the conference of publication for the paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        \\t1- The conference name will always be one of the following:\n",
    "        \\t- \"DFRWS USA\"\n",
    "        \\t- \"DFRWS Europe\"\n",
    "        \\t- \"DFRWS APAC\"\n",
    "        \n",
    "        \\t2- Read the paper carefully to determine which DFRWS conference it was published in.\n",
    "        \n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"conference\": \"DFRWS USA\"\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "\n",
    "    elif task == \"published_year\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the year of publication from the digital forensics paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - The year is usually found near the conference name or in the paper's footer/header.\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"year\": \"Year of publication here\"\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "    elif task == \"forensic_vs_anti\":\n",
    "        return f'''\n",
    "       \n",
    "       You are tasked with extracting the following metadata from the provided digital forensics paper titled \"{title}\":\n",
    "       \n",
    "       You must classify the paper as either **Forensic** or **Anti-Forensic**.\n",
    "\n",
    "        Definition for Identifying paper type:\n",
    "        \\t1- **Digital Forensic**: A paper that supports, enhances, or proposes solutions for conducting digital forensic investigations.\n",
    "        \\t2- **Anti-Forensic**: A paper that explores techniques or tools meant to subvert, obscure, or defeat digital forensic processes, often discussed either to develop countermeasures or expose threats.\n",
    "\n",
    "        Book Definitions:\n",
    "        \\t1- **Digital Forensics**: The application of science to the identification, collection/acquisition, examination, and analysis of data while preserving the integrity of the information and maintaining a strict chain \n",
    "        of custody for the data.\n",
    "        \\t2- **Anti-Forensic**: A technique for concealing or destroying data so that others cannot access it.\n",
    "\n",
    "        Guidelines:\n",
    "        \\t1- Determine whether the focus of the paper aligns more with digital forensic goals (acquisition, analysis, etc.) or with undermining such goals.\n",
    "        \\t2- You must assess the paper’s core contribution and stance. Just because a paper discusses anti-forensic techniques does **not** necessarily mean it is an anti-forensic paper.\n",
    "        \\t3- If a paper studies anti-forensic techniques with the goal of developing better forensic tools, defenses, or acquisition methods, it is still considered ** Digital Forensic**.\n",
    "        \\t4- If a paper introduces or promotes anti-forensic techniques without a forensic enhancement objective, then it is **Anti-Forensic**.\n",
    "\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"forensic_type\": \"Digital Forensic\"  // or \"Anti-Forensic\"\n",
    "        }}\n",
    "    \n",
    "    \n",
    "        \\t- Examples in JSON format.\n",
    "        Example 1:\n",
    "        In the paper \"Anti-forensic resilient memory acquisition\", the authors design a memory acquisition technique that is robust against anti-forensic attacks.\n",
    "        Since the paper contributes to strengthening digital forensic methods and improving resilience, it should be classified as forensic.\n",
    "        {{\n",
    "            \"forensic_type\": \"Forensic\"\n",
    "        }}\n",
    "\n",
    "        Example 2:\n",
    "        In the paper \"Android anti-forensics through a local paradigm\", the authors present practical techniques to undermine forensic processes on Android devices, including delaying and manipulating evidence. \n",
    "        These methods aim to defeat forensic tools, making the paper anti-forensic.\n",
    "        {{\n",
    "            \"forensic_type\": \"Anti-Forensic\"\n",
    "        }}\n",
    "        \n",
    "        Example 3:\n",
    "        In the paper \"Forensic carving of network packets and associated data structures\" (2011), the focus is on developing techniques for reconstructing network packets from raw data to aid in forensic investigations. \n",
    "        It enhances forensic analysis and does not attempt to conceal or disrupt it. Thus, it is classified as:\n",
    "        {{\n",
    "            \"forensic_type\": \"Forensic\"\n",
    "        }}\n",
    "        Example 4:\n",
    "        In the paper “Anti-forensics in ext4: On secrecy and usability of timestamp-based data hiding”, \n",
    "        the authors design and implement a timestamp-based steganographic technique that conceals data within ext4 \n",
    "        file system metadata using nanosecond-level timestamp fields. The work prioritizes secrecy, indistinguishability, and usability of hidden data, explicitly aiming to evade forensic detection. Since the primary contribution enables anti-forensic data hiding without proposing defensive detection or mitigation mechanisms, \n",
    "        the paper is classified as anti-forensic.\n",
    "        {{\n",
    "            \"forensic_type\": \"Anti-Forensic\"\n",
    "        }}\n",
    "        \n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b537d0-cc09-4938-ae7f-5b40f410c613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv(\"api_key.env\")\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Your prompt generator import or definition:\n",
    "# Make sure generate_all_metadata_prompt is defined exactly as you have it\n",
    "\n",
    "# Example: \n",
    "# from your_prompt_file import generate_all_metadata_prompt\n",
    "\n",
    "# Main processor\n",
    "def process_papers_for_tasks(papers, tasks):\n",
    "    task_results = {}\n",
    "\n",
    "    for i, paper in enumerate(papers):\n",
    "        paper_title = paper['title']\n",
    "        print(f\"Processing paper: {paper_title}\")\n",
    "\n",
    "        task_results[paper_title] = {}\n",
    "\n",
    "        for task in tasks:\n",
    "            user_prompt = generate_all_metadata_prompt(task, paper)\n",
    "\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": user_prompt,\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0.2,\n",
    "                    max_tokens=1500\n",
    "                )\n",
    "\n",
    "                response_text = response.choices[0].message.content\n",
    "                print(response_text)\n",
    "                task_results[paper_title][task] = response_text\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {task} for paper {i+1}: {e}\")\n",
    "                task_results[paper_title][task] = \"error: \" + str(e)\n",
    "\n",
    "    return task_results\n",
    "\n",
    "#  Replace 'papers' with your dataset\n",
    "# For testing, slice first 5 papers\n",
    "test_papers = papers[:]\n",
    "\n",
    "#  Define your actual tasks\n",
    "tasks = [\"title\", \"authors\", \"school_names\", \"author_countries\", \"conference\", \"published_year\", \"forensic_vs_anti\", \"ontology_classification\"]\n",
    "\n",
    "# Process papers\n",
    "all_results = process_papers_for_tasks(test_papers, tasks)\n",
    "\n",
    "# Save to CSV exactly like you had it\n",
    "output_csv_path = 'results_2025_prompts_new.csv'\n",
    "with open(output_csv_path, 'w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    headers = ['Paper Title'] + tasks\n",
    "    csv_writer.writerow(headers)\n",
    "\n",
    "    for paper_title, results in all_results.items():\n",
    "        row = [paper_title]\n",
    "        for task in tasks:\n",
    "            row.append(results.get(task, \"No result\"))\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2b5a5-a7dc-4a4b-9e13-1a8dbd662d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import chardet  # Auto-detect encoding\n",
    "\n",
    "# Input and output file paths\n",
    "input_csv = \"results_combined_prompts_new.csv\"\n",
    "output_jsonl = \"results_combined_prompts_new.jsonl\"\n",
    "\n",
    "# Detect encoding automatically\n",
    "with open(input_csv, \"rb\") as raw_file:\n",
    "    result = chardet.detect(raw_file.read(100000))  # Read first 100KB for encoding detection\n",
    "    detected_encoding = result[\"encoding\"]\n",
    "\n",
    "print(f\"Detected encoding: {detected_encoding}\")\n",
    "\n",
    "# Read CSV and write to JSONL using detected encoding\n",
    "with open(input_csv, \"r\", encoding=detected_encoding, errors=\"replace\") as csv_file, \\\n",
    "     open(output_jsonl, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "    \n",
    "    reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    for row in reader:\n",
    "        # Convert JSON-like strings back to Python objects where applicable\n",
    "        for key, value in row.items():\n",
    "            try:\n",
    "                row[key] = json.loads(value)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Keep as-is if not a JSON string\n",
    "        \n",
    "        # Write each row as a JSON object in JSONL format\n",
    "        jsonl_file.write(json.dumps(row, ensure_ascii=False) + \"\\n\")  # Keep non-ASCII characters readable\n",
    "\n",
    "print(f\"Converted CSV to JSONL: {output_jsonl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23a2ba-712e-4b3f-8967-5d0bfe33f649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "RESULTS_PATH = \"results_combined_prompts_new.jsonl\"\n",
    "EXTRACTED_PATH = \"extracted_test_papers_new_dfrws_all.jsonl\"\n",
    "\n",
    "RESULTS_TITLE_COL = \"Paper Title\"\n",
    "EXTRACTED_TITLE_COL = \"title\"\n",
    "\n",
    "# -------------------------\n",
    "# JSONL loader that PRESERVES the original JSONL line number\n",
    "# -------------------------\n",
    "def load_jsonl_with_line_numbers(path: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                obj[\"_jsonl_line\"] = line_no  # <-- keep original row/line number\n",
    "                rows.append(obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"Bad JSON on line {line_no} in {path}: {e}\") from e\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def normalize_title(s):\n",
    "    \"\"\"\n",
    "    Normalize to make title matching reliable:\n",
    "    - Unicode normalize (fix ligatures like ﬁ -> fi)\n",
    "    - lowercase\n",
    "    - strip + collapse whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = s.lower().strip()\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "# -------------------------\n",
    "# Load (with line numbers)\n",
    "# -------------------------\n",
    "df_r = load_jsonl_with_line_numbers(RESULTS_PATH)\n",
    "df_e = load_jsonl_with_line_numbers(EXTRACTED_PATH)\n",
    "\n",
    "if RESULTS_TITLE_COL not in df_r.columns:\n",
    "    raise KeyError(f\"'{RESULTS_TITLE_COL}' not found in {RESULTS_PATH}. Columns: {df_r.columns.tolist()}\")\n",
    "if EXTRACTED_TITLE_COL not in df_e.columns:\n",
    "    raise KeyError(f\"'{EXTRACTED_TITLE_COL}' not found in {EXTRACTED_PATH}. Columns: {df_e.columns.tolist()}\")\n",
    "\n",
    "# Keep raw + normalized titles\n",
    "df_r[\"_title_raw\"] = df_r[RESULTS_TITLE_COL].astype(str)\n",
    "df_e[\"_title_raw\"] = df_e[EXTRACTED_TITLE_COL].astype(str)\n",
    "\n",
    "df_r[\"_title_norm\"] = df_r[\"_title_raw\"].apply(normalize_title)\n",
    "df_e[\"_title_norm\"] = df_e[\"_title_raw\"].apply(normalize_title)\n",
    "\n",
    "# Drop empty titles (usually correct; change if you want to keep empties)\n",
    "df_r_nonempty = df_r[df_r[\"_title_norm\"] != \"\"].copy()\n",
    "df_e_nonempty = df_e[df_e[\"_title_norm\"] != \"\"].copy()\n",
    "\n",
    "print(\"Rows (results):\", len(df_r), \"nonempty titles:\", len(df_r_nonempty))\n",
    "print(\"Rows (extracted):\", len(df_e), \"nonempty titles:\", len(df_e_nonempty))\n",
    "\n",
    "# -------------------------\n",
    "# Count-based comparison (handles duplicates)\n",
    "# -------------------------\n",
    "r_counts = df_r_nonempty[\"_title_norm\"].value_counts()\n",
    "e_counts = df_e_nonempty[\"_title_norm\"].value_counts()\n",
    "\n",
    "all_norm_titles = sorted(set(r_counts.index).union(set(e_counts.index)))\n",
    "\n",
    "diff_rows = []\n",
    "for tnorm in all_norm_titles:\n",
    "    rc = int(r_counts.get(tnorm, 0))\n",
    "    ec = int(e_counts.get(tnorm, 0))\n",
    "    if rc != ec:\n",
    "        r_example = (\n",
    "            df_r_nonempty.loc[df_r_nonempty[\"_title_norm\"] == tnorm, \"_title_raw\"].iloc[0]\n",
    "            if rc > 0 else \"\"\n",
    "        )\n",
    "        e_example = (\n",
    "            df_e_nonempty.loc[df_e_nonempty[\"_title_norm\"] == tnorm, \"_title_raw\"].iloc[0]\n",
    "            if ec > 0 else \"\"\n",
    "        )\n",
    "\n",
    "        diff_rows.append({\n",
    "            \"title_norm\": tnorm,\n",
    "            \"results_count\": rc,\n",
    "            \"extracted_count\": ec,\n",
    "            \"results_example_title\": r_example,\n",
    "            \"extracted_example_title\": e_example,\n",
    "            \"status\": \"missing_in_results\" if rc == 0 else (\"missing_in_extracted\" if ec == 0 else \"count_mismatch\")\n",
    "        })\n",
    "\n",
    "diff_df = pd.DataFrame(diff_rows).sort_values(\n",
    "    by=[\"status\", \"results_count\", \"extracted_count\", \"title_norm\"],\n",
    "    ascending=[True, True, True, True]\n",
    ")\n",
    "\n",
    "missing_in_results = diff_df[diff_df[\"status\"] == \"missing_in_results\"]\n",
    "missing_in_extracted = diff_df[diff_df[\"status\"] == \"missing_in_extracted\"]\n",
    "count_mismatch = diff_df[diff_df[\"status\"] == \"count_mismatch\"]\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(\"Missing in results:\", len(missing_in_results))\n",
    "print(\"Missing in extracted:\", len(missing_in_extracted))\n",
    "print(\"Count mismatches:\", len(count_mismatch))\n",
    "\n",
    "# -------------------------\n",
    "# Show which titles are missing\n",
    "# -------------------------\n",
    "if len(missing_in_results) > 0:\n",
    "    print(\"\\n--- Titles present in extracted but missing in RESULTS ---\")\n",
    "    for t in missing_in_results[\"extracted_example_title\"].tolist():\n",
    "        print(\" •\", t)\n",
    "\n",
    "if len(missing_in_extracted) > 0:\n",
    "    print(\"\\n--- Titles present in results but missing in EXTRACTED ---\")\n",
    "    for t in missing_in_extracted[\"results_example_title\"].tolist():\n",
    "        print(\" •\", t)\n",
    "\n",
    "# -------------------------\n",
    "# Save diff summaries\n",
    "# -------------------------\n",
    "diff_df.to_csv(\"title_differences.csv\", index=False)\n",
    "missing_in_results.to_csv(\"missing_in_results.csv\", index=False)\n",
    "missing_in_extracted.to_csv(\"missing_in_extracted.csv\", index=False)\n",
    "count_mismatch.to_csv(\"count_mismatch.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved: title_differences.csv\")\n",
    "print(\"Saved: missing_in_results.csv, missing_in_extracted.csv, count_mismatch.csv\")\n",
    "\n",
    "# -------------------------\n",
    "# IMPORTANT PART:\n",
    "# Find the exact JSONL row number(s) in EXTRACTED for titles missing in RESULTS\n",
    "# -------------------------\n",
    "if len(missing_in_results) > 0:\n",
    "    missing_norms = set(missing_in_results[\"title_norm\"])\n",
    "    missing_rows = df_e_nonempty[df_e_nonempty[\"_title_norm\"].isin(missing_norms)].copy()\n",
    "\n",
    "    cols_to_show = [\"_jsonl_line\", EXTRACTED_TITLE_COL]\n",
    "    for c in [\"published_year\", \"year\", \"conference\", \"authors\", \"doi\", \"url\"]:\n",
    "        if c in missing_rows.columns and c not in cols_to_show:\n",
    "            cols_to_show.append(c)\n",
    "\n",
    "    print(\"\\n=== Missing paper(s) location in EXTRACTED JSONL (line number) ===\")\n",
    "    print(missing_rows[cols_to_show].sort_values(\"_jsonl_line\").to_string(index=False))\n",
    "\n",
    "    # Save the missing records to a JSONL file (parsed objects)\n",
    "    missing_records = missing_rows.drop(\n",
    "        columns=[c for c in missing_rows.columns if c.startswith(\"_\")],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    missing_records.to_json(\n",
    "        \"missing_records_from_extracted.jsonl\",\n",
    "        orient=\"records\",\n",
    "        lines=True,\n",
    "        force_ascii=False\n",
    "    )\n",
    "    print(\"\\nSaved: missing_records_from_extracted.jsonl\")\n",
    "\n",
    "    # Also print the *raw original JSONL line(s)* exactly as in the file\n",
    "    missing_line_numbers = sorted(missing_rows[\"_jsonl_line\"].unique().tolist())\n",
    "    print(\"\\n=== Raw JSONL line(s) from extracted file ===\")\n",
    "    with open(EXTRACTED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            if i in missing_line_numbers:\n",
    "                print(f\"\\n--- line {i} ---\")\n",
    "                print(line.rstrip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3bb5c-b3e5-4eed-b49b-cf05ff63e0d8",
   "metadata": {},
   "source": [
    "DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66123ce-0edf-48f4-9f7f-70a333a99c9d",
   "metadata": {},
   "source": [
    "Venue × Discipline table (USA, EU, APAC) — counts + MEDIAN + CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c5e34-5682-4844-97dd-a82b08cb9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250863b9-b192-45b2-8cd3-0bd38f3ef719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8) Venue × Discipline table (USA, EU, APAC)\n",
    "# -------------------------------\n",
    "# Helper: map conference string -> venue bucket\n",
    "def map_conference_to_venue(conf_str: str) -> str:\n",
    "    s = (conf_str or \"\").strip().upper()\n",
    "    if \"USA\" in s:\n",
    "        return \"DFRWS USA\"\n",
    "    if \"EU\" in s or \"EUROPE\" in s:\n",
    "        return \"DFRWS EU\"\n",
    "    if \"APAC\" in s or \"ASIA\" in s or \"ASIA-PACIFIC\" in s or \"ASIA PACIFIC\" in s:\n",
    "        return \"DFRWS APAC\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    # parse fields\n",
    "    conf_json = safe_parse_json(row.get(\"conference\", \"\"))\n",
    "    ont_json  = safe_parse_json(row.get(\"ontology_classification\", \"\"))\n",
    "\n",
    "    conf_raw = conf_json.get(\"conference\", \"\")\n",
    "    venue    = map_conference_to_venue(conf_raw)\n",
    "\n",
    "    discipline = (ont_json.get(\"discipline\", \"\") or \"\").strip()\n",
    "\n",
    "  \n",
    "    # skip empties & unknown venues\n",
    "    if not discipline:\n",
    "        continue\n",
    "    if venue == \"Unknown\":\n",
    "        continue\n",
    "\n",
    "    records.append({\"venue\": venue, \"discipline\": discipline})\n",
    "\n",
    "venue_disc_df = pd.DataFrame(records)\n",
    "\n",
    "# Build full matrix, include all venues that have papers and all disciplines seen in your run\n",
    "venue_disc_matrix = (\n",
    "    pd.crosstab(venue_disc_df[\"venue\"], venue_disc_df[\"discipline\"])\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Order columns (disciplines) by global total descending\n",
    "col_order = venue_disc_matrix.sum(axis=0).sort_values(ascending=False).index\n",
    "venue_disc_matrix = venue_disc_matrix[col_order]\n",
    "\n",
    "# Ensure the row order is USA, EU, APAC (only those that exist)\n",
    "desired_rows = [v for v in [\"DFRWS USA\", \"DFRWS EU\", \"DFRWS APAC\"] if v in venue_disc_matrix.index]\n",
    "venue_disc_matrix = venue_disc_matrix.reindex(desired_rows)\n",
    "\n",
    "# Add summary columns\n",
    "venue_disc_matrix[\"TOTAL\"]   = venue_disc_matrix.sum(axis=1)\n",
    "venue_disc_matrix[\"AVERAGE\"] = (venue_disc_matrix[col_order].mean(axis=1)).round(2)\n",
    "\n",
    "print(\"\\n==m= Venue × Discipline (counts) ===\")\n",
    "print(venue_disc_matrix)\n",
    "\n",
    "# Save for your paper/supplement\n",
    "venue_disc_matrix.to_csv(\"venue_discipline_matrix.csv\")\n",
    "print(\"\\nSaved: venue_discipline_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5c7f8-af05-4ea7-ab21-36f25db6ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "disc_cols = [c for c in venue_disc_matrix.columns if c not in [\"TOTAL\",\"AVERAGE\"]]\n",
    "\n",
    "# Average across all disciplines (zeros counted) – what you already have\n",
    "venue_disc_matrix[\"AVERAGE_ALL\"] = (venue_disc_matrix[disc_cols].mean(axis=1)).round(2)\n",
    "\n",
    "# Average across non-zero disciplines only\n",
    "venue_disc_matrix[\"AVERAGE_NONZERO\"] = (\n",
    "    venue_disc_matrix[disc_cols].replace(0, pd.NA).mean(axis=1)\n",
    ").round(2)\n",
    "\n",
    "# Median per venue (robust to outliers)\n",
    "venue_disc_matrix[\"MEDIAN\"] = venue_disc_matrix[disc_cols].median(axis=1).round(2)\n",
    "\n",
    "# Coefficient of variation (std/mean) – lower = more consistent across domains\n",
    "means = venue_disc_matrix[disc_cols].mean(axis=1).replace(0, np.nan)\n",
    "stds  = venue_disc_matrix[disc_cols].std(axis=1, ddof=0)\n",
    "venue_disc_matrix[\"CV\"] = (stds / means).round(3)\n",
    "\n",
    "# (Optional) Keep just one \"AVERAGE\" and rename it:\n",
    "venue_disc_matrix[\"AVERAGE\"] = venue_disc_matrix[\"AVERAGE_ALL\"].round(2)\n",
    "venue_disc_matrix.drop(columns=[\"AVERAGE_ALL\"], inplace=True)\n",
    "\n",
    "# Show full table and save\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None,\n",
    "                       \"display.width\", 200, \"display.max_colwidth\", None):\n",
    "    print(\"\\n=== Venue × Discipline (counts + averages) ===\")\n",
    "    print(venue_disc_matrix.to_string())\n",
    "\n",
    "venue_disc_matrix.to_csv(\"venue_discipline_matrix_with_stats.csv\")\n",
    "print(\"\\nSaved: venue_discipline_matrix_with_stats.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a77fe-945f-4192-a2ed-80fcc746532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load CSV File\n",
    "# -------------------------------\n",
    "csv_path = \"results_combined_prompts_new.csv\"\n",
    "df = pd.read_csv(csv_path, encoding=\"latin1\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Safe JSON Parser\n",
    "# -------------------------------\n",
    "def safe_parse_json(cell):\n",
    "    try:\n",
    "        cell = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "        return json.loads(cell)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Extract and Normalize Fields\n",
    "# -------------------------------\n",
    "discipline_year_counts = defaultdict(lambda: defaultdict(int))\n",
    "subdiscipline_year_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    ont = safe_parse_json(row.get(\"ontology_classification\", \"\"))\n",
    "    year_entry = safe_parse_json(row.get(\"published_year\", \"\"))\n",
    "    year = year_entry.get(\"year\", None)\n",
    "\n",
    "    if year:\n",
    "        try:\n",
    "            year = int(year)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        discipline = ont.get(\"discipline\", \"\").strip()\n",
    "        subdiscipline = ont.get(\"subdiscipline\", \"\").strip()\n",
    "\n",
    "        if subdiscipline == \"Smart/Cell phones\":\n",
    "            subdiscipline = \"Small Scale Device Forensics\"\n",
    "\n",
    "        if discipline:\n",
    "            discipline_year_counts[discipline][year] += 1\n",
    "        if subdiscipline:\n",
    "            subdiscipline_year_counts[subdiscipline][year] += 1\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Convert to DataFrames\n",
    "# -------------------------------\n",
    "discipline_df = pd.DataFrame(discipline_year_counts).fillna(0).astype(int).sort_index()\n",
    "subdiscipline_df = pd.DataFrame(subdiscipline_year_counts).fillna(0).astype(int).sort_index()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Plot Line Graph: Discipline Trends (unchanged)\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "discipline_df.rolling(window=2, min_periods=1).mean().plot()\n",
    "plt.title(\"Discipline Trends in DFRWS (2002–2025)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Papers\")\n",
    "plt.legend(title=\"Discipline\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Plot Line Graph: Subdiscipline Trends (unchanged)\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "subdiscipline_df.rolling(window=2, min_periods=1).mean().plot()\n",
    "plt.title(\"Subdiscipline Trends in DFRWS (2002–2025)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Papers\")\n",
    "plt.legend(title=\"Subdiscipline\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Save ONLY one CSV: Yearly trend for Top 15 subdisciplines\n",
    "# -------------------------------\n",
    "subdiscipline_totals = subdiscipline_df.sum().sort_values(ascending=False)\n",
    "top15_subs = subdiscipline_totals.head(15).index.tolist()\n",
    "\n",
    "sub_top15 = subdiscipline_df[top15_subs].copy()\n",
    "sub_top15.index.name = \"year\"\n",
    "sub_top15.to_csv(\"subdiscipline_yearly_trends_top15.csv\")\n",
    "print(\"Saved: subdiscipline_yearly_trends_top15.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84b458-929d-4496-a6e2-75de013e77bc",
   "metadata": {},
   "source": [
    "Key Domains , All papers and CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e9066-2cf4-4ef5-a330-0eb7e8c68b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, json, re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- config ----------\n",
    "INPUT_CSV = \"results_combined_prompts_new.csv\"\n",
    "YEAR_MIN, YEAR_MAX = 2002, 2025\n",
    "all_years = list(range(YEAR_MIN, YEAR_MAX + 1))\n",
    "\n",
    "# ---------- robust parsers ----------\n",
    "def strip_fences(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    # remove ANY ``` or ```json occurrences, anywhere in the cell\n",
    "    s = re.sub(r\"```(?:json)?\", \"\", s, flags=re.IGNORECASE)\n",
    "    s = s.replace(\"```\", \"\")\n",
    "    return s.strip()\n",
    "\n",
    "def parse_json_messy(cell, default=None):\n",
    "    if isinstance(cell, dict):\n",
    "        return cell\n",
    "    s = strip_fences(cell)\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        # fallback: pull \"year\": \"2007\" with regex if JSON is broken\n",
    "        m = re.search(r'\"year\"\\s*:\\s*\"?(?P<y>\\d{4})\"?', s or \"\")\n",
    "        if m:\n",
    "            return {\"year\": m.group(\"y\")}\n",
    "        return default if default is not None else {}\n",
    "\n",
    "def coerce_year(y):\n",
    "    try:\n",
    "        y = int(str(y).strip())\n",
    "        return y if YEAR_MIN <= y <= YEAR_MAX else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------- rebuild df (year + discipline) ----------\n",
    "raw = pd.read_csv(INPUT_CSV, encoding=\"latin1\")\n",
    "\n",
    "years = raw.get(\"published_year\", pd.Series([\"\"] * len(raw))).apply(parse_json_messy, default={})\n",
    "years = years.apply(lambda d: (d or {}).get(\"year\"))\n",
    "years = years.apply(coerce_year)\n",
    "\n",
    "onto = raw.get(\"ontology_classification\", pd.Series([\"\"] * len(raw))).apply(parse_json_messy, default={})\n",
    "discipline = onto.apply(lambda d: (d or {}).get(\"discipline\", \"\")).fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "df = pd.DataFrame({\"year\": years, \"discipline\": discipline}).dropna(subset=[\"year\"])\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "# ---------- year × discipline matrix (zeros for missing years) ----------\n",
    "counts = (\n",
    "    df.groupby([\"year\", \"discipline\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "mat = (\n",
    "    counts.pivot(index=\"year\", columns=\"discipline\", values=\"count\")\n",
    "          .reindex(all_years, fill_value=0)\n",
    "          .fillna(0).astype(int)\n",
    ")\n",
    "\n",
    "# ---------- CDF-style: ALL papers + Top-5 disciplines ----------\n",
    "# ALL papers per year\n",
    "yearly_all = df.groupby(\"year\").size().reindex(all_years, fill_value=0)\n",
    "\n",
    "# Top-5 disciplines by total volume\n",
    "top5 = mat.sum(axis=0).sort_values(ascending=False).head(6).index.tolist()\n",
    "print(\"Top-5 disciplines:\", top5)\n",
    "\n",
    "# Yearly counts table for ALL + Top-5\n",
    "cdf_counts = pd.DataFrame({\"ALL Papers\": yearly_all})\n",
    "for d in top5:\n",
    "    cdf_counts[d] = mat[d].reindex(all_years, fill_value=0)\n",
    "\n",
    "# Cumulative sums (CDF curves)\n",
    "cdf_cum = cdf_counts.cumsum()\n",
    "\n",
    "# Plot cumulative counts\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in cdf_cum.columns:\n",
    "    plt.plot(cdf_cum.index, cdf_cum[col], linewidth=2.2, label=col)\n",
    "plt.title(\"Cumulative Papers by Year (ALL + Top-6 Disciplines)\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Cumulative Count\")\n",
    "plt.xticks(all_years, rotation=45); plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend(title=\"Series\", loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# (Optional) Normalized CDFs (0..1) to compare timing of growth\n",
    "cdf_norm = cdf_cum.div(cdf_cum.iloc[-1])\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in cdf_norm.columns:\n",
    "    plt.plot(cdf_norm.index, cdf_norm[col], linewidth=2.0, label=col)\n",
    "plt.title(\"Normalized CDF (Proportion Reached by Year)\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Proportion of Final Total\"); plt.ylim(0, 1.02)\n",
    "plt.xticks(all_years, rotation=45); plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend(title=\"Series\", loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Exports\n",
    "cdf_counts.to_csv(\"cdf_all_plus_top6_yearly_counts.csv\")\n",
    "cdf_cum.to_csv(\"cdf_all_plus_top6_cumulative_counts.csv\")\n",
    "cdf_norm.to_csv(\"cdf_all_plus_top6_normalized_cdf.csv\")\n",
    "print(\"Saved CDF CSVs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34242d2f-085f-4fc0-9805-a876f89c6229",
   "metadata": {},
   "source": [
    "SCHOOL AFFILIATION(ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8cec5-c7d7-4738-a0d2-ebfd9c5e9151",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "csv_path = \"results_combined_prompts_new.csv\"  # Ensure this file is in the same folder\n",
    "year_min, year_max = 2001, 2025               # Inclusive range\n",
    "\n",
    "# Helpers\n",
    "def safe_parse_json(cell):\n",
    "    \"\"\"\n",
    "    Parse cells that should contain JSON.\n",
    "    Strips code fences like ```json ... ``` if present.\n",
    "    Returns {} on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def is_year_in_range(entry, lo=year_min, hi=year_max):\n",
    "    try:\n",
    "        year = int(entry.get(\"year\", \"0\"))\n",
    "        return lo <= year <= hi\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_dfrws_any(entry):\n",
    "    \"\"\"\n",
    "    Match ALL DFRWS venues (USA, EU, APAC, etc.)\n",
    "    Expects parsed conference like {\"conference\": \"DFRWS USA\"}.\n",
    "    \"\"\"\n",
    "    conf = str(entry.get(\"conference\", \"\")).strip()\n",
    "    return conf.startswith(\"DFRWS\")\n",
    "\n",
    "\n",
    "# Load\n",
    "try:\n",
    "    df = pd.read_csv(csv_path, encoding=\"latin1\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'results_combined_prompts_new.csv' not found.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Parse columns expected to be JSON blobs\n",
    "df[\"school_names_parsed\"] = df[\"school/organization_names\"].apply(safe_parse_json)\n",
    "df[\"authors_parsed\"]      = df[\"authors\"].apply(safe_parse_json)\n",
    "df[\"countries_parsed\"]    = df[\"author_countries\"].apply(safe_parse_json)\n",
    "df[\"year_parsed\"]         = df[\"published_year\"].apply(safe_parse_json)\n",
    "df[\"conference_parsed\"]   = df[\"conference\"].apply(safe_parse_json)\n",
    "\n",
    "# Filter: ALL DFRWS venues + year range (USA filter removed)\n",
    "mask = (\n",
    "    df[\"conference_parsed\"].apply(is_dfrws_any) &\n",
    "    df[\"year_parsed\"].apply(is_year_in_range)\n",
    ")\n",
    "filtered_df = df[mask].copy()\n",
    "\n",
    "# Counters\n",
    "school_counter     = Counter()\n",
    "author_counter     = Counter()\n",
    "country_counter    = Counter()\n",
    "conference_counter = Counter()   # e.g., DFRWS USA vs DFRWS EU, etc.\n",
    "\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # --- Conferences (string) ---\n",
    "    conf = str(row[\"conference_parsed\"].get(\"conference\", \"\")).strip()\n",
    "    if conf:\n",
    "        conference_counter[conf] += 1\n",
    "\n",
    "    # --- Schools (list) ---\n",
    "    schools = row[\"school_names_parsed\"].get(\"school_names\", [])\n",
    "    for school in schools:\n",
    "        s = str(school).strip()\n",
    "        if s:\n",
    "            school_counter[s] += 1\n",
    "\n",
    "    # --- Authors (comma-separated string inside JSON) ---\n",
    "    authors_raw = row[\"authors_parsed\"].get(\"authors\", \"\")\n",
    "    authors = [a.strip() for a in str(authors_raw).split(\",\") if a.strip()]\n",
    "    for author in authors:\n",
    "        author_counter[author] += 1\n",
    "\n",
    "    # --- Countries (list) ---\n",
    "    countries = row[\"countries_parsed\"].get(\"author_countries\", [])\n",
    "    for c in countries:\n",
    "        cc = str(c).strip()\n",
    "        if cc:\n",
    "            country_counter[cc] += 1\n",
    "\n",
    "# DataFrames\n",
    "top_schools_df = (\n",
    "    pd.DataFrame(school_counter.items(), columns=[\"School\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "top_authors_df = (\n",
    "    pd.DataFrame(author_counter.items(), columns=[\"Author\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "top_countries_df = (\n",
    "    pd.DataFrame(country_counter.items(), columns=[\"Country\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "conference_df = (\n",
    "    pd.DataFrame(conference_counter.items(), columns=[\"Conference\", \"PaperCount\"])\n",
    "    .sort_values(by=\"PaperCount\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n=== DFRWS Venue Breakdown (2002–2025) ===\")\n",
    "for _, r in conference_df.iterrows():\n",
    "    print(f\"{r['Conference']}: {r['PaperCount']} papers\")\n",
    "\n",
    "print(\"\\n=== Top School Affiliations (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_schools_df.iterrows():\n",
    "    print(f\"{r['School']}: {r['Count']}\")\n",
    "\n",
    "print(\"\\n=== Most Frequent Authors (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_authors_df.iterrows():\n",
    "    print(f\"{r['Author']}: {r['Count']}\")\n",
    "\n",
    "print(\"\\n=== Countries in Author Affiliations (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_countries_df.iterrows():\n",
    "    print(f\"{r['Country']}: {r['Count']}\")\n",
    "\n",
    "print(f\"\\n📄 Total Number of Matching Papers: {len(filtered_df)}\")\n",
    "\n",
    "# Save CSVs (neutral filenames)\n",
    "top_schools_df.to_csv(\"top_schools_dfrws_2002_2025.csv\", index=False)\n",
    "top_authors_df.to_csv(\"top_authors_dfrws_2002_2025.csv\", index=False)\n",
    "top_countries_df.to_csv(\"top_countries_dfrws_2002_2025.csv\", index=False)\n",
    "conference_df.to_csv(\"dfrws_venue_breakdown_2002_2025.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8d304-3cd9-4a37-b00c-f2629aaae416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Config\n",
    "csv_path = \"results_combined_prompts_new.csv\"  # Ensure this file is in the same folder\n",
    "year_min, year_max = 2001, 2025               # Inclusive range\n",
    "TOP_N = 10                                    # <-- Top 10 only\n",
    "\n",
    "# Helpers\n",
    "def safe_parse_json(cell):\n",
    "    \"\"\"\n",
    "    Parse cells that should contain JSON.\n",
    "    Strips code fences like ```json ... ``` if present.\n",
    "    Returns {} on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def is_year_in_range(entry, lo=year_min, hi=year_max):\n",
    "    try:\n",
    "        year = int(entry.get(\"year\", \"0\"))\n",
    "        return lo <= year <= hi\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_dfrws_any(entry):\n",
    "    \"\"\"\n",
    "    Match ALL DFRWS venues (USA, EU, APAC, etc.)\n",
    "    Expects parsed conference like {\"conference\": \"DFRWS USA\"}.\n",
    "    \"\"\"\n",
    "    conf = str(entry.get(\"conference\", \"\")).strip()\n",
    "    return conf.startswith(\"DFRWS\")\n",
    "\n",
    "# Load\n",
    "try:\n",
    "    df = pd.read_csv(csv_path, encoding=\"latin1\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'results_combined_prompts_new.csv' not found.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Parse columns expected to be JSON blobs\n",
    "df[\"school_names_parsed\"] = df[\"school/organization_names\"].apply(safe_parse_json)\n",
    "df[\"authors_parsed\"]      = df[\"authors\"].apply(safe_parse_json)\n",
    "df[\"countries_parsed\"]    = df[\"author_countries\"].apply(safe_parse_json)\n",
    "df[\"year_parsed\"]         = df[\"published_year\"].apply(safe_parse_json)\n",
    "df[\"conference_parsed\"]   = df[\"conference\"].apply(safe_parse_json)\n",
    "\n",
    "# Filter: ALL DFRWS venues + year range (no USA-only filter)\n",
    "mask = (\n",
    "    df[\"conference_parsed\"].apply(is_dfrws_any) &\n",
    "    df[\"year_parsed\"].apply(is_year_in_range)\n",
    ")\n",
    "filtered_df = df[mask].copy()\n",
    "\n",
    "# Counters\n",
    "school_counter     = Counter()\n",
    "author_counter     = Counter()\n",
    "country_counter    = Counter()\n",
    "conference_counter = Counter()   # e.g., DFRWS USA vs DFRWS EU, etc.\n",
    "\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # --- Conferences (string) ---\n",
    "    conf = str(row[\"conference_parsed\"].get(\"conference\", \"\")).strip()\n",
    "    if conf:\n",
    "        conference_counter[conf] += 1\n",
    "\n",
    "    # --- Schools (list) ---\n",
    "    schools = row[\"school_names_parsed\"].get(\"school_names\", [])\n",
    "    for school in schools:\n",
    "        s = str(school).strip()\n",
    "        if s:\n",
    "            school_counter[s] += 1\n",
    "\n",
    "    # --- Authors (comma-separated string inside JSON) ---\n",
    "    authors_raw = row[\"authors_parsed\"].get(\"authors\", \"\")\n",
    "    authors = [a.strip() for a in str(authors_raw).split(\",\") if a.strip()]\n",
    "    for author in authors:\n",
    "        author_counter[author] += 1\n",
    "\n",
    "    # --- Countries (list) ---\n",
    "    countries = row[\"countries_parsed\"].get(\"author_countries\", [])\n",
    "    for c in countries:\n",
    "        cc = str(c).strip()\n",
    "        if cc:\n",
    "            country_counter[cc] += 1\n",
    "\n",
    "# DataFrames (full), then restrict to Top N\n",
    "schools_df_all = (\n",
    "    pd.DataFrame(school_counter.items(), columns=[\"School\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "authors_df_all = (\n",
    "    pd.DataFrame(author_counter.items(), columns=[\"Author\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "countries_df_all = (\n",
    "    pd.DataFrame(country_counter.items(), columns=[\"Country\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "conference_df_all = (\n",
    "    pd.DataFrame(conference_counter.items(), columns=[\"Conference\", \"PaperCount\"])\n",
    "    .sort_values(by=\"PaperCount\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "top_schools_df   = schools_df_all.head(TOP_N)\n",
    "top_authors_df   = authors_df_all.head(TOP_N)\n",
    "top_countries_df = countries_df_all.head(TOP_N)\n",
    "top_conference_df = conference_df_all.head(TOP_N)\n",
    "\n",
    "# Output (formatted to avoid \"USA4\" run-ons)\n",
    "# \n",
    "print(f\"\\n=== DFRWS Venue Breakdown (2002–2025) — Top {TOP_N} ===\")\n",
    "for _, r in top_conference_df.iterrows():\n",
    "    print(f\"{r['Conference']}: {r['PaperCount']} papers\")\n",
    "\n",
    "print(f\"\\n=== Top {TOP_N} School Affiliations (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_schools_df.iterrows():\n",
    "    print(f\"{r['School']}: {r['Count']}\")\n",
    "\n",
    "print(f\"\\n=== Top {TOP_N} Most Frequent Authors (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_authors_df.iterrows():\n",
    "    print(f\"{r['Author']}: {r['Count']}\")\n",
    "\n",
    "print(f\"\\n=== Top {TOP_N} Countries in Author Affiliations (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_countries_df.iterrows():\n",
    "    print(f\"{r['Country']}: {r['Count']}\")\n",
    "\n",
    "print(f\"\\n Total Number of Matching Papers: {len(filtered_df)}\")\n",
    "\n",
    "# Save CSVs (Top N only)\n",
    "top_schools_df.to_csv(f\"top_{TOP_N}_schools_dfrws_2002_2025.csv\", index=False)\n",
    "top_authors_df.to_csv(f\"top_{TOP_N}_authors_dfrws_2002_2025.csv\", index=False)\n",
    "top_countries_df.to_csv(f\"top_{TOP_N}_countries_dfrws_2002_2025.csv\", index=False)\n",
    "top_conference_df.to_csv(f\"top_{TOP_N}_dfrws_venue_breakdown_2002_2025.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e7719-dfd9-450c-b9b2-7bbd010762a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DFRWS ANALYSIS — ONE GROUPED+STACKED GRAPH (Top-10 global countries + Other)\n",
    "+ prints/saves the per-year LONG table for LaTeX\n",
    "+ Top-10 tables + Per-conference Top-10 (with Other)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Config\n",
    "\n",
    "CSV_PATH = \"results_combined_prompts_new.csv\"\n",
    "OUTDIR = \"dfrws_outputs_grouped\"\n",
    "YEAR_MIN, YEAR_MAX = 2002, 2025\n",
    "\n",
    "TOP_N_LISTS = 10              # Top-N for schools/authors/countries tables\n",
    "TOP_N_COUNTRIES_GLOBAL = 10   # EXACTLY top-10 countries in stacks; others -> \"Other\"\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def safe_parse_json(cell):\n",
    "    try:\n",
    "        cleaned = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def is_year_in_range(entry, lo=YEAR_MIN, hi=YEAR_MAX):\n",
    "    try:\n",
    "        year = int(entry.get(\"year\", \"0\"))\n",
    "        return lo <= year <= hi\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_dfrws_any(entry):\n",
    "    conf = str(entry.get(\"conference\", \"\")).strip()\n",
    "    return conf.startswith(\"DFRWS\")\n",
    "\n",
    "# Normalize country names to avoid splitting (UK vs United Kingdom, etc.)\n",
    "ALIASES = {\n",
    "    # USA\n",
    "    \"united states\": \"USA\", \"united states of america\": \"USA\",\n",
    "    \"u.s.\": \"USA\", \"u.s.a.\": \"USA\", \"us\": \"USA\", \"u.s\": \"USA\", \"usa\": \"USA\",\n",
    "    # UK\n",
    "    \"uk\": \"United Kingdom\", \"u.k.\": \"United Kingdom\",\n",
    "    \"england\": \"United Kingdom\", \"scotland\": \"United Kingdom\",\n",
    "    \"wales\": \"United Kingdom\", \"great britain\": \"United Kingdom\",\n",
    "    \"united kingdom\": \"United Kingdom\",\n",
    "    # South Korea\n",
    "    \"republic of korea\": \"South Korea\",\n",
    "    \"korea, republic of\": \"South Korea\",\n",
    "    \"korea (south)\": \"South Korea\",\n",
    "    # ISO-2 shortcuts & variants\n",
    "    \"de\": \"Germany\", \"gb\": \"United Kingdom\", \"kr\": \"South Korea\",\n",
    "    \"ch\": \"Switzerland\", \"ie\": \"Ireland\", \"ca\": \"Canada\",\n",
    "    \"au\": \"Australia\", \"nl\": \"Netherlands\",\n",
    "    \"the netherlands\": \"Netherlands\",\n",
    "    \"deutschland\": \"Germany\",\n",
    "}\n",
    "\n",
    "def canon_country(c):\n",
    "    key = str(c).strip().lower()\n",
    "    return ALIASES.get(key, str(c).strip())\n",
    "\n",
    "def sort_confs(confs):\n",
    "    def key_fn(s):\n",
    "        s_low = s.lower()\n",
    "        if \"usa\" in s_low:  return (0, s_low)\n",
    "        if \"eu\" in s_low:   return (1, s_low)\n",
    "        if \"apac\" in s_low: return (2, s_low)\n",
    "        return (3, s_low)\n",
    "    return sorted(confs, key=key_fn)\n",
    "\n",
    "def _safe_name(s):\n",
    "    return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", str(s))\n",
    "\n",
    "\n",
    "# Load & Parse\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH, encoding=\"latin1\")\n",
    "except FileNotFoundError:\n",
    "    raise SystemExit(f\"ERROR: '{CSV_PATH}' not found.\")\n",
    "\n",
    "df[\"school_names_parsed\"] = df[\"school/organization_names\"].apply(safe_parse_json)\n",
    "df[\"authors_parsed\"]      = df[\"authors\"].apply(safe_parse_json)\n",
    "df[\"countries_parsed\"]    = df[\"author_countries\"].apply(safe_parse_json)\n",
    "df[\"year_parsed\"]         = df[\"published_year\"].apply(safe_parse_json)\n",
    "df[\"conference_parsed\"]   = df[\"conference\"].apply(safe_parse_json)\n",
    "\n",
    "mask = (\n",
    "    df[\"conference_parsed\"].apply(is_dfrws_any) &\n",
    "    df[\"year_parsed\"].apply(is_year_in_range)\n",
    ")\n",
    "filtered = df[mask].copy()\n",
    "\n",
    "\n",
    "# Top lists + raw records\n",
    "\n",
    "school_counter  = Counter()\n",
    "author_counter  = Counter()\n",
    "country_counter = Counter()\n",
    "\n",
    "records = []  # (year, conference, country)\n",
    "\n",
    "for _, row in filtered.iterrows():\n",
    "    # schools\n",
    "    for s in row[\"school_names_parsed\"].get(\"school_names\", []):\n",
    "        s = str(s).strip()\n",
    "        if s:\n",
    "            school_counter[s] += 1\n",
    "    # authors\n",
    "    authors_raw = row[\"authors_parsed\"].get(\"authors\", \"\")\n",
    "    for a in [x.strip() for x in str(authors_raw).split(\",\") if x.strip()]:\n",
    "        author_counter[a] += 1\n",
    "    # countries (+ capture conf/year)\n",
    "    year = row[\"year_parsed\"].get(\"year\", None)\n",
    "    try:\n",
    "        year = int(year)\n",
    "    except Exception:\n",
    "        year = None\n",
    "    conf = str(row[\"conference_parsed\"].get(\"conference\", \"\")).strip()\n",
    "    for c in row[\"countries_parsed\"].get(\"author_countries\", []):\n",
    "        cc = canon_country(c)\n",
    "        if cc:\n",
    "            country_counter[cc] += 1\n",
    "            if year is not None and conf:\n",
    "                records.append((year, conf, cc))\n",
    "\n",
    "# Save Top-10 CSVs\n",
    "pd.DataFrame(school_counter.items(), columns=[\"School\",\"Count\"]) \\\n",
    "  .sort_values(\"Count\", ascending=False).head(TOP_N_LISTS) \\\n",
    "  .to_csv(os.path.join(OUTDIR, f\"top_{TOP_N_LISTS}_schools_dfrws_{YEAR_MIN}_{YEAR_MAX}.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame(author_counter.items(), columns=[\"Author\",\"Count\"]) \\\n",
    "  .sort_values(\"Count\", ascending=False).head(TOP_N_LISTS) \\\n",
    "  .to_csv(os.path.join(OUTDIR, f\"top_{TOP_N_LISTS}_authors_dfrws_{YEAR_MIN}_{YEAR_MAX}.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame(country_counter.items(), columns=[\"Country\",\"Count\"]) \\\n",
    "  .sort_values(\"Count\", ascending=False).head(TOP_N_LISTS) \\\n",
    "  .to_csv(os.path.join(OUTDIR, f\"top_{TOP_N_LISTS}_countries_dfrws_{YEAR_MIN}_{YEAR_MAX}.csv\"), index=False)\n",
    "\n",
    "print(f\"Saved Top-{TOP_N_LISTS} tables to: {OUTDIR}\")\n",
    "\n",
    "# Build counts for grouped+stacked chart\n",
    "\n",
    "if not records:\n",
    "    raise SystemExit(\"No (year, conference, country) records found after filtering.\")\n",
    "\n",
    "counts = pd.DataFrame(records, columns=[\"year\",\"conference\",\"country\"]) \\\n",
    "           .groupby([\"year\",\"conference\",\"country\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "years = sorted([y for y in counts[\"year\"].unique() if YEAR_MIN <= y <= YEAR_MAX])\n",
    "confs = sort_confs(counts[\"conference\"].unique())\n",
    "\n",
    "# ----- EXACTLY Top-10 global countries; rest = \"Other\" -----\n",
    "global_totals = counts.groupby(\"country\")[\"count\"].sum().sort_values(ascending=False)\n",
    "named_countries = list(global_totals.head(TOP_N_COUNTRIES_GLOBAL).index)\n",
    "\n",
    "counts[\"country\"] = counts[\"country\"].where(counts[\"country\"].isin(named_countries), other=\"Other\")\n",
    "counts = counts.groupby([\"year\",\"conference\",\"country\"])[\"count\"].sum().reset_index()\n",
    "\n",
    "stack_cols = named_countries + ([\"Other\"] if \"Other\" in counts[\"country\"].values else [])\n",
    "\n",
    "# Build per-conference pivot with consistent columns\n",
    "by_conf = {}\n",
    "for conf in confs:\n",
    "    pivot = counts[counts[\"conference\"] == conf].pivot(index=\"year\", columns=\"country\", values=\"count\")\n",
    "    pivot = pivot.reindex(years).fillna(0.0)\n",
    "    for c in stack_cols:\n",
    "        if c not in pivot.columns:\n",
    "            pivot[c] = 0.0\n",
    "    pivot = pivot[stack_cols]\n",
    "    by_conf[conf] = pivot\n",
    "\n",
    "# -------------------------\n",
    "# SAVE & PRINT the per-year data for LaTeX\n",
    "# -------------------------\n",
    "counts_long = counts.sort_values([\"year\", \"conference\", \"country\"]).copy()\n",
    "counts_long_path = os.path.join(OUTDIR, f\"counts_top10_other_LONG_{YEAR_MIN}_{YEAR_MAX}.csv\")\n",
    "counts_long.to_csv(counts_long_path, index=False)\n",
    "\n",
    "for conf, pivot in by_conf.items():\n",
    "    out = pivot.reset_index()  # columns: year + each of the 10 countries + Other\n",
    "    out_path = os.path.join(OUTDIR, f\"{_safe_name(conf)}_year_by_country_top10_other_{YEAR_MIN}_{YEAR_MAX}.csv\")\n",
    "    out.to_csv(out_path, index=False)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "print(\"\\n DATA (per-year, after Top-10 + Other mapping) \")\n",
    "print(counts_long.to_string(index=False))\n",
    "print(\"\\n[INFO] Country stack order (legend order in the figure):\")\n",
    "print(\", \".join(stack_cols))\n",
    "print(f\"\\n[WROTE] {counts_long_path}\")\n",
    "for conf in by_conf:\n",
    "    print(f\"[WROTE] {os.path.join(OUTDIR, f'{_safe_name(conf)}_year_by_country_top10_other_{YEAR_MIN}_{YEAR_MAX}.csv')}\")\n",
    "\n",
    "# Plot: ONE grouped+stacked figure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))  # single plot; default matplotlib colors\n",
    "\n",
    "n_conf = len(confs)\n",
    "group_width = 0.8\n",
    "bar_w = group_width / max(n_conf, 1)\n",
    "\n",
    "x = np.arange(len(years))  # year positions\n",
    "country_handles = []\n",
    "\n",
    "HATCHES = [\"\", \"///\", \"\\\\\\\\\", \"xx\", \"...\", \"++\", \"oo\", \"**\", \"||\", \"--\"]\n",
    "\n",
    "for i, conf in enumerate(confs):\n",
    "    x_offset = x - (group_width/2) + (i + 0.5)*bar_w\n",
    "    bottom = np.zeros(len(years))\n",
    "    first_conf_for_country_legend = (i == 0)\n",
    "\n",
    "    for j, c in enumerate(stack_cols):\n",
    "        vals = by_conf[conf][c].values\n",
    "        bars = ax.bar(\n",
    "            x_offset, vals, bottom=bottom, width=bar_w,\n",
    "            label=(c if first_conf_for_country_legend else None),\n",
    "            hatch=HATCHES[i % len(HATCHES)], edgecolor=\"black\"\n",
    "        )\n",
    "        if first_conf_for_country_legend and j == 0:\n",
    "            country_handles = []\n",
    "        if first_conf_for_country_legend:\n",
    "            country_handles.append(bars[0])\n",
    "        bottom = bottom + vals\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(years, rotation=45, ha=\"right\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"DFRWS Authors by Country per Year (Grouped by Conference; Top {TOP_N_COUNTRIES_GLOBAL} + Other)\")\n",
    "\n",
    "# Legends\n",
    "country_labels = stack_cols\n",
    "leg1 = ax.legend(country_handles, country_labels, loc=\"upper center\",\n",
    "                 bbox_to_anchor=(0.5, -0.12), ncol=min(6, len(country_labels)),\n",
    "                 frameon=False, fontsize=\"small\")\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "conf_patches = [Patch(facecolor=\"white\", edgecolor=\"black\",\n",
    "                      hatch=HATCHES[i % len(HATCHES)], label=conf)\n",
    "                for i, conf in enumerate(confs)]\n",
    "leg2 = ax.legend(handles=conf_patches, loc=\"upper left\", bbox_to_anchor=(0.0, 1.02),\n",
    "                 ncol=min(4, len(conf_patches)), frameon=False, fontsize=\"small\")\n",
    "ax.add_artist(leg1)\n",
    "\n",
    "fig.tight_layout()\n",
    "one_graph_path = os.path.join(\n",
    "    OUTDIR,\n",
    "    f\"grouped_stacked_countries_by_year_top{TOP_N_COUNTRIES_GLOBAL}_{YEAR_MIN}_{YEAR_MAX}.png\"\n",
    ")\n",
    "fig.savefig(one_graph_path, dpi=300)\n",
    "plt.close(fig)\n",
    "print(f\"[OK] Saved ONE grouped+stacked figure: {one_graph_path}\")\n",
    "\n",
    "\n",
    "# ALL conferences: Top-10 countries tables (LONG + WIDE) with \"Other\"\n",
    "\n",
    "raw_counts = pd.DataFrame(records, columns=[\"year\",\"conference\",\"country\"])\n",
    "totals_by_conf_country = raw_counts.groupby([\"conference\", \"country\"]).size().reset_index(name=\"total_count\")\n",
    "\n",
    "per_conf_rows = []\n",
    "for conf, g in totals_by_conf_country.groupby(\"conference\"):\n",
    "    g = g.sort_values(\"total_count\", ascending=False).reset_index(drop=True)\n",
    "    named = g.head(TOP_N_COUNTRIES_GLOBAL).copy()\n",
    "    other_count = int(g[\"total_count\"].iloc[TOP_N_COUNTRIES_GLOBAL:].sum()) if len(g) > TOP_N_COUNTRIES_GLOBAL else 0\n",
    "    if other_count > 0:\n",
    "        named = pd.concat([named, pd.DataFrame([{\"conference\": conf, \"country\": \"Other\", \"total_count\": other_count}])],\n",
    "                          ignore_index=True)\n",
    "    total_conf = int(g[\"total_count\"].sum())\n",
    "    named[\"rank\"] = range(1, len(named) + 1)\n",
    "    named[\"share\"] = (named[\"total_count\"] / total_conf).round(4) if total_conf > 0 else 0.0\n",
    "    per_conf_rows.append(named)\n",
    "\n",
    "top_long = pd.concat(per_conf_rows, ignore_index=True).sort_values([\"conference\", \"rank\"])\n",
    "\n",
    "long_path = os.path.join(OUTDIR, f\"top_countries_per_conference_LONG_top{TOP_N_COUNTRIES_GLOBAL}_{YEAR_MIN}_{YEAR_MAX}.csv\")\n",
    "top_long.to_csv(long_path, index=False)\n",
    "\n",
    "rows = []\n",
    "for conf, g in top_long.groupby(\"conference\"):\n",
    "    g = g.sort_values(\"rank\")\n",
    "    row = {\"conference\": conf}\n",
    "    for i, r in enumerate(g.itertuples(index=False), start=1):\n",
    "        row[f\"country_{i}\"] = r.country\n",
    "        row[f\"count_{i}\"]   = int(r.total_count)\n",
    "        row[f\"share_{i}\"]   = float(r.share)\n",
    "    rows.append(row)\n",
    "top_wide = pd.DataFrame(rows)\n",
    "\n",
    "wide_path = os.path.join(OUTDIR, f\"top_countries_per_conference_WIDE_top{TOP_N_COUNTRIES_GLOBAL}_{YEAR_MIN}_{YEAR_MAX}.csv\")\n",
    "top_wide.to_csv(wide_path, index=False)\n",
    "\n",
    "print(f\"[OK] Saved per-conference Top-{TOP_N_COUNTRIES_GLOBAL} country tables (LONG): {long_path}\")\n",
    "print(f\"[OK] Saved per-conference Top-{TOP_N_COUNTRIES_GLOBAL} country tables (WIDE): {wide_path}\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f6b35-76c4-44e8-9a49-071511eac429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# ======== CONFIG ========\n",
    "CSV_PATH = \"results_combined_prompts_new.csv\"   # your file\n",
    "TARGET_COL = \"forensic_vs_anti\"\n",
    "# ========================\n",
    "\n",
    "# Robust read: let pandas infer comma/tab, handle quoted newlines\n",
    "df = pd.read_csv(\n",
    "    CSV_PATH,\n",
    "    sep=None,\n",
    "    engine=\"python\",\n",
    "    dtype=str,\n",
    "    keep_default_na=False,\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "\n",
    "\n",
    "# Clean column names only for safety (preserve spaces like \"Paper Title\")\n",
    "df.columns = [c.replace(\"\\u00A0\", \" \").strip() for c in df.columns]\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise KeyError(f\"Column '{TARGET_COL}' not found. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# --- helpers ---\n",
    "CODE_FENCE_RE = re.compile(r\"^\\s*```(?:json)?\\s*|\\s*```\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return CODE_FENCE_RE.sub(\"\", s).strip()\n",
    "\n",
    "def parse_json_cell(cell):\n",
    "    raw = strip_code_fences(cell)\n",
    "    if not raw:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def normalize_forensic_type(val):\n",
    "    if not val:\n",
    "        return None\n",
    "    v = str(val).strip().lower()\n",
    "    if v == \"forensic\" or v in {\"digital forensic\", \"digital forensics\"}:\n",
    "        return \"Digital Forensic\"\n",
    "    if v in {\"anti-forensic\", \"antiforensic\", \"anti forensic\"}:\n",
    "        return \"Anti-Forensic\"\n",
    "    return None  # treat anything else as unparseable/unknown\n",
    "\n",
    "# --- parse + count ---\n",
    "types = (\n",
    "    df[TARGET_COL]\n",
    "      .apply(parse_json_cell)\n",
    "      .apply(lambda d: normalize_forensic_type(d.get(\"forensic_type\")))\n",
    ")\n",
    "\n",
    "counts = Counter(t for t in types if t)\n",
    "\n",
    "# Print EXACTLY what you asked for\n",
    "print(\"Digital Forensic:\", counts.get(\"Digital Forensic\", 0))\n",
    "print(\"Anti-Forensic:   \", counts.get(\"Anti-Forensic\", 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587edb1-790c-4aad-aa86-57b701d1dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter  # (not strictly needed, but ok to keep)\n",
    "import os\n",
    "\n",
    "# ========= CONFIG =========\n",
    "CSV_PATH = \"results_combined_prompts_new.csv\"\n",
    "OUTDIR = \"dfrws_outputs_dfvsaf_trends\"\n",
    "FILTER_DFRWS_ONLY = True   # set False if you want all venues\n",
    "INCLUDE_UNKNOWN = False    # set True to keep unknown/missing DF/AF labels\n",
    "MAKE_PLOT = False          # set True to also save a simple matplotlib plot\n",
    "# =========================\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# --- helpers (same pattern as before) ---\n",
    "CODE_FENCE_RE = re.compile(r\"^\\s*```(?:json)?\\s*|\\s*```\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return CODE_FENCE_RE.sub(\"\", s).strip()\n",
    "\n",
    "def parse_json_cell(cell):\n",
    "    raw = strip_code_fences(cell)\n",
    "    if not raw:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def normalize_forensic_type(val):\n",
    "    if not val:\n",
    "        return None\n",
    "    v = str(val).strip().lower()\n",
    "    if v == \"forensic\" or v in {\"digital forensic\", \"digital forensics\"}:\n",
    "        return \"Digital Forensic\"\n",
    "    if v in {\"anti-forensic\", \"antiforensic\", \"anti forensic\"}:\n",
    "        return \"Anti-Forensic\"\n",
    "    return None  # treat anything else as unknown\n",
    "\n",
    "def parse_year(cell):\n",
    "    d = parse_json_cell(cell)\n",
    "    y = d.get(\"year\")\n",
    "    try:\n",
    "        return int(y)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def parse_conf(cell):\n",
    "    d = parse_json_cell(cell)\n",
    "    return str(d.get(\"conference\", \"\")).strip()\n",
    "\n",
    "# --- load (let pandas infer delimiter; handles quoted newlines) ---\n",
    "df = pd.read_csv(\n",
    "    CSV_PATH, sep=None, engine=\"python\", dtype=str, keep_default_na=False, encoding=\"latin1\"\n",
    ")\n",
    "df.columns = [c.replace(\"\\u00A0\", \" \").strip() for c in df.columns]\n",
    "\n",
    "need_cols = [\"published_year\", \"forensic_vs_anti\", \"conference\"]\n",
    "for c in need_cols:\n",
    "    if c not in df.columns:\n",
    "        raise KeyError(f\"Missing column '{c}'. Found: {df.columns.tolist()}\")\n",
    "\n",
    "# --- parse needed fields ---\n",
    "df[\"year\"] = df[\"published_year\"].apply(parse_year)\n",
    "df[\"type\"] = df[\"forensic_vs_anti\"].apply(lambda s: normalize_forensic_type(parse_json_cell(s).get(\"forensic_type\")))\n",
    "df[\"conf\"] = df[\"conference\"].apply(parse_conf)\n",
    "\n",
    "# optional filter: only DFRWS venues\n",
    "if FILTER_DFRWS_ONLY:\n",
    "    df = df[df[\"conf\"].str.startswith(\"DFRWS\", na=False)].copy()\n",
    "\n",
    "if not INCLUDE_UNKNOWN:\n",
    "    df = df[df[\"type\"].isin([\"Digital Forensic\", \"Anti-Forensic\"])].copy()\n",
    "\n",
    "df = df[df[\"year\"].notna()].copy()\n",
    "\n",
    "if df.empty:\n",
    "    raise SystemExit(\"No rows after filtering/parsing.\")\n",
    "\n",
    "# ---------- LONG table: year × type ----------\n",
    "long = (\n",
    "    df.groupby([\"year\", \"type\"])\n",
    "      .size().reset_index(name=\"count\")\n",
    "      .sort_values([\"year\", \"type\"])\n",
    ")\n",
    "long_path = os.path.join(OUTDIR, \"dfvsaf_by_year_LONG.csv\")\n",
    "long.to_csv(long_path, index=False)\n",
    "\n",
    "# ---------- WIDE table (with shares) ----------\n",
    "wide = long.pivot(index=\"year\", columns=\"type\", values=\"count\").fillna(0).astype(int)\n",
    "for col in [\"Digital Forensic\", \"Anti-Forensic\"]:\n",
    "    if col not in wide.columns:\n",
    "        wide[col] = 0\n",
    "wide = wide[[\"Digital Forensic\", \"Anti-Forensic\"]]\n",
    "wide[\"Total\"] = wide.sum(axis=1)\n",
    "wide[\"Digital_Forensic_share\"] = (wide[\"Digital Forensic\"] / wide[\"Total\"]).fillna(0).round(4)\n",
    "wide[\"Anti-Forensic_share\"] = (wide[\"Anti-Forensic\"] / wide[\"Total\"]).fillna(0).round(4)\n",
    "wide_path = os.path.join(OUTDIR, \"dfvsaf_by_year_WIDE.csv\")\n",
    "wide.reset_index().to_csv(wide_path, index=False)\n",
    "\n",
    "# ---------- print to console ----------\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(\"\\n[DF vs AF by Year — LONG]\")\n",
    "print(long.to_string(index=False))\n",
    "print(\"\\n[DF vs AF by Year — WIDE]\")\n",
    "print(wide.reset_index().to_string(index=False))\n",
    "print(f\"\\n[WROTE] {long_path}\")\n",
    "print(f\"[WROTE] {wide_path}\")\n",
    "\n",
    "# ---------- optional plot ----------\n",
    "if MAKE_PLOT:\n",
    "    import matplotlib.pyplot as plt  # no seaborn\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    years = wide.index.tolist()\n",
    "    ax.plot(years, wide[\"Digital Forensic\"].tolist(), label=\"Digital Forensic\")\n",
    "    ax.plot(years, wide[\"Anti-Forensic\"].tolist(), label=\"Anti-Forensic\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"DF vs AF Counts by Year\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plot_path = os.path.join(OUTDIR, \"dfvsaf_by_year_lines.png\")\n",
    "    fig.savefig(plot_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"[WROTE] {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25a6b8-3216-4bfa-9a45-8dfd16e07f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CSV_PATH = \"results_combined_prompts_new.csv\"  \n",
    "\n",
    "# --- helpers ---\n",
    "CODE_FENCE_RE = re.compile(r\"^\\s*```(?:json)?\\s*|\\s*```\\s*$\", re.IGNORECASE)\n",
    "def strip_code_fences(s): return CODE_FENCE_RE.sub(\"\", str(s)).strip()\n",
    "\n",
    "def parse_json(cell):\n",
    "    raw = strip_code_fences(cell)\n",
    "    try: return json.loads(raw) if raw else {}\n",
    "    except Exception: return {}\n",
    "\n",
    "def norm_type(v):\n",
    "    if not v: return None\n",
    "    v = str(v).strip().lower()\n",
    "    if v == \"forensic\" or v in {\"digital forensic\",\"digital forensics\"}:\n",
    "        return \"Digital Forensic\"\n",
    "    if v in {\"anti-forensic\",\"antiforensic\",\"anti forensic\"}:\n",
    "        return \"Anti-Forensic\"\n",
    "    return None\n",
    "\n",
    "def parse_year(cell):\n",
    "    y = parse_json(cell).get(\"year\")\n",
    "    try: return int(y)\n",
    "    except Exception: return None\n",
    "\n",
    "def parse_conf(cell):\n",
    "    return str(parse_json(cell).get(\"conference\",\"\")).strip()\n",
    "\n",
    "# --- load & parse ---\n",
    "df = pd.read_csv(CSV_PATH, sep=None, engine=\"python\", dtype=str, keep_default_na=False, encoding=\"latin1\")\n",
    "df.columns = [c.replace(\"\\u00A0\",\" \").strip() for c in df.columns]\n",
    "\n",
    "df[\"year\"] = df[\"published_year\"].apply(parse_year)\n",
    "df[\"type\"] = df[\"forensic_vs_anti\"].apply(lambda s: norm_type(parse_json(s).get(\"forensic_type\")))\n",
    "df[\"conf\"] = df[\"conference\"].apply(parse_conf)\n",
    "\n",
    "# DFRWS only + drop unknowns/invalid years\n",
    "df = df[df[\"conf\"].str.startswith(\"DFRWS\", na=False)]\n",
    "df = df[df[\"type\"].isin([\"Digital Forensic\",\"Anti-Forensic\"])]\n",
    "df = df[df[\"year\"].notna()]\n",
    "\n",
    "# --- LONG table (year, type, count) ---\n",
    "df_long = (\n",
    "    df.groupby([\"year\",\"type\"])\n",
    "      .size().reset_index(name=\"count\")\n",
    "      .sort_values([\"year\",\"type\"])\n",
    ")\n",
    "\n",
    "# --- WIDE for plotting ---\n",
    "wide = df_long.pivot(index=\"year\", columns=\"type\", values=\"count\").fillna(0).astype(int)\n",
    "for col in [\"Digital Forensic\",\"Anti-Forensic\"]:\n",
    "    if col not in wide.columns: wide[col] = 0\n",
    "wide = wide.reindex(sorted(wide.index))\n",
    "years = wide.index.astype(int).tolist()\n",
    "dfc = wide[\"Digital Forensic\"].to_numpy()\n",
    "afc = wide[\"Anti-Forensic\"].to_numpy()\n",
    "\n",
    "# --------- Figure A: Grouped bars (counts) ---------\n",
    "x = range(len(years)); w = 0.42\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.bar([i - w/2 for i in x], dfc, width=w, label=\"Digital Forensic\")\n",
    "ax.bar([i + w/2 for i in x], afc, width=w, label=\"Anti-Forensic\")\n",
    "\n",
    "# label tiny AF bars\n",
    "for i, v in enumerate(afc):\n",
    "    if v > 0:\n",
    "        ax.text(i + w/2, v + 0.2, str(v), ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(years, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Paper count\")\n",
    "ax.set_title(\"DF vs AF papers by year (DFRWS)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------- Figure B (optional): 100% stacked (shares) ---------\n",
    "wide[\"Total\"] = (wide[\"Digital Forensic\"] + wide[\"Anti-Forensic\"]).replace(0, 1)\n",
    "share_df = wide[[\"Digital Forensic\",\"Anti-Forensic\"]].div(wide[\"Total\"], axis=0)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(12,5))\n",
    "ax2.bar(years, share_df[\"Digital Forensic\"].values, label=\"Digital Forensic\")\n",
    "ax2.bar(years, share_df[\"Anti-Forensic\"].values,\n",
    "        bottom=share_df[\"Digital Forensic\"].values, label=\"Anti-Forensic\")\n",
    "ax2.set_xticks(list(range(len(years))))\n",
    "ax2.set_xticklabels(years, rotation=45, ha=\"right\")\n",
    "ax2.set_ylabel(\"Share\")\n",
    "ax2.set_title(\"DF vs AF shares by year (DFRWS)\")\n",
    "ax2.legend()\n",
    "fig2.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d629606-d7fc-43ef-811f-484d8f2a6dcf",
   "metadata": {},
   "source": [
    "social graph, collaboration landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4bb5eb-168a-46e9-8d13-84eaec3a8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"results_combined_prompts_new.csv\", encoding=\"latin1\")\n",
    "\n",
    "def clean_json(cell):\n",
    "    if pd.isna(cell):\n",
    "        return {}\n",
    "    cell = re.sub(r\"```json|```\", \"\", cell).strip()\n",
    "    try:\n",
    "        return json.loads(cell)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# Clean columns\n",
    "df[\"title_clean\"] = df[\"title\"].apply(lambda x: clean_json(x).get(\"title\"))\n",
    "df[\"authors_clean\"] = df[\"authors\"].apply(lambda x: clean_json(x).get(\"authors\", \"\"))\n",
    "df[\"institutions_clean\"] = df[\"school/organization_names\"].apply(\n",
    "    lambda x: clean_json(x).get(\"school_names\", [])\n",
    ")\n",
    "df[\"countries_clean\"] = df[\"author_countries\"].apply(\n",
    "    lambda x: clean_json(x).get(\"author_countries\", [])\n",
    ")\n",
    "df[\"year\"] = df[\"published_year\"].apply(\n",
    "    lambda x: clean_json(x).get(\"year\")\n",
    ")\n",
    "df[\"conference_clean\"] = df[\"conference\"].apply(\n",
    "    lambda x: clean_json(x).get(\"conference\")\n",
    ")\n",
    "\n",
    "# Create publication ID\n",
    "df[\"pub_id\"] = [f\"P{i}\" for i in range(len(df))]\n",
    "\n",
    "# Split authors\n",
    "author_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    authors = [a.strip() for a in row[\"authors_clean\"].split(\",\") if a.strip()]\n",
    "    for author in authors:\n",
    "        author_rows.append({\n",
    "            \"pub_id\": row[\"pub_id\"],\n",
    "            \"author\": author,\n",
    "            \"year\": row[\"year\"]\n",
    "        })\n",
    "\n",
    "authors_df = pd.DataFrame(author_rows)\n",
    "\n",
    "# Save outputs\n",
    "df[[\"pub_id\", \"title_clean\", \"year\", \"conference_clean\"]].to_csv(\n",
    "    \"publications.csv\", index=False\n",
    ")\n",
    "authors_df.to_csv(\"authorship.csv\", index=False)\n",
    "\n",
    "print(\"✔ Step 1 complete: publications.csv and authorship.csv created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0106e-4adb-444a-9b66-3502d0adba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "df = pd.read_csv(\"results_combined_prompts_new.csv\", encoding=\"latin1\")\n",
    "\n",
    "def clean_json(cell):\n",
    "    if pd.isna(cell):\n",
    "        return {}\n",
    "    cell = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "    try:\n",
    "        return json.loads(cell)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# Clean/parse columns\n",
    "df[\"title_clean\"] = df[\"title\"].apply(lambda x: clean_json(x).get(\"title\"))\n",
    "df[\"authors_clean\"] = df[\"authors\"].apply(lambda x: clean_json(x).get(\"authors\", \"\"))\n",
    "df[\"institutions_clean\"] = df[\"school/organization_names\"].apply(\n",
    "    lambda x: clean_json(x).get(\"school_names\", [])\n",
    ")\n",
    "df[\"countries_clean\"] = df[\"author_countries\"].apply(\n",
    "    lambda x: clean_json(x).get(\"author_countries\", [])\n",
    ")\n",
    "df[\"year\"] = df[\"published_year\"].apply(lambda x: clean_json(x).get(\"year\"))\n",
    "df[\"conference_clean\"] = df[\"conference\"].apply(lambda x: clean_json(x).get(\"conference\"))\n",
    "\n",
    "# Publication IDs\n",
    "df[\"pub_id\"] = [f\"P{i}\" for i in range(len(df))]\n",
    "\n",
    "# --- Build tables ---\n",
    "author_pubcount = defaultdict(int)\n",
    "institution_pubcount = defaultdict(int)\n",
    "country_pubcount = defaultdict(int)\n",
    "\n",
    "# relationships\n",
    "authorship_edges = []\n",
    "affiliation_edges = []          # author -> institution, with year/pub_id\n",
    "inst_country_edges = set()      # institution -> country\n",
    "coauthor_weights = defaultdict(int)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    pub_id = row[\"pub_id\"]\n",
    "    year = row[\"year\"]\n",
    "    authors = [a.strip() for a in str(row[\"authors_clean\"]).split(\",\") if a.strip()]\n",
    "    insts = row[\"institutions_clean\"] if isinstance(row[\"institutions_clean\"], list) else []\n",
    "    countries = row[\"countries_clean\"] if isinstance(row[\"countries_clean\"], list) else []\n",
    "    country = countries[0] if countries else None  # your data is paper-level; take first\n",
    "\n",
    "    # pub counts\n",
    "    for a in authors:\n",
    "        author_pubcount[a] += 1\n",
    "        authorship_edges.append({\"source\": a, \"target\": pub_id, \"type\": \"AUTHORED\", \"year\": year})\n",
    "\n",
    "    for inst in insts:\n",
    "        institution_pubcount[inst] += 1\n",
    "        if country:\n",
    "            country_pubcount[country] += 1\n",
    "            inst_country_edges.add((inst, country))\n",
    "\n",
    "    # author->institution edges (paper-level affiliation)\n",
    "    for a in authors:\n",
    "        for inst in insts:\n",
    "            affiliation_edges.append({\n",
    "                \"source\": a, \"target\": inst, \"type\": \"AFFILIATED_WITH\", \"year\": year, \"pub_id\": pub_id\n",
    "            })\n",
    "\n",
    "    # coauthor weights (derived)\n",
    "    for a, b in combinations(sorted(set(authors)), 2):\n",
    "        coauthor_weights[(a, b)] += 1\n",
    "\n",
    "# --- Nodes ---\n",
    "nodes = []\n",
    "\n",
    "# Author nodes\n",
    "for a, c in author_pubcount.items():\n",
    "    nodes.append({\"id\": a, \"label\": a, \"node_type\": \"Author\", \"pub_count\": c})\n",
    "\n",
    "# Institution nodes\n",
    "for inst, c in institution_pubcount.items():\n",
    "    nodes.append({\"id\": inst, \"label\": inst, \"node_type\": \"Institution\", \"pub_count\": c})\n",
    "\n",
    "# Country nodes\n",
    "for country, c in country_pubcount.items():\n",
    "    nodes.append({\"id\": country, \"label\": country, \"node_type\": \"Country\", \"pub_count\": c})\n",
    "\n",
    "# Publication nodes\n",
    "for _, row in df.iterrows():\n",
    "    nodes.append({\n",
    "        \"id\": row[\"pub_id\"],\n",
    "        \"label\": row[\"title_clean\"],\n",
    "        \"node_type\": \"Publication\",\n",
    "        \"year\": row[\"year\"],\n",
    "        \"conference\": row[\"conference_clean\"]\n",
    "    })\n",
    "\n",
    "nodes_df = pd.DataFrame(nodes).drop_duplicates(subset=[\"id\"])\n",
    "\n",
    "# --- Edges ---\n",
    "edges = []\n",
    "\n",
    "# authorship edges\n",
    "edges.extend([{\n",
    "    \"source\": e[\"source\"], \"target\": e[\"target\"],\n",
    "    \"relationship\": e[\"type\"], \"weight\": 1, \"year\": e[\"year\"]\n",
    "} for e in authorship_edges])\n",
    "\n",
    "# affiliation edges\n",
    "edges.extend([{\n",
    "    \"source\": e[\"source\"], \"target\": e[\"target\"],\n",
    "    \"relationship\": e[\"type\"], \"weight\": 1, \"year\": e[\"year\"], \"pub_id\": e[\"pub_id\"]\n",
    "} for e in affiliation_edges])\n",
    "\n",
    "# institution -> country\n",
    "for inst, country in inst_country_edges:\n",
    "    edges.append({\n",
    "        \"source\": inst, \"target\": country,\n",
    "        \"relationship\": \"LOCATED_IN\", \"weight\": 1\n",
    "    })\n",
    "\n",
    "# coauthor edges (author-author)\n",
    "for (a, b), w in coauthor_weights.items():\n",
    "    edges.append({\n",
    "        \"source\": a, \"target\": b,\n",
    "        \"relationship\": \"COAUTHORED_WITH\", \"weight\": w\n",
    "    })\n",
    "\n",
    "edges_df = pd.DataFrame(edges)\n",
    "\n",
    "# Save\n",
    "nodes_df.to_csv(\"graph_nodes.csv\", index=False)\n",
    "edges_df.to_csv(\"graph_edges.csv\", index=False)\n",
    "\n",
    "print(\"✔ Step 2 complete: graph_nodes.csv and graph_edges.csv created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5d7dc-f3b1-4dbc-9800-800af8f3ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD ORIGINAL CSV\n",
    "# =========================\n",
    "INPUT_FILE = \"results_combined_prompts_new.csv\"\n",
    "df = pd.read_csv(INPUT_FILE, encoding=\"latin1\")\n",
    "\n",
    "\n",
    "def clean_json(cell):\n",
    "    if pd.isna(cell):\n",
    "        return {}\n",
    "    cell = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "    try:\n",
    "        return json.loads(cell)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# =========================\n",
    "# 2. PARSE COLUMNS\n",
    "# =========================\n",
    "df[\"title\"] = df[\"title\"].apply(lambda x: clean_json(x).get(\"title\"))\n",
    "df[\"authors\"] = df[\"authors\"].apply(lambda x: clean_json(x).get(\"authors\", \"\"))\n",
    "df[\"institutions\"] = df[\"school/organization_names\"].apply(\n",
    "    lambda x: clean_json(x).get(\"school_names\", [])\n",
    ")\n",
    "df[\"countries\"] = df[\"author_countries\"].apply(\n",
    "    lambda x: clean_json(x).get(\"author_countries\", [])\n",
    ")\n",
    "df[\"year\"] = df[\"published_year\"].apply(lambda x: clean_json(x).get(\"year\"))\n",
    "df[\"conference\"] = df[\"conference\"].apply(lambda x: clean_json(x).get(\"conference\"))\n",
    "\n",
    "df[\"pub_id\"] = [f\"P{i}\" for i in range(len(df))]\n",
    "\n",
    "# =========================\n",
    "# 3. BUILD COUNTS\n",
    "# =========================\n",
    "author_pubcount = defaultdict(int)\n",
    "coauthor_weights = defaultdict(int)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    authors = [a.strip() for a in row[\"authors\"].split(\",\") if a.strip()]\n",
    "    for a in authors:\n",
    "        author_pubcount[a] += 1\n",
    "    for a, b in combinations(sorted(set(authors)), 2):\n",
    "        coauthor_weights[(a, b)] += 1\n",
    "\n",
    "# =========================\n",
    "# 4. SELECT TOP 10 AUTHORS\n",
    "# =========================\n",
    "top_authors = set(\n",
    "    sorted(author_pubcount, key=author_pubcount.get, reverse=True)[:10]\n",
    ")\n",
    "\n",
    "print(\"Top 10 authors:\")\n",
    "for a in top_authors:\n",
    "    print(a, author_pubcount[a])\n",
    "\n",
    "# =========================\n",
    "# 5. BUILD NODES\n",
    "# =========================\n",
    "nodes = []\n",
    "\n",
    "# Author nodes\n",
    "for author in top_authors:\n",
    "    nodes.append({\n",
    "        \"id\": author,\n",
    "        \"label\": author,\n",
    "        \"node_type\": \"Author\",\n",
    "        \"pub_count\": author_pubcount[author]\n",
    "    })\n",
    "\n",
    "nodes_df = pd.DataFrame(nodes)\n",
    "\n",
    "# =========================\n",
    "# 6. BUILD EDGES (COAUTHOR ONLY)\n",
    "# =========================\n",
    "edges = []\n",
    "\n",
    "for (a, b), w in coauthor_weights.items():\n",
    "    if a in top_authors and b in top_authors:\n",
    "        edges.append({\n",
    "            \"source\": a,\n",
    "            \"target\": b,\n",
    "            \"relationship\": \"COAUTHORED_WITH\",\n",
    "            \"weight\": w\n",
    "        })\n",
    "\n",
    "edges_df = pd.DataFrame(edges)\n",
    "\n",
    "# =========================\n",
    "# 7. SAVE FINAL FILES\n",
    "# =========================\n",
    "nodes_df.to_csv(\"graph_nodes_top10.csv\", index=False)\n",
    "edges_df.to_csv(\"graph_edges_top10.csv\", index=False)\n",
    "\n",
    "print(\"\\n✔ DONE\")\n",
    "print(\"Created:\")\n",
    "print(\" - graph_nodes_top10.csv\")\n",
    "print(\" - graph_edges_top10.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6bc2c-5aaf-40ab-9e53-105e290b3bd1",
   "metadata": {},
   "source": [
    "multiple affiliations over time, social graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b029ec-1e2a-41d6-b101-43465abe5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "INPUT_FILE = \"results_combined_prompts_new.csv\"\n",
    "TOP_N_AUTHORS = 10\n",
    "\n",
    "def clean_json(cell):\n",
    "    if pd.isna(cell):\n",
    "        return {}\n",
    "    cell = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "    try:\n",
    "        return json.loads(cell)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def split_authors(s):\n",
    "    return [a.strip() for a in str(s).split(\",\") if a.strip()]\n",
    "\n",
    "# ---------------- Load ----------------\n",
    "df = pd.read_csv(INPUT_FILE, encoding=\"latin1\")\n",
    "\n",
    "df[\"authors\"] = df[\"authors\"].apply(lambda x: clean_json(x).get(\"authors\", \"\"))\n",
    "df[\"schools\"] = df[\"school/organization_names\"].apply(\n",
    "    lambda x: clean_json(x).get(\"school_names\", [])\n",
    ")\n",
    "\n",
    "# ---------------- Publication counts ----------------\n",
    "author_pubcount = defaultdict(int)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    for a in split_authors(row[\"authors\"]):\n",
    "        author_pubcount[a] += 1\n",
    "\n",
    "top_authors = set(\n",
    "    sorted(author_pubcount, key=author_pubcount.get, reverse=True)[:TOP_N_AUTHORS]\n",
    ")\n",
    "\n",
    "print(\"Top 10 authors:\")\n",
    "for a in sorted(top_authors, key=lambda x: author_pubcount[x], reverse=True):\n",
    "    print(a, author_pubcount[a])\n",
    "\n",
    "# ---------------- Build nodes & edges ----------------\n",
    "nodes = []\n",
    "edge_weights = Counter()\n",
    "institution_nodes = set()\n",
    "\n",
    "# Author nodes\n",
    "for a in top_authors:\n",
    "    nodes.append({\n",
    "        \"id\": a,\n",
    "        \"label\": a,\n",
    "        \"node_type\": \"Author\",\n",
    "        \"pub_count\": author_pubcount[a]\n",
    "    })\n",
    "\n",
    "skipped_rows = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    authors = split_authors(row[\"authors\"])\n",
    "    schools = row[\"schools\"] if isinstance(row[\"schools\"], list) else []\n",
    "\n",
    "    # Pair ONLY when counts match (safe)\n",
    "    if len(authors) == len(schools) and len(authors) > 0:\n",
    "        for a, inst in zip(authors, schools):\n",
    "            a = a.strip()\n",
    "            inst = str(inst).strip()\n",
    "            if a in top_authors and inst:\n",
    "                institution_nodes.add(inst)\n",
    "                edge_weights[(a, inst)] += 1\n",
    "    else:\n",
    "        skipped_rows += 1\n",
    "\n",
    "print(f\"\\nSkipped {skipped_rows} rows where author/school counts didn't match.\")\n",
    "\n",
    "# Institution nodes\n",
    "for inst in institution_nodes:\n",
    "    nodes.append({\n",
    "        \"id\": inst,\n",
    "        \"label\": inst,\n",
    "        \"node_type\": \"Institution\",\n",
    "        \"pub_count\": 0\n",
    "    })\n",
    "\n",
    "# Edges\n",
    "edges = []\n",
    "for (a, inst), w in edge_weights.items():\n",
    "    edges.append({\n",
    "        \"source\": a,\n",
    "        \"target\": inst,\n",
    "        \"relationship\": \"AFFILIATED_WITH\",\n",
    "        \"weight\": int(w)\n",
    "    })\n",
    "\n",
    "nodes_df = pd.DataFrame(nodes).drop_duplicates(subset=[\"id\"])\n",
    "edges_df = pd.DataFrame(edges)\n",
    "\n",
    "nodes_df.to_csv(\"graph_nodes_top10_affiliations_only.csv\", index=False)\n",
    "edges_df.to_csv(\"graph_edges_top10_affiliations_only.csv\", index=False)\n",
    "\n",
    "print(\"\\n✔ Created:\")\n",
    "print(\" - graph_nodes_top10_affiliations_only.csv\")\n",
    "print(\" - graph_edges_top10_affiliations_only.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a474d0a-9e0d-4e4c-85cc-c4daeb204f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30fce0-63f1-4759-b9a0-a1d6b40ec8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
