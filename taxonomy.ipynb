{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849b16d-de6b-45d0-a937-d62a083fe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b782a8-4248-4c78-b650-70f7951691b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import PyPDF2\n",
    "def load_papers_from_jsonl(file_path):\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    paper = json.loads(line.strip())\n",
    "                    papers.append(paper)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping a line due to JSON decoding error.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "\n",
    "    print(f\"Loaded papers from {file_path}.\")\n",
    "    return papers\n",
    "\n",
    "# Specify the path to your JSONL file\n",
    "jsonl_file_path = \"extracted_dfrws_papers_NEWEST_final.jsonl\"\n",
    "\n",
    "# Load papers\n",
    "papers = load_papers_from_jsonl(jsonl_file_path)\n",
    "\n",
    "# Display loaded papers (Optional)\n",
    "for i, paper in enumerate(papers[:5]):  # Limit display to the first 5 papers for readability\n",
    "    print(f\"Paper {i+1}:\")\n",
    "    print(f\"Title: {paper.get('title', 'No title provided')}\")\n",
    "    print(\"Content:\")\n",
    "    print(paper.get('content', 'No content provided')[:500])  # Print first 500 characters of content\n",
    "    print(\"-\" * 50)  # Separator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96aff1c-14ae-4836-8586-696d17575f11",
   "metadata": {},
   "source": [
    "Initial Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4e1a9-3430-4c34-b0cb-c0e87a124f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_metadata_prompt(task, paper, ontology_json=None):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "\n",
    "    if task == \"title\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the full title from the digital forensics paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - The title is usually at the top of the first page or in the first section.\n",
    "        - Extract the title in its entirety.\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"title\": \"Title of the paper here\"\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "       \n",
    "    elif task == \"taxonomy_classification\":\n",
    "         \n",
    "          return f'''\n",
    "          You are tasked with classifying the research paper titled \"{title}\" using the digital forensics taxonomy below.\n",
    "          \n",
    "          Rules & Guidelines:\n",
    "          \\t1- Choose only ONE value for each of the following fields: primary_domain, sub_domain.\n",
    "          \\t2- If no match is found, return \"unknown\" for that field.\n",
    "          \\t3- If a new domain or sub_domain not in the list is clearly found, return it directly.\n",
    "          \\t4- tags MUST be a list of 5–10 short strings.\n",
    "          \\t5- Use the taxonomy below as your starting point. It is not final; you may extend it using rule 3.\n",
    "          \\t6- Object lists are illustrative examples to aid disambiguation; a paper may be classified under a sub_domain even if it does not explicitly mention the listed objects.\n",
    "          \\t7- Artifact types and low-level sub-objects (e.g., heaps, stacks, page tables, registry keys, prefetch files) may be included as tags for Computer Forensics to capture fine-grained technical details and aid disambiguation (e.g., distinguishing physical vs. process memory), \n",
    "          but should not be used as values for primary_domain, sub_domain, or object.\n",
    "         \n",
    "          TAXONOMY (initial):\n",
    "        {{\n",
    "          \"Computer Forensics\": {{\n",
    "          \"Memory Forensics\": {{ \"Physical Memory Forensics\": [  \"Physical RAM\", \"Memory Dump\", \"DMA Artifacts\" ],\n",
    "          \"Process Memory Forensics\": [ \"Heaps\", \"Stacks\",\"Loaded Modules/DLLs\", \"Decrypted Artifacts\"],\n",
    "          \"Kernel Memory Forensics\": [\"Kernel Objects\", \"Drivers/Modules\",\"Kernel Hooks/Rootkits\"],\n",
    "          \"Virtual Address Space Analysis\": [\"Page Tables\",\"Address Translation\",\"Paging Structures\"],\n",
    "          \"Memory Acquisition\": [\"Live Acquisition\",\"Offline Dump Analysis\",\"Acquisition Integrity\"]\n",
    "        }},\n",
    "        \n",
    "        \"Disk / Storage Forensics\": {{ \n",
    "        \"File System Forensics\": [\"NTFS/FAT/ext/APFS\",\"Directories\",\"File Allocation\"],\n",
    "        \"File System Metadata & System Artifacts\": [\"Registry\",\"Journals\",\"Browser Artifacts\",\"System Logs\",\"Temp Artifacts\"],\n",
    "        \"Unallocated Space & File Carving\": [\"Deleted Files\",\"Fragmented Files\",\"Carved Objects\"],\n",
    "        \"Low-Level Sector Forensics\": [\"Sectors\",\"Bad Sectors\",\"Firmware/Controller Areas\"],\n",
    "        \"Disk Imaging & Acquisition\": [\"Imaging Tools\",\"Faulty Sector Handling\",\"Integrity/Hashing\"]\n",
    "        }},\n",
    "\n",
    "        \"System & OS Artifact Forensics\": {{ \n",
    "        \"Operating System State\": [\"Running Services\",\"Loaded Drivers\", \"System Configuration\"],\n",
    "        \"User & Account Artifacts\": [\"User Accounts\",\"Login Sessions\",\"Access Control Data\"],\n",
    "        \"Execution & Activity Artifacts\": [\"Prefetch\",\"ShimCache\",\"Amcache\",\"Recent File Artifacts\"],\n",
    "        \"Application & System Logs\": [\"Event Logs\",\"Audit Logs\",\"Application Logs\", \"E-mail artifacts\"]\n",
    "        }},\n",
    " \n",
    "        \"Vehicle / Automotive Forensics\": {{\n",
    "        \"Automotive Forensics\": [\"Vehicle Electronic Control Units (ECUs)\", \"In-Vehicle Networks (CAN/LIN/FlexRay/Ethernet)\", \"On-Board Diagnostics Interface (OBD-II)\", \"Vehicle Service Systems\"],\n",
    "        \"Drone / UAV Forensics\": [\"Unmanned Aerial Vehicles (UAVs)\", \"Flight Controllers\", \"Onboard Sensor Modules\", \"Communication Modules\", \"Ground Control Stations\"],\n",
    "        \"Telematics & Event Data Forensics\": [ \"Event Data Recorders (EDR)\", \"Vehicle Telematics Control Units (TCU)\", \"Navigation / Infotainment Systems\"]\n",
    "        }},\n",
    "          \n",
    "        \"Software Forensics\": {{\n",
    "        \"Operating System Forensics\": [\"File systems for Windows/Mac/Unix/Linux\", \"Windows/Mac/Unix/Linux\"],\n",
    "        \"Application Software Forensics\": [\"Mail Services\", \"Web Services\", \"DBMS\", \"Access Control Systems\", \"E-Commerce Services\"],\n",
    "        \"Forensic Tools Analysis (Open source/Proprietary)\": [\"E4Case/FTK/File Hound/Sleuthkit/WinHex\"]\n",
    "        }},\n",
    "          \n",
    "        \"Database Forensics\": {{\n",
    "        \"Database Metadata/Contents Forensics\": [\"DBMS\", \"Databases\"]\n",
    "        }},\n",
    "          \n",
    "        \"Multimedia Forensics\": {{\n",
    "        \"Image Forensics\": [\"Digital Images\"],\n",
    "        \"Video Forensics\": [\"Digital Video\"],\n",
    "        \"Audio Forensics\": [\"Digital Audio\"]\n",
    "        }},\n",
    "          \n",
    "        \"Device Forensics\": {{\n",
    "        \"Peripherals Device Forensics\": [\"Copiers\", \"Printers\", \"Scanners\"],\n",
    "        \"Network Enabled Device Forensics\": [\"Wireless AP\", \"IDS\", \"Firewalls\", \"Hubs\", \"Switches\", \"Routers\"],\n",
    "        \"Storage Device Forensics\": [\"RFID Tags/Smart cards/Memory cards\", \"DVD/CD/Floppy/Tapes\", \"External Hard Drives\", \"Thumb Drive\", \"Digital Music Players\"],\n",
    "        \"Large Scale Device Forensics\": [\"SAN (Storage Area Network)\", \"NAS (Network Attached Storage)\"],\n",
    "        \"Obscure Device Forensics\": [\"Recording Devices (Video/Audio)\", \"Gaming Devices\"],\n",
    "        \"Mobile Forensics\": [\"PDAs\", \"Smart/Cell phones\", \"Tablets\"],\n",
    "        \"Small Scale Device Forensics\": [\"Embedded Devices\"],\n",
    "        \"Wearable & Immersive Device Forensics\": [\"Smart Watches\", \"Smart Glasses\", \"VR Headsets\", \"AR Glasses\", \"Motion Controllers\"],\n",
    "        \"Additive Manufacturing / 3D Printer Forensics\": [\"3D Printer\"],\n",
    "        \"Medical Device Forensics\": [\"Implantable Medical Devices\", \"Wearable Medical Devices\",\"Diagnostic Medical Equipment\", \"Therapeutic Devices\"]\n",
    "        }},\n",
    "          \n",
    "        \"Network Forensics\": {{\n",
    "        \"Cloud Forensics\": [\"Clouds (Cloud Computing)\"],\n",
    "        \"Telecom Network Forensics\": [\"Cell Phone/Telecom Service Provider Network\"],\n",
    "        \"Internet Forensics\": [\"Web Documents\", \"Webmails\", \"Emails\", \"Domain Name Records\", \"ISP Logs\"],\n",
    "        \"Wireless Forensics\": [\"Bluetooth, Infrared, Wi-Fi\"]\n",
    "        }},\n",
    "          \n",
    "        \"IoT Forensics\": {{\n",
    "        \"Smart Home & Building Systems\": [ \"IoT Cameras\",  \"AI Speaker Devices\", \"Thermostats\",  \"Relays / Switch Actuators\", \"Building Automation Systems\", \"HVAC Controllers\", \"Access Control Systems\", \n",
    "        \"Smart Lighting\", \"Energy Management Systems\", \"Occupancy Sensors\"],\n",
    "        \"Industrial IoT Systems\": [\"SCADA Systems\", \"ICS Platforms / Control Systems\",\"Embedded Controllers (PLCs, RTUs)\"],\n",
    "        \"Medical IoT devices,\": [\"Smart Contact Lenses\", \"Glucose Monitoring Devices\", \"Remote Patient Monitoring Devices\", \"Networked Medical Devices\",\"Implantable Medical Devices\" \"Wearable Medical Devices\", \"Diagnostic Medical Equipment\"]\n",
    "        }},\n",
    "          \n",
    "        \"AI Forensics\": {{\n",
    "        \"AI Training Forensics\": [\"Training Process Forensics\", \"Dataset Forensics\", \"Environment Forensics\"],\n",
    "        \"AI Substrate Forensics\": [\"Disk, Network, Sensor, Actuator\"],\n",
    "        \"AI Application Forensics\": [\"API, Artifacts\"],\n",
    "        \"AI Model Forensics\": [\"Model Authentication/Fingerprinting/Identification/Performance/Malware/Chain of Custody\"]\n",
    "        }},\n",
    "        \n",
    "        \"Blockchain Forensics\": {{\n",
    "        \"Wallet Forensics\": [\"Cryptocurrency Wallets\", \"Wallet Key Stores\"],\n",
    "        \"Transaction Analysis\": [\"Blockchain Ledger\"]\n",
    "        }},\n",
    "          \n",
    "        \"Knowledge Systematization\": {{\n",
    "        \"Systematization of Knowledge (SoK)\": [],\n",
    "        \"Systematic Literature Review (SLR)\": [],\n",
    "        \"Ontology Development/Taxonomy Development\": [],\n",
    "        \"Survey Papers\": [],\n",
    "        \"Frameworks\": [\"Validation Methodologies\", \"Community Standards\"],\n",
    "        \"Policy Papers\": [],\n",
    "        \"Education Papers\": [],\n",
    "        \"Legal & Regulatory Studies\": []\n",
    "        }},\n",
    "\n",
    "        \"AI-Assisted Digital Forensics\": {{\n",
    "        \"AI-Assisted Evidence Interpretation\": [\"Forensic Data Interpretation\"],\n",
    "        \"AI-Assisted Investigative Reasoning\": [\"Forensic Reasoning Support\"],\n",
    "        \"AI-Assisted Explanation & Sensemaking\": [\"Explainability Support\"],\n",
    "        \"AI-Assisted Automation & Triage\": [\"Forensic Workflow Support\"]\n",
    "        }}\n",
    "    }}\n",
    "        \n",
    "        Return ONLY valid JSON in this format:\n",
    "        \n",
    "        {{\n",
    "         \"primary_domain\": \"...\",\n",
    "         \"sub_domain\": \"...\",\n",
    "         \"object\": \"...\",\n",
    "         \"tags\": [\"...\", \"...\", \"...\", \"...\", \"...\"]\n",
    "        }}\n",
    "        \n",
    "        \\t6- Examples:\n",
    "        \n",
    "        \\t- In the paper “Avoiding Burnout at the Digital Forensics Coalface”, the authors perform a thematic synthesis and propose evidence-based \n",
    "        frameworks for stress management in the digital forensics workforce.\n",
    "\n",
    "        {{\n",
    "          \"primary_domain\": \"Knowledge Systematization\",\n",
    "          \"sub_domain\": \"Frameworks\",\n",
    "          \"object\": \"Validation Methodologies\"\n",
    "          \"tags\": [\"Resilience\",\"training\", \"stress management\", \"Organisational/occupational job stress\", \"Stress management strategies\"]\n",
    "        }}\n",
    "\n",
    "        \\t- In the paper “TLS Key Material Identification and Extraction from Memory”, the authors extract TLS session keys from \n",
    "        system memory and use them to decrypt previously captured encrypted network traffic.\n",
    "\n",
    "        {{\n",
    "          \"primary_domain\": \"Computer Forensics\",\n",
    "          \"sub_domain\": \"Memory Forensics\",\n",
    "          \"object\": \"Process Memory\",\n",
    "          \"tags\": [\"Memory forensics\", \"Network forensics\", \"TLS\", \"Transport layer security\", \"Live forensics\", \"Volatile memory\", \"TLS key material extraction\", \"Encrypted traffic decryption\"]\n",
    "        }}\n",
    "\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        Your response: \"\"\"\n",
    "        \n",
    "        '''\n",
    "   \n",
    "    else:\n",
    "        raise ValueError(\"Invalid task\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b387335-1b09-474c-8536-81b8197cdedd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv(\"api_key.env\")\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ----------------------------\n",
    "# FILES (separate!)\n",
    "# ----------------------------\n",
    "incremental_csv_path = \"results_incremental.csv\"   # append + resume\n",
    "final_csv_path = \"results_new_taxonomy_analysis.csv\"  # final clean export\n",
    "\n",
    "# ----------------------------\n",
    "# Incremental helpers\n",
    "# ----------------------------\n",
    "def load_processed_titles(csv_path):\n",
    "    if not os.path.isfile(csv_path):\n",
    "        return set()\n",
    "\n",
    "    processed = set()\n",
    "    with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        try:\n",
    "            next(reader)  # header\n",
    "        except StopIteration:\n",
    "            return set()\n",
    "\n",
    "        for row in reader:\n",
    "            if row and row[0].strip():\n",
    "                processed.add(row[0].strip())\n",
    "    return processed\n",
    "\n",
    "def ensure_header(csv_path, tasks):\n",
    "    needs_header = (not os.path.isfile(csv_path)) or (os.path.getsize(csv_path) == 0)\n",
    "    if needs_header:\n",
    "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Paper Title\"] + tasks)\n",
    "\n",
    "def append_incremental_row(csv_path, paper_title, tasks, results_for_paper):\n",
    "    with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        row = [paper_title] + [results_for_paper.get(t, \"No result\") for t in tasks]\n",
    "        writer.writerow(row)\n",
    "        f.flush()\n",
    "\n",
    "# ----------------------------\n",
    "# Main processor (same as yours + incremental save)\n",
    "# ----------------------------\n",
    "def process_papers_for_tasks(papers, tasks):\n",
    "    task_results = {}\n",
    "\n",
    "    # resume ONLY from incremental file\n",
    "    processed_titles = load_processed_titles(incremental_csv_path)\n",
    "    ensure_header(incremental_csv_path, tasks)\n",
    "\n",
    "    for i, paper in enumerate(papers):\n",
    "        paper_title = str(paper[\"title\"]).strip()\n",
    "        print(f\"Processing paper: {paper_title}\")\n",
    "\n",
    "        # skip ONLY based on incremental file\n",
    "        if paper_title in processed_titles:\n",
    "            print(f\"Skipping already processed paper: {paper_title}\")\n",
    "            continue\n",
    "\n",
    "        task_results[paper_title] = {}\n",
    "\n",
    "        for task in tasks:\n",
    "            user_prompt = generate_all_metadata_prompt(task, paper)\n",
    "\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=0.2,\n",
    "                    max_tokens=1500\n",
    "                )\n",
    "                response_text = response.choices[0].message.content\n",
    "                print(response_text)\n",
    "                task_results[paper_title][task] = response_text\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {task} for paper {i+1}: {e}\")\n",
    "                task_results[paper_title][task] = \"error: \" + str(e)\n",
    "\n",
    "        # incremental save AFTER each paper finishes (to incremental file only)\n",
    "        append_incremental_row(incremental_csv_path, paper_title, tasks, task_results[paper_title])\n",
    "        processed_titles.add(paper_title)\n",
    "\n",
    "    return task_results\n",
    "\n",
    "# ----------------------------\n",
    "# Run\n",
    "# ----------------------------\n",
    "test_papers = papers[:]\n",
    "tasks = [\"title\", \"taxonomy_classification\"]\n",
    "\n",
    "# Process papers (writes incremental as it runs)\n",
    "all_results = process_papers_for_tasks(test_papers, tasks)\n",
    "\n",
    "# ----------------------------\n",
    "# Final CSV export (fresh overwrite, separate file)\n",
    "# ----------------------------\n",
    "with open(final_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    headers = [\"Paper Title\"] + tasks\n",
    "    csv_writer.writerow(headers)\n",
    "\n",
    "    for paper_title, results in all_results.items():\n",
    "        row = [paper_title]\n",
    "        for task in tasks:\n",
    "            row.append(results.get(task, \"No result\"))\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(f\"Incremental results: {incremental_csv_path}\")\n",
    "print(f\"Final results: {final_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e71f9-675c-40ac-b244-67b148bce6a3",
   "metadata": {},
   "source": [
    "Venue × Discipline table (USA, EU, APAC) — robust + MEDIAN & CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8eb28-10c3-4961-9428-773b0b177d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re, json, ast\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# Robust parsing helpers\n",
    "# -------------------------------\n",
    "def safe_parse_json(cell):\n",
    "    \"\"\"\n",
    "    Parses cells that look like:\n",
    "      ```json\n",
    "      {...}\n",
    "      ```\n",
    "    Returns Python object or None.\n",
    "    \"\"\"\n",
    "    s = str(cell)\n",
    "\n",
    "    # strip code fences\n",
    "    s = re.sub(r\"```(?:json)?\", \"\", s, flags=re.IGNORECASE).strip()\n",
    "    s = s.replace(\"```\", \"\").strip()\n",
    "\n",
    "    if not s or s.lower() in (\"nan\", \"none\", \"null\"):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def extract_from_raw(raw: str, key: str):\n",
    "    \"\"\"\n",
    "    Regex fallback to extract a string field like \"primary_domain\":\"...\"\n",
    "    even if JSON is malformed.\n",
    "    \"\"\"\n",
    "    if raw is None:\n",
    "        return None\n",
    "    s = str(raw)\n",
    "    s = re.sub(r\"```(?:json)?\", \"\", s, flags=re.IGNORECASE).strip()\n",
    "    s = s.replace(\"```\", \"\")\n",
    "\n",
    "    m = re.search(rf'\"{re.escape(key)}\"\\s*:\\s*\"([^\"]+)\"', s)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Venue mapping \n",
    "# -------------------------------\n",
    "def map_conference_to_venue(conf_str: str) -> str:\n",
    "    s = (conf_str or \"\").strip().upper()\n",
    "    if \"USA\" in s:\n",
    "        return \"DFRWS USA\"\n",
    "    if \"EU\" in s or \"EUROPE\" in s:\n",
    "        return \"DFRWS EU\"\n",
    "    if \"APAC\" in s or \"ASIA\" in s or \"ASIA-PACIFIC\" in s or \"ASIA PACIFIC\" in s:\n",
    "        return \"DFRWS APAC\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_conference(row) -> str:\n",
    "    \"\"\"\n",
    "    Try JSON in 'conference'; if that fails, try plain strings / alternate cols.\n",
    "    Expected new format: row['conference'] contains ```json {\"conference\":\"DFRWS USA\"} ```\n",
    "    \"\"\"\n",
    "    conf_json = safe_parse_json(row.get(\"conference\", \"\"))\n",
    "    if isinstance(conf_json, dict):\n",
    "        val = conf_json.get(\"conference\", \"\") or conf_json.get(\"name\", \"\")\n",
    "        if isinstance(val, str) and val.strip():\n",
    "            return val.strip()\n",
    "\n",
    "    raw = row.get(\"conference\", \"\")\n",
    "    if isinstance(raw, str) and raw.strip():\n",
    "        return raw.strip()\n",
    "\n",
    "    for alt in (\"venue\", \"conf\", \"event\", \"conference_name\"):\n",
    "        if alt in row and isinstance(row[alt], str) and row[alt].strip():\n",
    "            return row[alt].strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "#  extract PRIMARY DOMAIN from taxonomy_classification\n",
    "# -------------------------------\n",
    "def extract_primary_domain(row) -> str:\n",
    "    \"\"\"\n",
    "    Extract primary_domain from taxonomy_classification.\n",
    "    Works even if some rows have malformed JSON (regex fallback).\n",
    "    \"\"\"\n",
    "    raw = row.get(\"taxonomy_classification\", \"\")\n",
    "    tax = safe_parse_json(raw)\n",
    "\n",
    "    if isinstance(tax, dict):\n",
    "        d = tax.get(\"primary_domain\", \"\")\n",
    "        if isinstance(d, str) and d.strip():\n",
    "            return d.strip()\n",
    "\n",
    "    # fallback for malformed JSON\n",
    "    d = extract_from_raw(raw, \"primary_domain\")\n",
    "    return d.strip() if isinstance(d, str) and d.strip() else \"\"\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Venue × Primary Domain table (USA, EU, APAC) — MEDIAN & CV\n",
    "# -------------------------------\n",
    "records = []\n",
    "debug_counts = {\n",
    "    \"rows\": 0,\n",
    "    \"kept\": 0,\n",
    "    \"no_conf\": 0,\n",
    "    \"unknown_venue\": 0,\n",
    "    \"no_domain\": 0,\n",
    "}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    debug_counts[\"rows\"] += 1\n",
    "\n",
    "    conf_raw = extract_conference(row)\n",
    "    venue = map_conference_to_venue(conf_raw)\n",
    "    domain = extract_primary_domain(row)\n",
    "\n",
    "    if not domain:\n",
    "        debug_counts[\"no_domain\"] += 1\n",
    "        continue\n",
    "    if not conf_raw:\n",
    "        debug_counts[\"no_conf\"] += 1\n",
    "        continue\n",
    "    if venue == \"Unknown\":\n",
    "        debug_counts[\"unknown_venue\"] += 1\n",
    "        continue\n",
    "\n",
    "    records.append({\"venue\": venue, \"primary_domain\": domain})\n",
    "    debug_counts[\"kept\"] += 1\n",
    "\n",
    "venue_domain_df = pd.DataFrame.from_records(records, columns=[\"venue\", \"primary_domain\"])\n",
    "\n",
    "if venue_domain_df.empty:\n",
    "    print(\"\\n[WARN] No venue/domain records parsed. Debug:\", debug_counts)\n",
    "    print(\"Tip: Inspect a few raw cells, e.g.:\")\n",
    "    print(\" - df['conference'].head() =\", df.get(\"conference\", pd.Series(dtype=object)).head().to_list())\n",
    "    print(\" - df['taxonomy_classification'].head() =\", df.get(\"taxonomy_classification\", pd.Series(dtype=object)).head().to_list())\n",
    "else:\n",
    "    # Build matrix\n",
    "    venue_domain_matrix = pd.crosstab(\n",
    "        venue_domain_df[\"venue\"],\n",
    "        venue_domain_df[\"primary_domain\"]\n",
    "    ).astype(int)\n",
    "\n",
    "    # Order columns by global total (desc)\n",
    "    col_order = venue_domain_matrix.sum(axis=0).sort_values(ascending=False).index\n",
    "    venue_domain_matrix = venue_domain_matrix[col_order]\n",
    "\n",
    "    # Row order: USA, EU, APAC (only those present)\n",
    "    desired_rows = [v for v in [\"DFRWS USA\", \"DFRWS EU\", \"DFRWS APAC\"] if v in venue_domain_matrix.index]\n",
    "    venue_domain_matrix = venue_domain_matrix.reindex(desired_rows)\n",
    "\n",
    "    # ---- Stats ----\n",
    "    core = venue_domain_matrix[col_order]\n",
    "\n",
    "    venue_domain_matrix[\"TOTAL\"] = core.sum(axis=1)\n",
    "\n",
    "    # Median across domains (including zeros)\n",
    "    venue_domain_matrix[\"MEDIAN\"] = core.median(axis=1).round(2)\n",
    "\n",
    "    # CV = std/mean (protect div-by-zero)\n",
    "    means = core.mean(axis=1)\n",
    "    stds = core.std(axis=1, ddof=1)\n",
    "    venue_domain_matrix[\"CV\"] = (stds / means.replace(0, pd.NA)).fillna(0.0).round(3)\n",
    "\n",
    "    print(\"\\n=== Venue × Primary Domain (counts + MEDIAN & CV) ===\")\n",
    "    print(venue_domain_matrix)\n",
    "\n",
    "    out_path = \"venue_primary_domain_matrix.csv\"\n",
    "    venue_domain_matrix.to_csv(out_path)\n",
    "    print(\"\\nSaved:\", out_path)\n",
    "    print(\"\\n[Debug summary]\", debug_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c528cb-f09c-4f16-b449-ff63d1883cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "CSV_PATH = \"results_new_taxonomy_analysis.csv\"\n",
    "OUTDIR = \"dfrws_top10_country_domain\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2002, 2025\n",
    "TOP_COUNTRIES = 10\n",
    "TOP_DOMAINS = 12                 # heatmap columns\n",
    "\n",
    "# Domain extraction mode:\n",
    "# - \"primary_domain\"             : use primary_domain (fallback to sub_domain if primary missing)\n",
    "# - \"primary_domain_sub_domain\"  : show \"primary / sub\" when both exist (fallback to whichever exists)\n",
    "DOMAIN_MODE = \"primary_domain\"\n",
    "\n",
    "# How to count country contribution per paper:\n",
    "# - \"presence\"   : each (paper,country) counts as 1 regardless of #authors from that country\n",
    "# - \"fractional\" : each paper contributes total=1 split across its unique countries (1/k per country)\n",
    "COUNT_MODE = \"presence\"          # or \"fractional\"\n",
    "\n",
    "# Domains you always want in the heatmap columns (if present after filtering)\n",
    "FORCE_INCLUDE_DOMAINS = [\"AI Forensics\"]\n",
    "\n",
    "# Write rows that were dropped because domain couldn't be extracted (for debugging)\n",
    "WRITE_DROPPED_DOMAIN_DEBUG = True\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SAFE PARSING\n",
    "# -------------------------\n",
    "def safe_parse(cell):\n",
    "    \"\"\"\n",
    "    Parses cells that look like:\n",
    "      ```json\n",
    "      {...}\n",
    "      ```\n",
    "    Also tries python literal parsing as fallback.\n",
    "    Returns Python object or None.\n",
    "    \"\"\"\n",
    "    s = str(cell)\n",
    "\n",
    "    # Remove code fences like ```json ... ```\n",
    "    s = re.sub(r\"```(?:json)?\", \"\", s, flags=re.IGNORECASE).strip()\n",
    "    s = s.replace(\"```\", \"\").strip()\n",
    "\n",
    "    # Treat empty / null-like as None\n",
    "    if not s or s.lower() in (\"nan\", \"none\", \"null\"):\n",
    "        return None\n",
    "\n",
    "    # Try JSON then python literal\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "ALIASES = {\n",
    "    \"usa\":\"USA\",\"us\":\"USA\",\"u s\":\"USA\",\"u.s.\":\"USA\",\"u.s.a.\":\"USA\",\"united states\":\"USA\",\"united states of america\":\"USA\",\n",
    "    \"uk\":\"United Kingdom\",\"u k\":\"United Kingdom\",\"u.k.\":\"United Kingdom\",\"united kingdom\":\"United Kingdom\",\n",
    "    \"england\":\"United Kingdom\",\"scotland\":\"United Kingdom\",\"wales\":\"United Kingdom\",\"great britain\":\"United Kingdom\",\"britain\":\"United Kingdom\",\n",
    "    \"uae\":\"United Arab Emirates\",\"u a e\":\"United Arab Emirates\",\"u.a.e.\":\"United Arab Emirates\",\n",
    "    \"united arab emirates\":\"United Arab Emirates\",\"emirates\":\"United Arab Emirates\",\n",
    "    \"republic of korea\":\"South Korea\",\"korea, republic of\":\"South Korea\",\"south korea\":\"South Korea\",\"korea\":\"South Korea\",\"r o k\":\"South Korea\",\"r.o.k.\":\"South Korea\",\n",
    "}\n",
    "\n",
    "def canon_country(c):\n",
    "    s = str(c).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)   # collapse whitespace\n",
    "    s = s.replace(\".\", \"\")       # remove dots so \"u.s.a.\" -> \"usa\"\n",
    "    return ALIASES.get(s, str(c).strip())\n",
    "\n",
    "def extract_countries(obj):\n",
    "    \"\"\"Return a list of raw country strings from a parsed object.\"\"\"\n",
    "    if obj is None:\n",
    "        return []\n",
    "    if isinstance(obj, dict):\n",
    "        obj = obj.get(\"author_countries\", obj.get(\"countries\", []))\n",
    "    if isinstance(obj, list):\n",
    "        out = []\n",
    "        for x in obj:\n",
    "            if isinstance(x, str):\n",
    "                out.append(x)\n",
    "            elif isinstance(x, dict) and \"country\" in x:\n",
    "                out.append(x[\"country\"])\n",
    "        return out\n",
    "    if isinstance(obj, str):\n",
    "        return [obj]\n",
    "    return []\n",
    "\n",
    "def extract_domain(obj):\n",
    "    \"\"\"\n",
    "    IMPORTANT: This function NEVER returns \"Unknown\".\n",
    "    If it can't extract a domain, it returns None.\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        return None\n",
    "\n",
    "    d = str(obj.get(\"primary_domain\", \"\")).strip()\n",
    "    s = str(obj.get(\"sub_domain\", \"\")).strip()\n",
    "\n",
    "    if DOMAIN_MODE == \"primary_domain_sub_domain\":\n",
    "        if d and s:\n",
    "            return f\"{d} / {s}\"\n",
    "        return d or s or None\n",
    "\n",
    "    # default: primary_domain, but fall back to sub_domain if primary missing\n",
    "    return d or s or None\n",
    "\n",
    "def extract_year(obj):\n",
    "    \"\"\"Handle both dict years and raw strings.\"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "    if isinstance(obj, dict) and \"year\" in obj:\n",
    "        try:\n",
    "            return int(obj[\"year\"])\n",
    "        except Exception:\n",
    "            return None\n",
    "    try:\n",
    "        return int(obj)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_conf(obj):\n",
    "    if obj is None:\n",
    "        return \"\"\n",
    "    if isinstance(obj, dict) and \"conference\" in obj:\n",
    "        return str(obj[\"conference\"])\n",
    "    return str(obj)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "df = pd.read_csv(CSV_PATH, dtype=str, keep_default_na=False, encoding=\"latin1\")\n",
    "\n",
    "df[\"countries\"] = df[\"author_countries\"].apply(safe_parse)\n",
    "df[\"taxonomy\"]  = df[\"taxonomy_classification\"].apply(safe_parse)\n",
    "df[\"year\"]      = df[\"published_year\"].apply(lambda x: extract_year(safe_parse(x)))\n",
    "df[\"conf\"]      = df[\"conference\"].apply(lambda x: extract_conf(safe_parse(x)))\n",
    "\n",
    "df[\"domain\"] = df[\"taxonomy\"].apply(extract_domain)\n",
    "\n",
    "# filter DFRWS + years\n",
    "df = df[df[\"conf\"].str.startswith(\"DFRWS\", na=False)]\n",
    "df = df[df[\"year\"].notna()]\n",
    "df = df[(df[\"year\"] >= YEAR_MIN) & (df[\"year\"] <= YEAR_MAX)]\n",
    "\n",
    "# build UNIQUE COUNTRY LIST per paper (dedup within paper!)\n",
    "df[\"country_list\"] = df[\"countries\"].apply(\n",
    "    lambda x: sorted({canon_country(c) for c in extract_countries(x) if str(c).strip()})\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# DROP ROWS WITH NO DOMAIN (so \"Unknown\" can never exist)\n",
    "# -------------------------\n",
    "before = len(df)\n",
    "dropped = df[df[\"domain\"].isna() | (df[\"domain\"].astype(str).str.strip() == \"\")].copy()\n",
    "\n",
    "df = df[df[\"domain\"].notna()]\n",
    "df = df[df[\"domain\"].astype(str).str.strip() != \"\"]\n",
    "\n",
    "after = len(df)\n",
    "print(f\"[INFO] Dropped {before - after} rows with missing/unparseable domain (no Unknown bucket).\")\n",
    "\n",
    "if WRITE_DROPPED_DOMAIN_DEBUG and len(dropped) > 0:\n",
    "    debug_path = os.path.join(OUTDIR, \"dropped_rows_missing_domain.csv\")\n",
    "    keep_cols = []\n",
    "    for col in [\"title\", \"conf\", \"year\", \"conference\", \"published_year\", \"author_countries\", \"taxonomy_classification\"]:\n",
    "        if col in dropped.columns:\n",
    "            keep_cols.append(col)\n",
    "    dropped[keep_cols].to_csv(debug_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[DEBUG] Wrote {debug_path} (rows dropped for missing domain)\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# COUNTRY COUNTING (PER PAPER, NOT PER AUTHOR)\n",
    "# -------------------------\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    countries = [c for c in r[\"country_list\"] if c and c != \"Unknown\"]\n",
    "    if not countries:\n",
    "        continue\n",
    "\n",
    "    # weight per (paper,country)\n",
    "    w = (1.0 / len(countries)) if COUNT_MODE == \"fractional\" else 1.0\n",
    "\n",
    "    for c in countries:\n",
    "        rows.append((c, r[\"domain\"], w))\n",
    "\n",
    "bin_df = pd.DataFrame(rows, columns=[\"country\", \"domain\", \"weight\"])\n",
    "\n",
    "# totals per country (sum of weights)\n",
    "country_totals = bin_df.groupby(\"country\")[\"weight\"].sum().reset_index(name=\"total\")\n",
    "\n",
    "# Top countries by total\n",
    "topN = (\n",
    "    country_totals\n",
    "    .sort_values(\"total\", ascending=False)\n",
    "    .head(TOP_COUNTRIES)[\"country\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "bin_df = bin_df[bin_df[\"country\"].isin(topN)]\n",
    "country_totals = country_totals[country_totals[\"country\"].isin(topN)]\n",
    "\n",
    "# domain counts per country (sum of weights)\n",
    "country_domain = (\n",
    "    bin_df.groupby([\"country\", \"domain\"])[\"weight\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "country_domain = country_domain.merge(country_totals, on=\"country\", how=\"left\")\n",
    "country_domain[\"share_pct\"] = 100 * country_domain[\"count\"] / country_domain[\"total\"]\n",
    "\n",
    "# -------------------------\n",
    "# SAVE TABLE (TOP COUNTRIES ONLY)\n",
    "# -------------------------\n",
    "count_mode_label = \"fractional\" if COUNT_MODE == \"fractional\" else \"presence\"\n",
    "csv_out = os.path.join(OUTDIR, f\"top{TOP_COUNTRIES}_country_domain_{count_mode_label}.csv\")\n",
    "country_domain.sort_values([\"country\", \"share_pct\"], ascending=[True, False]).to_csv(csv_out, index=False)\n",
    "print(f\"[OK] Wrote {csv_out}\")\n",
    "\n",
    "# -------------------------\n",
    "# HEATMAP (TOP COUNTRIES × TOP DOMAINS)\n",
    "# -------------------------\n",
    "top_domains = (\n",
    "    country_domain.groupby(\"domain\")[\"count\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(TOP_DOMAINS)\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "# Force-include domains if present\n",
    "present_domains = set(country_domain[\"domain\"].unique())\n",
    "for d in FORCE_INCLUDE_DOMAINS:\n",
    "    if d in present_domains and d not in top_domains:\n",
    "        top_domains.append(d)\n",
    "\n",
    "plot_df = country_domain[country_domain[\"domain\"].isin(top_domains)]\n",
    "\n",
    "pivot = (\n",
    "    plot_df.pivot(index=\"country\", columns=\"domain\", values=\"share_pct\")\n",
    "    .reindex(topN)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "im = ax.imshow(pivot.values, aspect=\"auto\")\n",
    "\n",
    "ax.set_yticks(range(len(pivot.index)))\n",
    "ax.set_yticklabels(pivot.index)\n",
    "ax.set_xticks(range(len(pivot.columns)))\n",
    "ax.set_xticklabels(pivot.columns, rotation=45, ha=\"right\")\n",
    "\n",
    "# Add numbers inside cells (0 decimals)\n",
    "for i in range(pivot.shape[0]):\n",
    "    for j in range(pivot.shape[1]):\n",
    "        val = pivot.values[i, j]\n",
    "        txt_color = \"white\" if val >= 18 else \"black\"\n",
    "        ax.text(j, i, f\"{val:.0f}\", ha=\"center\", va=\"center\",\n",
    "                color=txt_color, fontsize=8, fontweight=\"bold\")\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Share of country output (%)\")\n",
    "\n",
    "title_mode = \"Fractional\" if COUNT_MODE == \"fractional\" else \"Country-presence\"\n",
    "ax.set_title(f\"Top-{TOP_COUNTRIES} Countries × Domains (DFRWS, {YEAR_MIN}-{YEAR_MAX}) — {title_mode} counting\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = os.path.join(OUTDIR, f\"top{TOP_COUNTRIES}_country_domain_heatmap_{count_mode_label}.png\")\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"[OK] Saved {fig_path}\")\n",
    "print(\"[DONE]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5807c6-af4d-4864-a59e-8064ad14adb5",
   "metadata": {},
   "source": [
    "CDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d47687-34b6-46ad-acb2-148a2d8e6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, json, re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- config ----------\n",
    "INPUT_CSV = \"results_new_taxonomy_analysis.csv\"\n",
    "YEAR_MIN, YEAR_MAX = 2002, 2025\n",
    "all_years = list(range(YEAR_MIN, YEAR_MAX + 1))\n",
    "\n",
    "# ---------- robust parsers ----------\n",
    "def strip_fences(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    # remove ANY ``` or ```json occurrences, anywhere in the cell\n",
    "    s = re.sub(r\"```(?:json)?\", \"\", s, flags=re.IGNORECASE)\n",
    "    s = s.replace(\"```\", \"\")\n",
    "    return s.strip()\n",
    "\n",
    "def parse_json_messy(cell, default=None):\n",
    "    if isinstance(cell, dict):\n",
    "        return cell\n",
    "    s = strip_fences(cell)\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        # fallback: pull \"year\": \"2007\" with regex if JSON is broken\n",
    "        m = re.search(r'\"year\"\\s*:\\s*\"?(?P<y>\\d{4})\"?', s or \"\")\n",
    "        if m:\n",
    "            return {\"year\": m.group(\"y\")}\n",
    "        return default if default is not None else {}\n",
    "\n",
    "def coerce_year(y):\n",
    "    try:\n",
    "        y = int(str(y).strip())\n",
    "        return y if YEAR_MIN <= y <= YEAR_MAX else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------- rebuild df (year + primary_domain) ----------\n",
    "raw = pd.read_csv(INPUT_CSV, encoding=\"latin1\")\n",
    "\n",
    "years = raw.get(\"published_year\", pd.Series([\"\"] * len(raw))).apply(parse_json_messy, default={})\n",
    "years = years.apply(lambda d: (d or {}).get(\"year\"))\n",
    "years = years.apply(coerce_year)\n",
    "\n",
    "onto = raw.get(\"taxonomy_classification\", pd.Series([\"\"] * len(raw))).apply(parse_json_messy, default={})\n",
    "primary_domain = onto.apply(lambda d: (d or {}).get(\"primary_domain\", \"\")).fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "df = pd.DataFrame({\"year\": years, \"primary_domain\": primary_domain}).dropna(subset=[\"year\"])\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "# ---------- year × primary_domain matrix (zeros for missing years) ----------\n",
    "counts = (\n",
    "    df.groupby([\"year\", \"primary_domain\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "mat = (\n",
    "    counts.pivot(index=\"year\", columns=\"primary_domain\", values=\"count\")\n",
    "          .reindex(all_years, fill_value=0)\n",
    "          .fillna(0).astype(int)\n",
    ")\n",
    "\n",
    "# ---------- CDF-style: ALL papers + Top-5 primary_domain ----------\n",
    "# ALL papers per year\n",
    "yearly_all = df.groupby(\"year\").size().reindex(all_years, fill_value=0)\n",
    "\n",
    "# Top-5 primary_domain by total volume\n",
    "top5 = mat.sum(axis=0).sort_values(ascending=False).head(6).index.tolist()\n",
    "print(\"Top-5 Primary Domain:\", top5)\n",
    "\n",
    "# Yearly counts table for ALL + Top-5\n",
    "cdf_counts = pd.DataFrame({\"ALL Papers\": yearly_all})\n",
    "for d in top5:\n",
    "    cdf_counts[d] = mat[d].reindex(all_years, fill_value=0)\n",
    "\n",
    "# Cumulative sums (CDF curves)\n",
    "cdf_cum = cdf_counts.cumsum()\n",
    "\n",
    "# Plot cumulative counts\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in cdf_cum.columns:\n",
    "    plt.plot(cdf_cum.index, cdf_cum[col], linewidth=2.2, label=col)\n",
    "plt.title(\"Cumulative Papers by Year (ALL + Top-6 Domains)\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Cumulative Count\")\n",
    "plt.xticks(all_years, rotation=45); plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend(title=\"Series\", loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# (Optional) Normalized CDFs (0..1) to compare timing of growth\n",
    "cdf_norm = cdf_cum.div(cdf_cum.iloc[-1])\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in cdf_norm.columns:\n",
    "    plt.plot(cdf_norm.index, cdf_norm[col], linewidth=2.0, label=col)\n",
    "plt.title(\"Normalized CDF (Proportion Reached by Year)\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Proportion of Final Total\"); plt.ylim(0, 1.02)\n",
    "plt.xticks(all_years, rotation=45); plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend(title=\"Series\", loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Exports\n",
    "cdf_counts.to_csv(\"cdf_all_plus_top6_yearly_counts.csv\")\n",
    "cdf_cum.to_csv(\"cdf_all_plus_top6_cumulative_counts.csv\")\n",
    "cdf_norm.to_csv(\"cdf_all_plus_top6_normalized_cdf.csv\")\n",
    "print(\"Saved CDF CSVs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f99e22-b19a-433f-82ab-77a80d62914d",
   "metadata": {},
   "source": [
    "Country Level Domain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c1e5c-301a-4e8a-95d7-672f1250be02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, re, json, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "CSV_PATH = \"results_new_taxonomy_analysis.csv\"\n",
    "OUTDIR = \"dfrws_top10_country_domain\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2002, 2025\n",
    "TOP_COUNTRIES = 10\n",
    "TOP_DOMAINS = 12  # columns in heatmap\n",
    "\n",
    "# Domain extraction mode:\n",
    "# - \"primary_domain\"              : use primary_domain only (fallback to sub_domain if primary missing)\n",
    "# - \"primary_domain_sub_domain\"   : show \"primary / sub\" when both exist (fallback to whichever exists)\n",
    "DOMAIN_MODE = \"primary_domain\"\n",
    "\n",
    "# How to count country contribution per paper:\n",
    "# - \"presence\"   : each (paper,country) counts as 1 regardless of #authors from that country\n",
    "# - \"fractional\" : each paper contributes total=1 split across its unique countries (1/k per country)\n",
    "COUNT_MODE = \"presence\"  # or \"fractional\"\n",
    "\n",
    "\n",
    "FORCE_INCLUDE_DOMAINS = [\n",
    "    \"AI Forensics\",\n",
    "]\n",
    "\n",
    "DROP_UNKNOWN_FROM_HEATMAP = True\n",
    "\n",
    "WRITE_UNKNOWN_DEBUG = True\n",
    "\n",
    "# -------------------------\n",
    "# SAFE PARSING (WITH OPTIONAL DEBUG)\n",
    "# -------------------------\n",
    "parse_errors = []\n",
    "\n",
    "def safe_parse(cell, field=\"\"):\n",
    "    s = str(cell)\n",
    "\n",
    "    s = re.sub(r\"```(?:json)?\", \"\", s, flags=re.IGNORECASE).strip()\n",
    "    s = s.replace(\"```\", \"\").strip()\n",
    "\n",
    "    if not s or s.lower() in (\"nan\", \"none\", \"null\"):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception as e2:\n",
    "            parse_errors.append({\n",
    "                \"field\": field,\n",
    "                \"preview\": s[:400],\n",
    "                \"json_error\": str(e1),\n",
    "                \"ast_error\": str(e2),\n",
    "            })\n",
    "            return None\n",
    "\n",
    "ALIASES = {\n",
    "    \"usa\":\"USA\",\"us\":\"USA\",\"u s\":\"USA\",\"u.s.\":\"USA\",\"u.s.a.\":\"USA\",\"united states\":\"USA\",\"united states of america\":\"USA\",\n",
    "    \"uk\":\"United Kingdom\",\"u k\":\"United Kingdom\",\"u.k.\":\"United Kingdom\",\"united kingdom\":\"United Kingdom\",\n",
    "    \"england\":\"United Kingdom\",\"scotland\":\"United Kingdom\",\"wales\":\"United Kingdom\",\"great britain\":\"United Kingdom\",\"britain\":\"United Kingdom\",\n",
    "    \"uae\":\"United Arab Emirates\",\"u a e\":\"United Arab Emirates\",\"u.a.e.\":\"United Arab Emirates\",\n",
    "    \"united arab emirates\":\"United Arab Emirates\",\"emirates\":\"United Arab Emirates\",\n",
    "    \"republic of korea\":\"South Korea\",\"korea, republic of\":\"South Korea\",\"south korea\":\"South Korea\",\"korea\":\"South Korea\",\"r o k\":\"South Korea\",\"r.o.k.\":\"South Korea\",\n",
    "}\n",
    "\n",
    "def canon_country(c):\n",
    "    s = str(c).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)  # collapse whitespace\n",
    "    s = s.replace(\".\", \"\")      # remove dots so \"u.s.a.\" -> \"usa\"\n",
    "    return ALIASES.get(s, str(c).strip())\n",
    "\n",
    "def extract_countries(obj):\n",
    "    \"\"\"Return a list of raw country strings from a parsed object.\"\"\"\n",
    "    if obj is None:\n",
    "        return []\n",
    "    if isinstance(obj, dict):\n",
    "        obj = obj.get(\"author_countries\", obj.get(\"countries\", []))\n",
    "    if isinstance(obj, list):\n",
    "        out = []\n",
    "        for x in obj:\n",
    "            if isinstance(x, str):\n",
    "                out.append(x)\n",
    "            elif isinstance(x, dict) and \"country\" in x:\n",
    "                out.append(x[\"country\"])\n",
    "        return out\n",
    "    if isinstance(obj, str):\n",
    "        return [obj]\n",
    "    return []\n",
    "\n",
    "def extract_domain(obj):\n",
    "    \"\"\"\n",
    "    Extract domain from taxonomy dict.\n",
    "    If taxonomy is missing/unparseable or has no domain keys, returns \"Unknown\".\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        return \"Unknown\"\n",
    "\n",
    "    d = str(obj.get(\"primary_domain\", \"\")).strip()\n",
    "    s = str(obj.get(\"sub_domain\", \"\")).strip()\n",
    "\n",
    "    if DOMAIN_MODE == \"primary_domain_sub_domain\":\n",
    "        if d and s:\n",
    "            return f\"{d} / {s}\"\n",
    "        return d or s or \"Unknown\"\n",
    "\n",
    "    # default: primary_domain, but fall back to sub_domain if primary missing\n",
    "    return d or s or \"Unknown\"\n",
    "\n",
    "def extract_year(obj):\n",
    "    \"\"\"Handle both dict years and raw strings.\"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "    if isinstance(obj, dict) and \"year\" in obj:\n",
    "        try:\n",
    "            return int(obj[\"year\"])\n",
    "        except Exception:\n",
    "            return None\n",
    "    try:\n",
    "        return int(obj)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_conf(obj):\n",
    "    if obj is None:\n",
    "        return \"\"\n",
    "    if isinstance(obj, dict) and \"conference\" in obj:\n",
    "        return str(obj[\"conference\"])\n",
    "    return str(obj)\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "df = pd.read_csv(CSV_PATH, dtype=str, keep_default_na=False, encoding=\"latin1\")\n",
    "\n",
    "df[\"countries\"] = df[\"author_countries\"].apply(lambda x: safe_parse(x, \"author_countries\"))\n",
    "df[\"taxonomy\"]  = df[\"taxonomy_classification\"].apply(lambda x: safe_parse(x, \"taxonomy_classification\"))\n",
    "df[\"year\"]      = df[\"published_year\"].apply(lambda x: extract_year(safe_parse(x, \"published_year\")))\n",
    "df[\"conf\"]      = df[\"conference\"].apply(lambda x: extract_conf(safe_parse(x, \"conference\")))\n",
    "\n",
    "df[\"domain\"] = df[\"taxonomy\"].apply(extract_domain)\n",
    "\n",
    "# filter DFRWS + years\n",
    "df = df[df[\"conf\"].str.startswith(\"DFRWS\", na=False)]\n",
    "df = df[df[\"year\"].notna()]\n",
    "df = df[(df[\"year\"] >= YEAR_MIN) & (df[\"year\"] <= YEAR_MAX)]\n",
    "\n",
    "# build UNIQUE COUNTRY LIST per paper (dedup within paper!)\n",
    "df[\"country_list\"] = df[\"countries\"].apply(\n",
    "    lambda x: sorted({canon_country(c) for c in extract_countries(x) if str(c).strip()})\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# OPTIONAL: DEBUG UNKNOWN DOMAIN ROWS\n",
    "# -------------------------\n",
    "if WRITE_UNKNOWN_DEBUG:\n",
    "    unk = df[df[\"domain\"] == \"Unknown\"].copy()\n",
    "    print(f\"[DEBUG] Unknown domain rows after filters: {len(unk)}\")\n",
    "\n",
    "    if len(unk) > 0:\n",
    "        debug_path = os.path.join(OUTDIR, \"unknown_domain_rows_debug.csv\")\n",
    "        # Save both raw and parsed previews\n",
    "        unk_out = unk[[\"year\", \"conf\", \"published_year\", \"taxonomy_classification\", \"domain\"]].copy()\n",
    "        unk_out.to_csv(debug_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"[DEBUG] Wrote {debug_path}\")\n",
    "\n",
    "# Also dump parse errors (if any)\n",
    "if parse_errors:\n",
    "    pe_path = os.path.join(OUTDIR, \"parse_errors_debug.csv\")\n",
    "    pd.DataFrame(parse_errors).to_csv(pe_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[DEBUG] Wrote {pe_path} ({len(parse_errors)} parse errors)\")\n",
    "\n",
    "# -------------------------\n",
    "# COUNTRY COUNTING (PER PAPER, NOT PER AUTHOR)\n",
    "# -------------------------\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    countries = [c for c in r[\"country_list\"] if c and c != \"Unknown\"]\n",
    "    if not countries:\n",
    "        continue\n",
    "\n",
    "    # weight per (paper,country)\n",
    "    w = (1.0 / len(countries)) if COUNT_MODE == \"fractional\" else 1.0\n",
    "\n",
    "    for c in countries:\n",
    "        rows.append((c, r[\"domain\"], w))\n",
    "\n",
    "bin_df = pd.DataFrame(rows, columns=[\"country\", \"domain\", \"weight\"])\n",
    "\n",
    "# totals per country (sum of weights)\n",
    "country_totals = bin_df.groupby(\"country\")[\"weight\"].sum().reset_index(name=\"total\")\n",
    "\n",
    "# Top-10 countries by total\n",
    "top10 = (\n",
    "    country_totals\n",
    "    .sort_values(\"total\", ascending=False)\n",
    "    .head(TOP_COUNTRIES)[\"country\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "bin_df = bin_df[bin_df[\"country\"].isin(top10)]\n",
    "country_totals = country_totals[country_totals[\"country\"].isin(top10)]\n",
    "\n",
    "# domain counts per country (sum of weights)\n",
    "country_domain = (\n",
    "    bin_df.groupby([\"country\", \"domain\"])[\"weight\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "country_domain = country_domain.merge(country_totals, on=\"country\", how=\"left\")\n",
    "country_domain[\"share_pct\"] = 100 * country_domain[\"count\"] / country_domain[\"total\"]\n",
    "\n",
    "# -------------------------\n",
    "# SAVE TABLE (TOP-10 ONLY)\n",
    "# -------------------------\n",
    "count_mode_label = \"fractional\" if COUNT_MODE == \"fractional\" else \"presence\"\n",
    "csv_out = os.path.join(OUTDIR, f\"top10_country_domain_country_{count_mode_label}.csv\")\n",
    "country_domain.sort_values([\"country\", \"share_pct\"], ascending=[True, False]).to_csv(csv_out, index=False)\n",
    "print(f\"[OK] Wrote {csv_out}\")\n",
    "\n",
    "# -------------------------\n",
    "# HEATMAP (TOP-10 × TOP DOMAINS)\n",
    "# -------------------------\n",
    "# Optionally remove Unknown from the domain pool so it can't appear in the heatmap columns\n",
    "domain_pool = country_domain.copy()\n",
    "if DROP_UNKNOWN_FROM_HEATMAP:\n",
    "    domain_pool = domain_pool[domain_pool[\"domain\"] != \"Unknown\"]\n",
    "\n",
    "top_domains = (\n",
    "    domain_pool.groupby(\"domain\")[\"count\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(TOP_DOMAINS)\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "# Force-include rare domains (e.g., \"AI Forensics\") if they exist in the dataset\n",
    "present_domains = set(domain_pool[\"domain\"].unique())\n",
    "for d in FORCE_INCLUDE_DOMAINS:\n",
    "    if d in present_domains and d not in top_domains:\n",
    "        top_domains.append(d)\n",
    "\n",
    "plot_df = domain_pool[domain_pool[\"domain\"].isin(top_domains)]\n",
    "\n",
    "pivot = (\n",
    "    plot_df.pivot(index=\"country\", columns=\"domain\", values=\"share_pct\")\n",
    "    .reindex(top10)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "im = ax.imshow(pivot.values, aspect=\"auto\")\n",
    "\n",
    "ax.set_yticks(range(len(pivot.index)))\n",
    "ax.set_yticklabels(pivot.index)\n",
    "ax.set_xticks(range(len(pivot.columns)))\n",
    "ax.set_xticklabels(pivot.columns, rotation=45, ha=\"right\")\n",
    "\n",
    "# Add numbers inside cells (0 decimals)\n",
    "for i in range(pivot.shape[0]):\n",
    "    for j in range(pivot.shape[1]):\n",
    "        val = pivot.values[i, j]\n",
    "        txt_color = \"white\" if val >= 18 else \"black\"\n",
    "        ax.text(j, i, f\"{val:.0f}\", ha=\"center\", va=\"center\",\n",
    "                color=txt_color, fontsize=8, fontweight=\"bold\")\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Share of country output (%)\")\n",
    "\n",
    "title_mode = \"Fractional\" if COUNT_MODE == \"fractional\" else \"Country-presence\"\n",
    "ax.set_title(f\"Top-{TOP_COUNTRIES} Countries × Domains (DFRWS, {YEAR_MIN}-{YEAR_MAX}) — {title_mode} counting\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = os.path.join(OUTDIR, f\"top10_country_domain_heatmap_{count_mode_label}.png\")\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"[OK] Saved {fig_path}\")\n",
    "print(\"[DONE]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6327a-22a7-41a0-a2a3-220f08fa94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "CSV_PATH = \"results_new_taxonomy_analysis.csv\"\n",
    "OUTDIR = \"dfrws_top10_country_domain\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2002, 2025\n",
    "TOP_COUNTRIES = 10\n",
    "TOP_DOMAINS = 13\n",
    "DOMAIN_MODE = \"primary_domain\"   # or \"discipline_subdiscipline\"\n",
    "\n",
    "# -------------------------\n",
    "# SAFE PARSING\n",
    "# -------------------------\n",
    "def safe_parse(cell):\n",
    "    s = str(cell)\n",
    "    s = re.sub(r\"```(?:json)?|```\", \"\", s).strip()\n",
    "    if not s or s.lower() in (\"nan\", \"none\", \"null\"):\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "ALIASES = {\n",
    "    # USA\n",
    "    \"usa\":\"USA\",\"u.s.\":\"USA\",\"u.s.a.\":\"USA\",\"u.s\":\"USA\",\"us\":\"USA\",\n",
    "    \"united states\":\"USA\",\"united states of america\":\"USA\",\n",
    "    # UK\n",
    "    \"uk\":\"United Kingdom\",\"u.k.\":\"United Kingdom\",\"u.k\":\"United Kingdom\",\n",
    "    \"united kingdom\":\"United Kingdom\",\"england\":\"United Kingdom\",\n",
    "    \"scotland\":\"United Kingdom\",\"wales\":\"United Kingdom\",\"great britain\":\"United Kingdom\",\n",
    "    # UAE\n",
    "    \"uae\":\"United Arab Emirates\",\"u.a.e.\":\"United Arab Emirates\",\n",
    "    \"united arab emirates\":\"United Arab Emirates\",\"emirates\":\"United Arab Emirates\",\n",
    "    # Korea\n",
    "    \"republic of korea\":\"South Korea\",\"korea, republic of\":\"South Korea\",\n",
    "}\n",
    "\n",
    "def canon_country(c):\n",
    "    return ALIASES.get(str(c).strip().lower(), str(c).strip())\n",
    "\n",
    "def extract_countries(obj):\n",
    "    if obj is None:\n",
    "        return []\n",
    "    if isinstance(obj, dict):\n",
    "        obj = obj.get(\"author_countries\", obj.get(\"countries\", []))\n",
    "    if isinstance(obj, list):\n",
    "        out = []\n",
    "        for x in obj:\n",
    "            if isinstance(x, str):\n",
    "                out.append(x)\n",
    "            elif isinstance(x, dict) and \"country\" in x:\n",
    "                out.append(x[\"country\"])\n",
    "        return out\n",
    "    if isinstance(obj, str):\n",
    "        return [obj]\n",
    "    return []\n",
    "\n",
    "def extract_domain(obj):\n",
    "    if not isinstance(obj, dict):\n",
    "        return \"Unknown\"\n",
    "    d = str(obj.get(\"primary_domain\",\"\")).strip()\n",
    "    s = str(obj.get(\"sub_domain\",\"\")).strip()\n",
    "    if DOMAIN_MODE == \"primary_domain_sub_domain\" and d and s:\n",
    "        return f\"{d} / {s}\"\n",
    "    return d if d else \"Unknown\"\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "df = pd.read_csv(CSV_PATH, dtype=str, keep_default_na=False, encoding=\"latin1\")\n",
    "\n",
    "df[\"countries\"] = df[\"author_countries\"].apply(safe_parse)\n",
    "df[\"taxonomy\"]  = df[\"taxonomy_classification\"].apply(safe_parse)\n",
    "\n",
    "# robust year/conf extraction\n",
    "def get_year(x):\n",
    "    j = safe_parse(x)\n",
    "    if isinstance(j, dict) and \"year\" in j:\n",
    "        try:\n",
    "            return int(j[\"year\"])\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def get_conf(x):\n",
    "    j = safe_parse(x)\n",
    "    if isinstance(j, dict) and \"conference\" in j:\n",
    "        return str(j[\"conference\"]).strip()\n",
    "    return \"\"\n",
    "\n",
    "df[\"year\"] = df[\"published_year\"].apply(get_year)\n",
    "df[\"conf\"] = df[\"conference\"].apply(get_conf)\n",
    "\n",
    "df[\"primary_domain\"] = df[\"taxonomy\"].apply(extract_domain)\n",
    "\n",
    "# filter DFRWS + years\n",
    "df = df[df[\"conf\"].str.startswith(\"DFRWS\")]\n",
    "df = df[df[\"year\"].between(YEAR_MIN, YEAR_MAX)]\n",
    "\n",
    "# build country lists (dedupe AFTER normalization)\n",
    "df[\"country_list\"] = df[\"countries\"].apply(\n",
    "    lambda x: sorted(set(\n",
    "        canon_country(c) for c in extract_countries(x) if str(c).strip()\n",
    "    ))\n",
    ")\n",
    "\n",
    "# drop papers with no countries (if any exist)\n",
    "df = df[df[\"country_list\"].map(len) > 0]\n",
    "\n",
    "# -------------------------\n",
    "# BINARY COUNTING\n",
    "# -------------------------\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    for c in r[\"country_list\"]:\n",
    "        rows.append((c, r[\"primary_domain\"]))\n",
    "\n",
    "bin_df = pd.DataFrame(rows, columns=[\"country\",\"primary_domain\"])\n",
    "\n",
    "country_totals = bin_df.groupby(\"country\").size().reset_index(name=\"total\")\n",
    "\n",
    "# Top-10 countries\n",
    "top10 = (\n",
    "    country_totals.sort_values(\"total\", ascending=False)\n",
    "    .head(TOP_COUNTRIES)[\"country\"].tolist()\n",
    ")\n",
    "\n",
    "bin_df = bin_df[bin_df[\"country\"].isin(top10)]\n",
    "\n",
    "# domain counts per country\n",
    "country_domain = (\n",
    "    bin_df.groupby([\"country\",\"primary_domain\"]).size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "country_domain = country_domain.merge(country_totals, on=\"country\", how=\"left\")\n",
    "country_domain[\"share_pct\"] = 100 * country_domain[\"count\"] / country_domain[\"total\"]\n",
    "\n",
    "# -------------------------\n",
    "# SAVE TABLE (TOP-10 ONLY) - LONG FORMAT\n",
    "# -------------------------\n",
    "csv_long = os.path.join(OUTDIR, \"top10_country_domain_binary_LONG.csv\")\n",
    "country_domain.sort_values([\"country\",\"share_pct\"], ascending=[True,False]).to_csv(csv_long, index=False)\n",
    "print(f\"[OK] Wrote {csv_long}\")\n",
    "\n",
    "# -------------------------\n",
    "# PICK TOP DOMAINS (global, within top10)\n",
    "# -------------------------\n",
    "top_domains = (\n",
    "    country_domain.groupby(\"primary_domain\")[\"count\"].sum()\n",
    "    .sort_values(ascending=False).head(TOP_DOMAINS).index.tolist()\n",
    ")\n",
    "\n",
    "plot_df = country_domain[country_domain[\"primary_domain\"].isin(top_domains)].copy()\n",
    "\n",
    "# -------------------------\n",
    "# HEATMAP MATRIX (WIDE) + SAVE AS CSV \n",
    "# -------------------------\n",
    "pivot = (\n",
    "    plot_df.pivot(index=\"country\", columns=\"primary_domain\", values=\"share_pct\")\n",
    "    .reindex(top10).fillna(0.0)\n",
    ")\n",
    "\n",
    "csv_wide = os.path.join(OUTDIR, \"top10_country_domain_heatmap_matrix.csv\")\n",
    "pivot.to_csv(csv_wide)\n",
    "print(f\"[OK] Wrote {csv_wide}\")\n",
    "\n",
    "# -------------------------\n",
    "# HEATMAP FIGURE\n",
    "# -------------------------\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "im = ax.imshow(pivot.values, aspect=\"auto\")\n",
    "\n",
    "ax.set_yticks(range(len(pivot.index)))\n",
    "ax.set_yticklabels(pivot.index)\n",
    "ax.set_xticks(range(len(pivot.columns)))\n",
    "ax.set_xticklabels(pivot.columns, rotation=45, ha=\"right\")\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Share of country output (%)\")\n",
    "\n",
    "ax.set_title(\"Top-10 Countries × Digital Forensics Domains (Binary Counting)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = os.path.join(OUTDIR, \"top10_country_domain_heatmap.png\")\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"[OK] Saved {fig_path}\")\n",
    "print(\"[DONE]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb252337-5551-4bdf-8813-0b28f5d88b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
